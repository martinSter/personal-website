[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Journals\n\nSterchi, M., Raubach, E., Hilfiker, L. (2025). SwissRailNet: Four representations of a spatially embedded railway network dataset. Data in Brief. https://doi.org/10.1016/j.dib.2025.112266\nSterchi, M., Hilfiker, L., Gr√ºtter, R., Bernstein, A. (2023). Active Querying Approach to Epidemic Source Detection on Contact Networks. Scientific Reports, 13(1). https://doi.org/10.1038/s41598-023-38282-8\nSterchi, M., Sarasua, C., Gr√ºtter, R., Bernstein, A. (2021). Outbreak detection for temporal contact data. Applied Network Science, 6(17). https://doi.org/10.1007/s41109-021-00360-z\nSterchi, M., Faverjon, C., Sarasua, C., Vargas, ME., Berezowski, J., Bernstein, A., Gr√ºtter, R., Nathues, H. (2019). The pig transport network in Switzerland: Structure, patterns, and implications for the transmission of infectious diseases between animal holdings. PLOS ONE, 14(5): e0217974. https://doi.org/10.1371/journal.pone.0217974\nFaverjon, C., Bernstein, A., Gr√ºtter, R., Nathues, C., Nathues, H., Sarasua, C., Sterchi, M., Vargas, ME., Berezowski, J. (2019). A Transdisciplinary Approach Supporting the Implementation of a Big Data Project in Livestock Production: An Example From the Swiss Pig Production Industry. Frontiers in Veterinary Science, 6(215). https://doi.org/10.3389/fvets.2019.00215\nSt√∂ckli, S., Messner, C., Sterchi, M., Dorn, M. (2019). Unreliable is Better: Theoretical and Practical Impulses for Performance Management. Die Unternehmung, 73(2), 167-180. https://www.jstor.org/stable/26696764\nHulliger, B. & Sterchi, M. (2018). A survey-based design of a pricing system for psychotherapy. Health Economics Review, 8(29), 2-11. https://doi.org/10.1186/s13561-018-0213-7\n\n\n\nProceedings\n\nSterchi, M., Sarasua, C., Gr√ºtter, R., Bernstein, A. (2019). Maximizing the Likelihood of Detecting Outbreaks in Temporal Networks. In: Proceedings of the 8th International Conference on Complex Networks and Their Applications, COMPLEX NETWORKS, 2019. Oral Presentation. https://doi.org/10.1007/978-3-030-36683-4_39\nSterchi, M. & Wolf, M. (2017). Weighted Least Squares and Adaptive Least Squares: Further Empirical Evidence. In: Kreinovic, Vladik; Sriboonchitta, Songsak; Huynh, Van-Nam. Robustness in Econometrics. Cham: Springer, 135-167. https://doi.org/10.1007/978-3-319-50742-2_9\n\n\n\nAbstracts\n\nSterchi, M., Hilfiker, L. (2025). Evaluating Graph Neural Networks for Epidemic Source Detection: A Benchmark Study (Abstract). International School and Conference on Network Science, NetSci, (upcoming, June 2025). Poster Presentation.\nSterchi, M., Hilfiker, L., Gr√ºtter, R., Bernstein, A. (2023). Active learning for epidemic source detection (Extended Abstract). Book of Abstracts of the 12th International Conference on Complex Networks and their Applications, COMPLEX NETWORKS, 2023. Oral Presentation.\nHilfiker, L., Sterchi, M. (2021). Covid‚Äë19 superspreading: lessons from simulations on an empirical contact network (Extended Abstract). Book of Abstracts of the 10th International Conference on Complex Networks and their Applications, COMPLEX NETWORKS, 2021. Poster Presentation."
  },
  {
    "objectID": "blog/sbb_nw/index.html",
    "href": "blog/sbb_nw/index.html",
    "title": "SBB Network Analysis - Part 1",
    "section": "",
    "text": "Update 10.03.2025: I updated the analysis in this blog so that it runs on more recent data. More precisely, I use the train traffic data from March 5, 2025 to construct the network. Moreover, I now properly reference the source data and I have added a bunch of additional node attributes. The most interesting new node attributes are average passenger frequency data for all stations.\nFor quite some time I have been wondering if there are some interesting Swiss data that would serve as the basis for some fun network analysis. As a fan of public transportation and a long-time owner of a Swiss train pass (‚ÄúGA‚Äù), the answer should have been obvious much sooner: the Swiss railway network.\nI wanted to create a (static) network in which each node corresponds to a train station and each directed edge between any two nodes, A and B, means there is at least one train going nonstop from A to B. Ideally, the edge would also be attributed with some weight representing the importance of the edge (e.g., how many trains go nonstop from A to B on a given day).\nThe structure of this post is as follows. I will first introduce the three datasets that I used to create the network. I will then show how to load and preprocess each one of them and how to join them. Finally, I will present how to transform those data into a form that is suitable for network analysis. The following image shows a visualization of the network data resulting from this post.\n\n\n\nThe Swiss railway network with a geographic layout (created using Gephi).\n\n\nThis is the first part of a series that will cover all kinds of fun network analysis based on the Swiss railway network.\n\nData sources\nIt was not that obvious how a network with nodes and edges following the definitions given above could be constructed based on data from the Swiss Federal Railways (abbreviated by the German speakers in Switzerland as SBB). With some help from SBB Experts and the Open Data Plattform Mobility Switzerland, I finally found the right data.\nThe first and most important dataset is called Ist-Daten and, for a given day, contains all regular stops of all trains in Switzerland with their planned and effective arrival and departure times. From this data, we can infer all nonstop stretches of any train in Switzerland. A description of this dataset can be found here.\nNote that the ‚ÄúIst-Daten‚Äù not only contain the data for trains but also for all other public transport (buses, trams, and even boats). To keep things simple we will focus on the train network.\nThe second dataset is the Dienststellen-Daten which basically allows to add node attributes such as the geographic coordinates of a node (i.e., a train station). A description of this dataset can be found here.\nThe third dataset is a statistic of the average number of passengers boarding and alighting. It will allow us to add further interesting node attributes.\n\n\nLoad and preprocess ‚ÄúIst-Daten‚Äù\nHere, we will load and preprocess the ‚ÄúIst-Daten‚Äù from which we can derive the edges of our network. First, I import some Python libraries and print their version number for better reproducibility of this code.\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Check versions of libraries.\nprint(\"NumPy version:\", np.__version__)\nprint(\"Pandas version:\", pd.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n\nNumPy version: 1.26.4\nPandas version: 2.1.4\n\n\nLet‚Äôs now load the data. You can see in the filename that I downloaded the ‚ÄúIst-Daten‚Äù from the SBB portal for March 5, 2025. You can get the data for any day you want here.\n\n# Load the data\ndf = pd.read_csv('2025-03-05_istdaten.csv', sep=\";\", low_memory=False)\n\nTo get a feeling for the data, let‚Äôs check the number of rows and columns.\n\n# Number of rows and columns\nprint(df.shape)\n\n(2510290, 21)\n\n\nOk, it‚Äôs actually a pretty big dataset: it has over 2.5 million rows. That makes sense as this file contains every stop of every vehicle involved in public transport on a given day. Thus, every row corresponds to a stop of a train, bus, or any other vehicle of public transport.\n\n# Missing values per column\ndf.isna().sum()\n\nBETRIEBSTAG                  0\nFAHRT_BEZEICHNER             0\nBETREIBER_ID                 0\nBETREIBER_ABK                0\nBETREIBER_NAME               0\nPRODUKT_ID                  31\nLINIEN_ID                    0\nLINIEN_TEXT                  0\nUMLAUF_ID              1362009\nVERKEHRSMITTEL_TEXT          0\nZUSATZFAHRT_TF               0\nFAELLT_AUS_TF                0\nBPUIC                        0\nHALTESTELLEN_NAME       169114\nANKUNFTSZEIT            149415\nAN_PROGNOSE             156828\nAN_PROGNOSE_STATUS      149200\nABFAHRTSZEIT            149426\nAB_PROGNOSE             157273\nAB_PROGNOSE_STATUS      149115\nDURCHFAHRT_TF                0\ndtype: int64\n\n\nWe can see that some columns contain many missing values. The only one I worry about for now is the column PRODUKT_ID. If you look through these rows (I don‚Äôt show that here), you can see that they should all be of type ‚ÄúZug‚Äù (train). Thus, we impute accordingly:\n\n# Impute 'Zug'\ndf.loc[df[\"PRODUKT_ID\"].isna(), \"PRODUKT_ID\"] = 'Zug'\n\nThere are quite a few date-timestamp columns that are not yet in the proper format. Thus, we now convert them to datetime formats:\n\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'], format = \"%d.%m.%Y\")\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\n\nNow is a good time to finally have a look at the dataframe:\n\n# Let's look at first few rows\ndf.head()\n\n\n\n\n\n\n\n\nBETRIEBSTAG\nFAHRT_BEZEICHNER\nBETREIBER_ID\nBETREIBER_ABK\nBETREIBER_NAME\nPRODUKT_ID\nLINIEN_ID\nLINIEN_TEXT\nUMLAUF_ID\nVERKEHRSMITTEL_TEXT\nZUSATZFAHRT_TF\nFAELLT_AUS_TF\nBPUIC\nHALTESTELLEN_NAME\nANKUNFTSZEIT\nAN_PROGNOSE\nAN_PROGNOSE_STATUS\nABFAHRTSZEIT\nAB_PROGNOSE\nAB_PROGNOSE_STATUS\nDURCHFAHRT_TF\n\n\n\n\n0\n2025-03-05\n80:800631:17230:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17230\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2025-03-05 04:59:00\n2025-03-05 04:59:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n1\n2025-03-05\n80:800631:17233:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17233\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\nNaT\nNaT\nNaN\n2025-03-05 06:07:00\n2025-03-05 06:08:00\nPROGNOSE\nFalse\n\n\n2\n2025-03-05\n80:800631:17234:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17234\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2025-03-05 05:56:00\n2025-03-05 06:02:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n3\n2025-03-05\n80:800631:17235:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17235\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\nNaT\nNaT\nNaN\n2025-03-05 06:43:00\n2025-03-05 06:53:00\nPROGNOSE\nFalse\n\n\n4\n2025-03-05\n80:800631:17236:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17236\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2025-03-05 06:31:00\n2025-03-05 06:34:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n\n\n\n\n\nBut what do all these columns mean? I have browsed the metadata a bit and found the following explanations (that I hopefully accurately reproduce in English):\n\nBETRIEBSTAG: Simply the day on which the data were recorded.\nFAHRT_BEZEICHNER: This is some elaborate identifier in the format [UIC-Countrycode]:[GO-Number]:[VM-Number]:[Extended Reference].\nBETREIBER_ID: [UIC-Countrycode]:[GO-Number]. GO is short for ‚ÄúGesch√§ftsorganisation‚Äù. For foreign organizations it is not a GO-Number but a TU-Number with TU meaning ‚ÄúTransportunternehmen‚Äù. It is basically an ID for the company running that particular train.\nBETREIBER_ABK: The abbreviation for the company running the train.\nBETREIBER_NAME: The full name of the company running the train.\nPRODUKT_ID: Type of public transport.\nLINIEN_ID: The ID for the route of that train.\nLINIEN_TEXT: The public ID for the route of that train.\nUMLAUF_ID: An ID for a ‚ÄúUmlauf‚Äù which describes the period starting with the vehicle leaving the garage and ending with the vehicle being deposited back in the garage.\nZUSATZFAHRT_TF: Is true if it is an extraordinary (not usually scheduled) trip.\nFAELLT_AUS_TF: Is true if the trip is cancelled.\nBPUIC: The ID of the station.\nHALTESTELLEN_NAME: The name of the station.\nANKUNFTSZEIT: Planned time of arrival at the station.\nAN_PROGNOSE: Prediction of time of arrival at the station.\nAN_PROGNOSE_STATUS: Status of that prediction. Possible values are: ‚ÄúUNBEKANNT‚Äù, ‚Äúleer‚Äù, ‚ÄúPROGNOSE‚Äù, ‚ÄúGESCHAETZT‚Äù, ‚ÄúREAL‚Äù. If the value of that column is ‚ÄúREAL‚Äù, it means that the predicted time of arrival is the time the train actually arrived at the station.\nABFAHRTSZEIT, AB_PROGNOSE, AB_PROGNOSE_STATUS: Same definitions as for arrival but here for departure from the station.\nDURCHFAHRT_TF: Is true if the vehicle does not stop even if a stop was scheduled.\n\nLet‚Äôs now have a look at the values in the column PRODUKT_ID:\n\n# Look at PRODUKT_ID\ndf[\"PRODUKT_ID\"].value_counts()\n\nPRODUKT_ID\nBus            1965207\nTram            249408\nZug             163649\nBUS             124171\nMetro             4936\nZahnradbahn       1944\nSchiff             975\nName: count, dtype: int64\n\n\nWe can see that trains are only the third most frequent category in this data. However, as mentioned before, we want to keep it simple and now reduce the dataset to only trains.\n\n# First we reduce to only trains\ndf = df[df['PRODUKT_ID'] == \"Zug\"]\n\nIn a next step, we remove all rows where the corresponding train has been cancelled.\n\n# Filter out all entries with FAELLT_AUS_TF == True\ndf = df[df['FAELLT_AUS_TF'] == False]\n\nWhen I was doing some analysis with a first version of that network, I noticed that it contains edges in both directions between Klosters Selfranga and Sagliains. Upon further inspection I found out that this corresponds to a car shuttle train. All such connections are marked with ‚ÄúATZ‚Äù in the variable LINIEN_TEXT which probably stands for ‚ÄúAutozug‚Äù. We remove these connections:\n\n# Filter out all entries with LINIEN_TEXT == \"ATZ\"\ndf = df[df['LINIEN_TEXT'] != \"ATZ\"]\n\nAnother problem I spotted was that certain stations are split up into two based on which company is running what part of the station. I actually computed geodesic distances between any pair of stations to identify the problematic ones. I decided to manually change three cases. In the following code chunk you can see that I merge Brig and Brig Bahnhofplatz, Lugano and Lugano FLP, as well as Locarno and Locarno FART.\n\n# Merge stations in Brig, Lugano, Locarno\ndf.loc[df['HALTESTELLEN_NAME'] == \"Brig Bahnhofplatz\", \"BPUIC\"] = 8501609\ndf.loc[df['HALTESTELLEN_NAME'] == \"Lugano FLP\", \"BPUIC\"] = 8505300\ndf.loc[df['HALTESTELLEN_NAME'] == \"Locarno FART\", \"BPUIC\"] = 8505400\n\ndf.loc[df['HALTESTELLEN_NAME'] == \"Brig Bahnhofplatz\", \"HALTESTELLEN_NAME\"] = \"Brig\"\ndf.loc[df['HALTESTELLEN_NAME'] == \"Lugano FLP\", \"HALTESTELLEN_NAME\"] = \"Lugano\"\ndf.loc[df['HALTESTELLEN_NAME'] == \"Locarno FART\", \"HALTESTELLEN_NAME\"] = \"Locarno\"\n\nLet‚Äôs explore the data a bit more before we move to the second dataset. Let‚Äôs check out the most frequent values that occur in the column BETREIBER_NAME:\n\n# Look at BETREIBER_NAME\ndf[\"BETREIBER_NAME\"].value_counts().head()\n\nBETREIBER_NAME\nSchweizerische Bundesbahnen SBB    63850\nBLS AG (bls)                       16256\nTHURBO                             13017\nAargau Verkehr AG                   7131\nSchweizerische S√ºdostbahn (sob)     6083\nName: count, dtype: int64\n\n\nAs expected, SBB is the company serving the largest number of stations. What about the column VERKEHRSMITTEL_TEXT?\n\n# Look at VERKEHRSMITTEL_TEXT\ndf[\"VERKEHRSMITTEL_TEXT\"].value_counts().head()\n\nVERKEHRSMITTEL_TEXT\nS     103331\nR      34435\nRE      9730\nIR      7532\nIC      3059\nName: count, dtype: int64\n\n\nWe can see that the most frequent type of trains are S-Bahns (S). Finally, let‚Äôs check the most frequent train stations that occur in the data:\n\n# Look at HALTESTELLEN_NAME\ndf[\"HALTESTELLEN_NAME\"].value_counts().head()\n\nHALTESTELLEN_NAME\nZ√ºrich HB          2239\nBern               1709\nWinterthur          955\nZ√ºrich Oerlikon     919\nLuzern              843\nName: count, dtype: int64\n\n\nUnsurprisingly, Z√ºrich and Bern are the most frequent values occuring in the data.\n\n\nLoad and preprocess ‚ÄúDienststellen-Daten‚Äù\nFortunately, we can go through the second dataset a bit more quickly. We again start by loading it and checking the dimensions of the dataframe.\n\n# Load the data\nds = pd.read_csv('actual_date-swiss-only-service_point-2025-03-06.csv', sep = \";\", low_memory = False)\n\n# Number of rows and columns\nprint(ds.shape)\n\n(55308, 55)\n\n\nThe data contains a column validTo that allows us to filter out all stations that are not valid anymore (closed down?). We check the values that appear in this column and see that all stations should be valid as of March 6, 2025. This is no surprise as we use the dataset of currently valid stations.\n\n# Check 'validTo' values.\nds['validTo'].unique()\n\narray(['9999-12-31', '2025-12-13', '2025-04-06', '2026-12-12',\n       '2025-08-29', '2027-12-11', '2025-03-16', '2025-05-30',\n       '2099-12-31', '2028-12-09', '2025-09-30', '2030-12-14',\n       '2025-06-30', '2050-12-31', '2029-12-09', '2025-08-31',\n       '2025-03-31', '2025-04-12', '2025-05-16', '2025-03-06'],\n      dtype=object)\n\n\nLet‚Äôs also quickly make sure that we have unique rows (based on ‚Äònumber‚Äô).\n\n# Is the number of unique 'number' (= BPUIC) values equal to the number of rows?\nlen(pd.unique(ds['number'])) == ds.shape[0]\n\nTrue\n\n\nFinally, we keep only the columns we need (identifier, official name, and geo coordinates).\n\n# Keep only the relevant columns\nds = ds[[\"number\",\"designationOfficial\",\"cantonName\",\"municipalityName\",\"businessOrganisationDescriptionEn\",\"wgs84East\",\"wgs84North\",\"height\"]]\n\n# Show first few rows\nds.head()\n\n\n\n\n\n\n\n\nnumber\ndesignationOfficial\ncantonName\nmunicipalityName\nbusinessOrganisationDescriptionEn\nwgs84East\nwgs84North\nheight\n\n\n\n\n0\n1322001\nAntronapiana\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.113620\n46.060120\n0.0\n\n\n1\n1322002\nAnzola d'Ossola\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.345715\n45.989869\n0.0\n\n\n2\n1322003\nBaceno\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.319256\n46.261501\n0.0\n\n\n3\n1322012\nCastiglione\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.214886\n46.020588\n0.0\n\n\n4\n1322013\nCeppo Morelli\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.069922\n45.971036\n0.0\n\n\n\n\n\n\n\n\n\nLoad and preprocess average traffic data\nThis part is also fairly easy. We load the data and check the dimensions, as always.\n\n# Load the data\nds_freq = pd.read_csv('t01x-sbb-cff-ffs-frequentia-2023.csv', sep = \";\", low_memory = False)\n\n# Number of rows and columns\nprint(ds_freq.shape)\n\n(3479, 14)\n\n\nIf you actually have a look at the data, you see that many stations have several measurements made at different times (and the times of measurements are identified by Jahr_Annee_Anno). We only want to keep the most recent measurements for every station:\n\n# For every station, we only keep the most recent measurements.\nds_freq = ds_freq.loc[ds_freq.groupby('UIC')['Jahr_Annee_Anno'].idxmax()]\n\nChecking the data types of all columns reveals that there is still a problem with the measurement columns DTV_TJM_TGM, DWV_TMJO_TFM, and DNWV_TMJNO_TMGNL. They are currently of type object because they contain the thousand separator ‚Äô. We thus remove all instances of this characters and transform these columns to integers.\n\n# Data types of columns\nds_freq.dtypes\n\n# Remove thousand separator and make integers out of it.\nds_freq['DTV_TJM_TGM'] = ds_freq['DTV_TJM_TGM'].str.replace('‚Äô', '').astype(int)\nds_freq['DWV_TMJO_TFM'] = ds_freq['DWV_TMJO_TFM'].str.replace('‚Äô', '').astype(int)\nds_freq['DNWV_TMJNO_TMGNL'] = ds_freq['DNWV_TMJNO_TMGNL'].str.replace('‚Äô', '').astype(int)\n\nFinally, we keep only the relevant columns.\n\n# Keep only the relevant columns\nds_freq = ds_freq[[\"UIC\",\"DTV_TJM_TGM\",\"DWV_TMJO_TFM\",\"DNWV_TMJNO_TMGNL\"]]\n\n\n# Show first few rows\nds_freq.head()\n\n\n\n\n\n\n\n\nUIC\nDTV_TJM_TGM\nDWV_TMJO_TFM\nDNWV_TMJNO_TMGNL\n\n\n\n\n411\n8500010\n98600\n105900\n81900\n\n\n423\n8500016\n90\n100\n60\n\n\n2024\n8500020\n5700\n7000\n2800\n\n\n2294\n8500021\n8500\n9900\n5200\n\n\n1072\n8500022\n3600\n4100\n2300\n\n\n\n\n\n\n\nBut what exactly are these three measurement variables? The source dataset provides the following definitions:\n\nDTV_TJM_TGM: ‚ÄúAverage daily traffic (Monday to Sunday).‚Äù\nDWV_TMJO_TFM: ‚ÄúAverage traffic on weekdays (Monday to Friday).‚Äù\nDNWV_TMJNO_TMGNL: ‚ÄúAverage non-work day traffic (Saturdays, Sundays and public holidays).‚Äù\n\nIt is further mentioned that all passengers boarding and exiting the trains are counted. That also means that passengers who switch trains are counted twice. For larger stations, the data may not cover all trains arriving and departing at the corresponding station. For example, the numbers for Bern do not include the traffic generated by the regional train company RBS.\n\n\nCombine the three datasets\nWe first merge the traffic data to the ‚ÄúDienststellen-Daten‚Äù:\n\n# Join to 'ds'\nds = pd.merge(ds, ds_freq, left_on = 'number', right_on = 'UIC', how = 'left')\n\n# Drop 'UIC'\nds = ds.drop('UIC', axis=1)\n\n# Better column names\nds.columns = ['BPUIC','STATION_NAME','CANTON','MUNICIPALITY','COMPANY',\n              'LONGITUDE','LATITUDE','ELEVATION','AVG_DAILY_TRAFFIC',\n              'AVG_DAILY_TRAFFIC_WEEKDAYS','AVG_DAILY_TRAFFIC_WEEKENDS']\n\nThen we merge the ‚ÄúDienststellen-Daten‚Äù to the ‚ÄúIst-Daten‚Äù via the BPUIC variable:\n\n# Left-join with station names and coordinates\ndf = pd.merge(df, ds, on = 'BPUIC', how = 'left')\n\nUnfortunately, there are some rows (18) for which HALTESTELLEN_NAME is missing. But fortunately, we know which stations are affected based on the STATION_NAME column that we have just merged from ds.\n\n# There are 18 missing values for 'HALTESTELLEN_NAME' which we impute from 'STATION_NAME'.\ndf.loc[df['HALTESTELLEN_NAME'].isna(), \"HALTESTELLEN_NAME\"] = df.loc[df['HALTESTELLEN_NAME'].isna(), \"STATION_NAME\"]\n\nNow, we are finally ready to start extracting the network from this data!\n\n\nConvert it to a network\nAs I mentioned several times, every row corresponds to a stop of a train at a train station. One train ride from some initial station to some end station (called ‚ÄúFahrt‚Äù in German) then typically consists of several stops along the way. However, there are some ‚ÄúFahrten‚Äù with only one entry. Presumably these are mostly foreign trains that have their final destination at some border station. I decided to remove those entries:\n\n# First group by FAHRT_BEZEICHNER and then filter out all groups with only one entry\n# It's mostly trains that stop at a place at the border (I think)\ndf_filtered = df.groupby('FAHRT_BEZEICHNER').filter(lambda g: len(g) &gt; 1)\n\n# How many rows do we loose with that?\nprint(df.shape[0] - df_filtered.shape[0])\n\n420\n\n\nThis preprocessing step removes 420 rows.\nNow we group the rows by FAHRT_BEZEICHNER so that each group is one ‚ÄúFahrt‚Äù. In every group we sort the stops along the way in an ascending order of the departure time.\n\n# Function to sort entries within a group in ascending order of ABFAHRTSZEIT\ndef sort_data(group):\n    return group.sort_values('ABFAHRTSZEIT', ascending = True)\n\n# Sort for each group\ndf_sorted = df_filtered.groupby('FAHRT_BEZEICHNER', group_keys=True).apply(sort_data)\n\nLet‚Äôs have a look at one ‚ÄúFahrt‚Äù to get a better idea:\n\n# Look at one example Fahrt\ndf_sorted.loc[['85:22:1083:000'],['BETREIBER_NAME','LINIEN_TEXT','HALTESTELLEN_NAME','ABFAHRTSZEIT']]\n\n\n\n\n\n\n\n\n\nBETREIBER_NAME\nLINIEN_TEXT\nHALTESTELLEN_NAME\nABFAHRTSZEIT\n\n\nFAHRT_BEZEICHNER\n\n\n\n\n\n\n\n\n\n85:22:1083:000\n64346\nAppenzeller Bahnen (ab)\nS23\nGossau SG\n2025-03-05 08:21:00\n\n\n64347\nAppenzeller Bahnen (ab)\nS23\nHerisau\n2025-03-05 08:28:00\n\n\n64348\nAppenzeller Bahnen (ab)\nS23\nHerisau Wilen\n2025-03-05 08:30:00\n\n\n64349\nAppenzeller Bahnen (ab)\nS23\nWaldstatt\n2025-03-05 08:34:00\n\n\n64350\nAppenzeller Bahnen (ab)\nS23\nZ√ºrchersm√ºhle\n2025-03-05 08:39:00\n\n\n64351\nAppenzeller Bahnen (ab)\nS23\nUrn√§sch\n2025-03-05 08:43:00\n\n\n64352\nAppenzeller Bahnen (ab)\nS23\nJakobsbad\n2025-03-05 08:48:00\n\n\n64353\nAppenzeller Bahnen (ab)\nS23\nGonten\n2025-03-05 08:50:00\n\n\n64354\nAppenzeller Bahnen (ab)\nS23\nGontenbad\n2025-03-05 08:52:00\n\n\n64355\nAppenzeller Bahnen (ab)\nS23\nAppenzell\nNaT\n\n\n\n\n\n\n\nThis is a train that goes from Gossau to Appenzell with many stops in-between. In Appenzell the ABFAHRTSZEIT is missing as that ‚ÄúFahrt‚Äù ends there (the train will most likely go back in the other direction, but that will be a new ‚ÄúFahrt‚Äù).\nWe now have enough knowledge about the data that we can extract the edges in a for loop. Basically, what we do is to loop over the rows of a given ‚ÄúFahrt‚Äù, starting with the second row and extracting the edges as\n(previous station, current station, travel time between stations).\nThe Python code for this looks as follows:\n\n# Empty list\nedgelist = []\n\n# Variables to store previous row and its index\nprev_row = None\nprev_idx = None\n\n# Loop over rows of dataframe\nfor i, row in df_sorted.iterrows():\n    # Only start with second row\n    # Only if the two rows belong to the same Fahrt\n    if prev_idx is not None and prev_idx == i[0]:\n        # Add edge to edgelist assuming it's a directed edge\n        edgelist.append((prev_row['STATION_NAME'], \n                         row['STATION_NAME'], \n                         (row['ANKUNFTSZEIT'] - prev_row['ABFAHRTSZEIT']).total_seconds() / 60))\n    # Set current row and row index to previous ones\n    prev_idx = i[0]\n    prev_row = row\n\nTo get a better idea, let‚Äôs have a look at the first list element:\n\n# First list element\nedgelist[0]\n\n('Z√ºrich HB', 'Basel SBB', 54.0)\n\n\nWe are still not quite done yet. The problem is that the edgelist contains many duplicated entries as, for example, the stretch Z√ºrich HB - Basel SBB is served by many different trains on a given day.\nWhat we want to do is to go through all possible edges and sum up the number of times they occur. In addition, we would like to average the travel time between a given pair of stations over all trips between the two stations. The following code does exactly that and saves the result in the form of a dictionary.\n\n# Empty dict\nedges = {}\n\n# Loop over elements in edgelist\nfor i in edgelist:\n    # Create key\n    key = (i[0], i[1])\n    # Get previous entries in dict (if there are any)\n    prev = edges.get(key, (0, 0))\n    # Update values in dict\n    edges[key] = (prev[0] + 1, prev[1] + i[2])\n\n# Divide summed up travel times by number of trips\nedges = {k: (v[0], round(v[1]/v[0], 2)) for k, v in edges.items()}\n\nLet‚Äôs look at the entry for the stretch between Z√ºrich and Basel again:\n\n# Look at some element in dict\nedges[('Z√ºrich HB', 'Basel SBB')]\n\n(36, 54.0)\n\n\nThere are 36 trips between these two stations (in this direction) and they take 54 minutes on average.\nAnother issue that I spotted when I was trying to visualize this network was two edges between Basel Bad Bf and Schaffhausen. When I consulted the SBB timetable I saw that these supposedly nonstop connections actually stop at quite a few stations in Germany. But because these stops are in Germany they do not appear in the data. As a conclusion of all this, I decided to remove these two edges:\n\n# Remove the two edges between Basel Bad Bf and Schaffhausen\ndel edges[('Basel Bad Bf', 'Schaffhausen')]\ndel edges[('Schaffhausen', 'Basel Bad Bf')]\n\nWe are now ready to create the final node list (and export it). First, we reduce ds to the train stations that actually appear in the edges (it still contains many bus and tram stops and other things).\n\n# Set of stations that appear in edgelist\nstations_in_edgelist = set(sum(list(edges.keys()), ()))\n\n# Reduces nodes dataframe to only places in edgelist\nnodes = ds[ds['STATION_NAME'].isin(stations_in_edgelist)]\n\nSecond, we quickly check the number of missing values again.\n\n# Missing values per column\nnodes.isna().sum()\n\nBPUIC                           0\nSTATION_NAME                    0\nCANTON                         21\nMUNICIPALITY                   21\nCOMPANY                         0\nLONGITUDE                       0\nLATITUDE                        0\nELEVATION                       1\nAVG_DAILY_TRAFFIC             496\nAVG_DAILY_TRAFFIC_WEEKDAYS    496\nAVG_DAILY_TRAFFIC_WEEKENDS    496\ndtype: int64\n\n\nThere are still some issues here. The one we can solve is the missing elevation. The station Tirano (in Italy) has no value for this column. We simply impute manually (Tirano‚Äôs elevation is approximately 441m).\n\n# Impute missing elevation for Tirano\nnodes.loc[nodes['STATION_NAME'] == \"Tirano\", \"ELEVATION\"] = 441\n\nThe missing values for CANTON and MUNICIPALITY concern municipalities abroad (in Germany and Italy mostly). The 500 missing values in the traffic columns are stations are run by smaller companies or stations abroad. There is nothing we can do about all these missing values.\n\n# Have a look\nnodes.head()\n\n# Export node list\n# nodes.sort_values(\"BPUIC\").to_csv(\"nodelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC\nSTATION_NAME\nCANTON\nMUNICIPALITY\nCOMPANY\nLONGITUDE\nLATITUDE\nELEVATION\nAVG_DAILY_TRAFFIC\nAVG_DAILY_TRAFFIC_WEEKDAYS\nAVG_DAILY_TRAFFIC_WEEKENDS\n\n\n\n\n12683\n8500100\nTavannes\nBern\nTavannes\nSwiss Federal Railways SBB\n7.201645\n47.219845\n754.17\n1400.0\n1600.0\n810.0\n\n\n12684\n8500121\nCourfaivre\nJura\nHaute-Sorne\nSwiss Federal Railways SBB\n7.291166\n47.335083\n450.99\n420.0\n480.0\n280.0\n\n\n12685\n8500103\nSorvilier\nBern\nSorvilier\nSwiss Federal Railways SBB\n7.305794\n47.239354\n681.07\n60.0\n70.0\n49.0\n\n\n12688\n8500120\nCourt√©telle\nJura\nCourt√©telle\nSwiss Federal Railways SBB\n7.317943\n47.342829\n436.90\n840.0\n970.0\n550.0\n\n\n12689\n8500102\nMalleray-B√©vilard\nBern\nValbirse\nSwiss Federal Railways SBB\n7.275946\n47.238714\n698.18\n630.0\n780.0\n280.0\n\n\n\n\n\n\n\nBefore we export the edges, we change the station names in the edgelist to the BPUIC to make the edges more compact. Then we transform the dictionary into a dataframe which can finally be exported.\n\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[k[0]], node_dict[k[1]], v[0], v[1]] for k,v in edges.items()]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','NUM_CONNECTIONS','AVG_DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist_SoSto.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC1\nBPUIC2\nNUM_CONNECTIONS\nAVG_DURATION\n\n\n\n\n0\n8503000\n8500010\n36\n54.00\n\n\n1\n8500010\n8500090\n67\n6.07\n\n\n2\n8500090\n8500010\n67\n6.39\n\n\n3\n8500010\n8503000\n39\n57.87\n\n\n4\n8506286\n8506271\n34\n3.00\n\n\n\n\n\n\n\nFeel free to download the final results: Nodelist (CSV) and Edgelist (CSV).\nThe title image has been created by Wikimedia user JoachimKohler-HB and is licensed under Creative Commons."
  },
  {
    "objectID": "blog/kmeans_imgseg/index.html",
    "href": "blog/kmeans_imgseg/index.html",
    "title": "k-Means for Image Segmentation",
    "section": "",
    "text": "The other day, I had a great session with my Master‚Äôs students exploring Clustering. We focused primarily on the k-Means algorithm, and I was reminded of just how cool that algorithm is.\nOne of the reasons k-Means is so interesting is that it‚Äôs easy to understand and visualize (well, at least in 2D and with numeric-only features). But beyond that, its versatility is what truly makes it stand out. k-Means isn‚Äôt just for clustering observations into similar groups, it has a wide range of applications.\nFor instance, we can use k-Means for dimensionality reduction, feature engineering, selecting which observations to label in a semi-supervised setting, and even image segmentation. If you‚Äôre interested in exploring more, I highly recommend Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur√©lien G√©ron‚Äîan excellent resource showcasing the many ways k-Means can be applied.\nHere, I‚Äôd like to quickly demonstrate how to use k-Means for image segmentation on your own portrait picture using R. This is just fun, nothing too terribly useful.\n\nLoading picture\nWe start by loading my FHNW portrait picture (FHNW is my current employer). For this we use the handy jpeg package.\n\nlibrary(jpeg)\n\n# Load image\nX &lt;- readJPEG(\"Martin-Sterchi.jpg\", native = FALSE)\n\n# Dimensions of X\ndim(X)\n\n[1] 512 512   3\n\n\nAs you can see, X is a three-dimensional array. You can imagine it as three stacked \\(512\\times 512\\) matrices, one per Red-Green-Blue (RGB) color channel.\nNext, we store the original dimensions of X and then transform the array into a large matrix, in which every row represents a pixel.\n\n# Store dimensions of original image\ndim_original &lt;- dim(X)\n\n# Transform 3D array to matrix\ndim(X) &lt;- c(dim_original[1] * dim_original[2], 3)\n\n# New dimensions of X\ndim(X)\n\n[1] 262144      3\n\n\nThe picture is now basically represented as a dataset of \\(n=512\\cdot 512=262'144\\) observations (every observation is a pixel) and \\(p=3\\) columns (the three color channels).\n\n\nRunning k-Means\nThe idea of this type of image segmentation is simple: we let k-Means cluster the observations (pixels) into \\(k\\) groups of similarly colored pixels. Or in other words, pixels with similar RGB color values will be grouped into the same color cluster.\nFor now, we let k-Means create \\(k=2\\) clusters.\n\n# Set k (number of clusters)\nk &lt;- 2\n\n# Run k-means\nkm.out &lt;- kmeans(X, k, nstart = 20)\n\n# Output k-means\nprint(km.out$centers)\n\n       [,1]      [,2]      [,3]\n1 0.8660900 0.8047682 0.7698332\n2 0.4066233 0.2899175 0.2120528\n\n\nThe output above are the two cluster centers (or centroids). These two centroids are obviously also colors. Let‚Äôs visualize the colors:\n\n# Two square plots next to each other\npar(mfrow = c(1, 2), pty = \"s\")\n\n# Plot the two centroid colors using rgb()\nplot(1, 1, col = rgb(0.8660900, 0.8047682, 0.7698332), pch = 15, cex = 30, ann = FALSE, axes = FALSE)\nplot(1, 1, col = rgb(0.4066233, 0.2899175, 0.2120528), pch = 15, cex = 30, ann = FALSE, axes = FALSE)\n\n\n\n\n\n\n\n\nThe trick is now to replace the colors of all pixels belonging to a given cluster by the cluster‚Äôs centroid color. This will lead to a segmentation of the image into (in our case) two areas of different color.\nIn R, we can create the segmented image array as follows:\n\n# Create segmented image\nX_segmented &lt;- km.out$centers[km.out$cluster, ]\n\nkm.out$cluster is a vector of length 262‚Äô144 defining which pixel belongs to which cluster.\nFinally, let‚Äôs transform the segmented image back to its orignal dimensions.\n\n# Reshape to original dimensions\ndim(X_segmented) &lt;- dim_original\n\n\n\nPlotting the image\nWe can now plot the resulting image:\n\n# Plot should be square\npar(pty = \"s\")\n\n# Empty plot\nplot(0:1, 0:1, type = \"n\", ann = FALSE, axes = FALSE)\n\n# Add image\nrasterImage(X_segmented, 0, 0, 1, 1)\n\n\n\n\n\n\n\n\nIf you want to export the segmented portrait picture to your working directory, you can run the following command:\n\n# Export image\nwriteJPEG(X_segmented, target = \"output.jpg\", quality = 0.7)\n\n\n\n10 Shades of me\nFinally, let‚Äôs plot the segmented images for values of \\(k=1,\\dots,10\\), using a for loop:\n\n# Some plot options\npar(mfrow = c(2, 5), oma = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), pty = \"s\")\n\n# Loop over k = 1, ..., 10\nfor (k in 1:10) {\n  # EMpty plot\n  plot(0:1, 0:1, type = \"n\", ann = FALSE, axes = FALSE)\n  # k-Means\n  km.out &lt;- kmeans(X, k, nstart = 3)\n  # Segmented image\n  X_segmented &lt;- km.out$centers[km.out$cluster, ]\n  # Reshape segmented image\n  dim(X_segmented) &lt;- dim_original\n  # Plot the image\n  rasterImage(X_segmented, 0, 0, 1, 1)\n}\n\n\n\n\n\n\n\n\nFor \\(k=1\\), only one cluster is built and its centroid color is the average color in the image. In that case, the image obviously contains no contours.\nI hope you enjoyed that brief post and maybe you can use it to create your own stylized portrait picture."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "SBB Network Analysis - Part 3\n\n\n\nNetworks\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nSBB Network Analysis - Part 2\n\n\n\nNetworks\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nk-Means for Image Segmentation\n\n\n\nData Science\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nSBB Network Analysis - Part 1\n\n\n\nNetworks\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nMartin Sterchi\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news/blog/index.html",
    "href": "news/blog/index.html",
    "title": "First blogpost",
    "section": "",
    "text": "I‚Äôve started a blog right here on this website (see here). I know the internet is already overflowing with blogs and other content, and the world certainly wasn‚Äôt waiting for yet another person eager to share their thoughts.\nBut honestly, this blog is, above all, a creative outlet for me. Writing my first post was a lot of fun, and if others find it interesting‚Äîgreat! If not, no hard feelings. üòâ\nThat said, this first post took a surprising amount of work (a full workday, if we‚Äôre being honest‚Äîdon‚Äôt tell my boss). So, I won‚Äôt be posting on a fixed schedule. Instead, whenever I have an interesting analysis that isn‚Äôt quite enough for a formal publication, a cool data visualization, or a neat tutorial on a model or algorithm‚Äîand, crucially, some spare time‚ÄîI‚Äôll share it here.\nSpeaking of which, my first post explores how to use Swiss public transport data to create a directed network. I plan to turn this into a series of blogposts, applying different network science concepts to that network.\nThe title image has been created by Wikimedia user Cortega9 and is licensed under Creative Commons."
  },
  {
    "objectID": "news/netsci25/index.html",
    "href": "news/netsci25/index.html",
    "title": "NetSci 2025, here we come",
    "section": "",
    "text": "My colleague Lorenz Hilfiker and I have been accepted for a poster presentation at NetSci2025! We look forward to traveling to Maastricht (NL) this June to share our work with experts in the field.\nOur research explores the potential of Graph Neural Networks (GNNs) for epidemic source detection‚Äîidentifying the origin of an outbreak given a contact network and the epidemic states of all nodes.\nOur (still preliminary) findings suggest that GNNs may not yet live up to their promise, as they are consistently outperformed by more traditional source detection methods. However, we have only explored a small region of the GNN design space so far, so it is too early to rule them out as a valuable tool for this problem.\nIf you‚Äôre interested, check out our abstract below‚Äîand stay tuned for a full paper on this topic!"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "News",
    "section": "",
    "text": "NetSci 2025\n\n\n\nConferences\n\n\n\n\n\n\n\n\n\nJul 11, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nFirst blogpost\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nNetSci 2025, here we come\n\n\n\nConferences\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nNew SNSF project\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nMartin Sterchi\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a lecturer at the School of Business at the University of Applied Sciences and Arts Northwestern Switzerland, or short FHNW. Additionally, I hold a part-time Postdoctoral position at the Chair of Psychological Methods, Evaluation and Statistics at the University of Zurich, led by Prof.¬†Dr.¬†Carolin Strobl.\nI have a background in economics, having studied at the Universities of Fribourg and Zurich, and hold a PhD in Computer Science from the University of Zurich. My doctoral research, titled Computational Approaches to Epidemic Prevention on Contact Networks, focused on developing methods to mitigate the spread of a disease when at least partial knowledge of the underlying contact network is available.\nAt FHNW I teach the following modules (more info):\n\nMachine Learning: This course is designed for business students, emphasizing the practical application of ML to real-world problems. We primarily use the tidymodels framework, building on students‚Äô prior knowledge of R.\nApplied Data Science: Currently, this week-long intensive course focuses on teaching students data visualization skills in R using ggplot2.\nBusiness Analytics: In this core module, I introduce students to programming in R and guide them through the fundamentals of linear regression.\n\nMy research interests are as follows:\n\nDynamical processes on networks: My primary research interest lies in epidemic processes, which was the focus of my PhD and remains a central theme in my current work.\nMonte Carlo simulations: For many problems, analytical solutions do not exist, and as a computer scientist, I am not reluctant to use simulations to find approximate solutions.\nGraph Neural Networks: Network science has not escaped the Deep Learning revolution‚Äîand naturally, GNNs have caught my attention. Specifically, I am curious if GNNs can help address the challenge of source detection.\nExplainability: Recently, I have begun exploring model explainability, primarily through visualizations‚Äîanother passion of mine. I plan to further investigate this topic, particularly in the context of models on networks.\n\nI enjoy implementing Machine Learning algorithms from scratch to deepen my understanding at the implementation level. Moreover, I can spend hours refining data visualizations‚Äîwhether in base R, ggplot2, or matplotlib‚Äîstriving for a balance of aesthetics, clarity, and engagement.\nI live in Switzerland together with my partner and our two adorable‚Äîyet incredibly demanding‚Äîcats.\n\n\n\nTwo bundles of joy"
  },
  {
    "objectID": "news/snf_epi/index.html",
    "href": "news/snf_epi/index.html",
    "title": "New SNSF project",
    "section": "",
    "text": "In March 2024, my colleagues Lorenz Hilfiker, Matthias Templ, and I submitted a project proposal to the Swiss National Science Foundation‚Äôs (SNSF) second Health and Wellbeing Call.\nIn November 2024, we received the exciting news that our project has been awarded funding! With a grant of CHF 391,938, we will be able to hire a full-time PhD student to support our research. Additionally, Lorenz will join Matthias and me at Fachhochschule Nordwestschweiz (FHNW) in 2025, starting in a part-time position.\nOver the next four years, our team‚Äîincluding the PhD student‚Äîwill work on developing new methods to trace the origins of epidemic outbreaks in contact networks. While researchers have been exploring epidemic source detection for about 15 years, many existing studies focus on simplified models and small networks, limiting their practical applications. Our project aims to address these challenges by pursuing two key goals:\n\nScalability: Developing methods that can be applied to large-scale contact networks.\nReal-world applicability: Designing techniques that account for uncertainty in key parameters, making them more suitable for practical use.\n\nOur research is not limited to human pandemics‚Äîit also has significant applications in the agricultural and livestock industries, where regulatory requirements often ensure the availability of contact data.\nUltimately, we hope our methods will become valuable tools for preventing future outbreaks, whether in human or veterinary health. We are thrilled to embark on this journey and look forward to the discoveries ahead!"
  },
  {
    "objectID": "news/netsci25_poster/index.html",
    "href": "news/netsci25_poster/index.html",
    "title": "NetSci 2025",
    "section": "",
    "text": "My colleague Lorenz Hilfiker and I attended this year‚Äôs NetSci conference in Maastricht, NL. We were pleased to present our preliminary work on GNNs for the source detection problem as a poster and to receive valuable feedback from the NetSci community.\n\nBesides enjoying the conference, we also had a chance to explore Maastricht a bit. Here are some impressions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeel free to download our poster: Poster (PDF). We hope to publish a paper soon."
  },
  {
    "objectID": "code/index.html",
    "href": "code/index.html",
    "title": "Code",
    "section": "",
    "text": "Shiny Apps\nAs part of my Postdoc employment with Prof.¬†Dr.¬†Carolin Strobl at the University of Zurich, I developed two Shiny apps to help psychology students understand the assumptions of (simple) linear regression models. In the process, I explored base R‚Äôs plotting capabilities and learned a great deal about how to make visually appealing plots using only base R.\nThe first app visually demonstrates how the distribution of predictors (\\(x_i\\)) and error terms (\\(\\epsilon_i\\)) influence both the marginal and conditional distributions of the dependent variable (\\(y_i\\)). The key takeaway we attempt to convey to students is that linear regression models impose distributional assumptions on the error terms (\\(\\epsilon_i\\)), not directly on \\(y_i\\).\n\n\n\nKey figure in the first app\n\n\nThe second app is more comprehensive and illustrates the five key assumptions of linear regression, as typically taught in statistics courses for psychology students (at least at the University of Zurich). The core idea is to explore each assumption by:\n\nDemonstrating how it may be violated and what that visually looks like.\nShowing the consequences of a violation.\nPresenting possible remedies to address the violation of the assumption.\n\nFor example, consider the assumption of homoscedasticity. The app first provides a visual representation of how a violation appears in typical regression plots. A small simulation experiment then reveals that, when this assumption is violated, the coverage probability of 95% confidence intervals drops to 85%. Finally, two potential remedies are introduced: robust standard errors and weighted least squares. The app allows the user to test the effect of those remedies.\n\n\n\nResiduals vs.¬†fitted values in case of heteroscedasticity"
  },
  {
    "objectID": "blog/sbb_nw2/index.html",
    "href": "blog/sbb_nw2/index.html",
    "title": "SBB Network Analysis - Part 2",
    "section": "",
    "text": "In Part 1 of this series on the Swiss train network, I demonstrated how to construct a directed network where nodes represent stations, and a directed edge exists whenever at least one nonstop train connection links two stations.\nFor some time, I believed this was the most intuitive graph representation for this context. However, after reading an insightful 2006 paper by Maciej Kurant and Patrick Thiran, I discovered that public transport networks can be represented in (at least) three distinct ways. The graph representation I introduced in Part 1 aligns with what they call the space-of-stops representation.\nYet, depending on the specific questions being asked, two other graph representations can also be useful. In the space-of-changes representation proposed by Kurant and Thiran (2006), an edge exists between any two stations connected by a train on a given ‚ÄúFahrt‚Äù, even if the train makes stops at other stations in between.\nThe third representation, space-of-stations, includes an undirected edge between two stations only if they are directly connected by railway tracks, with no other station in between. This approach offers a more infrastructure-focused perspective on the network.\nCrucially, all three representations share the same set of nodes‚Äînamely, all active train stations. What differs is how the edges are defined.\nKurant and Thiran (2006) also highlight how the shortest path length is interpreted differently in each representation:\n\nspace-of-stops: The number of train stops on a journey between two stations.\nspace-of-changes: The number of times a traveler must change trains between two stations.\nspace-of-stations: The number of stations passed through between two stations.\n\nLastly, they point out an important subgraph relationship among these representations: space-of-stations is a subgraph of space-of-stops, which in turn is a subgraph of space-of-changes.\nAs always, we begin the practical part with loading the libraries we are going to use.\n\nimport geopy.distance\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter, defaultdict\n\n# Check versions of libraries.\nprint(\"Pandas version:\", pd.__version__)\nprint(\"Numpy version:\", np.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n\nPandas version: 2.1.4\nNumpy version: 1.26.4\n\n\nWe first present how the space-of-changes representation can be extracted. After that we show one way of finding the edges for the space-of-stations representation.\n\nSpace-of-changes\nWe start by importing the already processed ‚ÄúIst-Daten‚Äù from Part 1. Since we load them from a CSV file we have to transform all date-time information into the Pandas datetime format.\n\n# Load the processed IST-DATEN.\ndf = pd.read_csv('ist-daten.csv', sep=\";\", low_memory=False)\n\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'])\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'])\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'])\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'])\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'])\n\nNext comes the key part of extracting the edges for the space-of-changes representation. We will group the rows by FAHRT_BEZEICHNER. Then, we will use two nested loops to create edges between any station and all subsequent stations on a given ‚ÄúFahrt‚Äù. Note that in contrast to Kurant and Thiran (2006) we will extract directed edges. The following function specifies how the edges can be extracted for one group. It‚Äôs not very performant code and there may be smarter and more efficient ways of doing this. But it does the job.\n\n# Function to compute (directed) edges according to spaces-of-changes principle.\ndef get_edges_in_groups(group):\n    # Empty list for results of a group.\n    results = []\n    # Loop over all rows in group.\n    for i in range(len(group)):\n        # Nested loop over all subsequent rows.\n        for j in range(i + 1, len(group)):\n            # Now, append edge to results list.\n            results.append((\n                group.iloc[i][\"STATION_NAME\"], # Station of origin\n                group.iloc[j][\"STATION_NAME\"], # Station of destination\n                (group.iloc[j]['ANKUNFTSZEIT'] - group.iloc[i]['ABFAHRTSZEIT']).total_seconds() / 60 # Time (minutes)\n            ))\n    # Return list.\n    return results\n\nWe can now apply that function to every group. On my machine, this step took roughly 10 minutes.\n\n# Now apply that function group-wise.\nedges_series = df.groupby(\"FAHRT_BEZEICHNER\", group_keys=False).apply(get_edges_in_groups)\n\nThe output of the previous step is a Pandas series, as the following check confirms. We can see that every element of that series is identified with FAHRT_BEZEICHNER and contains a list with the edges, also including the time between the two nodes.\n\n# Make sure the result is a pandas series.\nprint(\"Is pandas series:\", isinstance(edges_series, pd.Series))\n\n# Check first few elements:\nedges_series.head()\n\nIs pandas series: True\n\n\nFAHRT_BEZEICHNER\n60402-NZ-8503000-213400    [(Z√ºrich HB, Basel SBB, 54.0), (Z√ºrich HB, Bas...\n60403-NZ-8400058-191500    [(Basel Bad Bf, Basel SBB, 7.0), (Basel Bad Bf...\n60408-NZ-8098160-205700    [(Basel Bad Bf, Basel SBB, 8.0), (Basel Bad Bf...\n60409-NZ-8503000-195900    [(Z√ºrich HB, Basel SBB, 54.0), (Z√ºrich HB, Bas...\n60470-NZ-8503000-205900    [(Z√ºrich HB, Basel SBB, 54.0), (Z√ºrich HB, Bas...\ndtype: object\n\n\nWe perform another quick check to make sure the series contains as many elements as there are unique FAHRT_BEZEICHNER strings. That seems to be the case.\n\n# How many elements?\nprint(\"Number of elements in series:\", len(edges_series))\n\n# Is that the number of distinct FAHRTEN?\ndf[['FAHRT_BEZEICHNER']].nunique()\n\nNumber of elements in series: 15005\n\n\nFAHRT_BEZEICHNER    15005\ndtype: int64\n\n\nWe quickly check the list of edges for one ‚ÄúFahrt‚Äù to make sure it really extracted the edges in the right way.\n\n# Let's check out one FAHRT.\nedges_series[\"85:97:9:000\"]\n\n[('Yverdon-les-Bains', 'Vuiteboeuf', 10.0),\n ('Yverdon-les-Bains', 'Baulmes', 14.0),\n ('Yverdon-les-Bains', 'Six-Fontaines', 18.0),\n ('Yverdon-les-Bains', 'Ste-Croix', 33.0),\n ('Vuiteboeuf', 'Baulmes', 4.0),\n ('Vuiteboeuf', 'Six-Fontaines', 8.0),\n ('Vuiteboeuf', 'Ste-Croix', 23.0),\n ('Baulmes', 'Six-Fontaines', 4.0),\n ('Baulmes', 'Ste-Croix', 19.0),\n ('Six-Fontaines', 'Ste-Croix', 15.0)]\n\n\nThis seems to be a train that goes from Yverdon-les-Bains to Ste-Croix. It stops in Vuiteboeuf, Baulmes, and Six-Fontaines before getting to Ste-Croix. There is an edge between every station and all its subsequent stations on that ‚ÄúFahrt‚Äù. This is exactly what we wanted.\nNow, we flatten the Pandas series of lists into one edgelist.\n\n# Flatten the result into one edgelist.\nedgelist = [x for l in edges_series.values for x in l]\n\nprint(\"Number of edges:\", len(edgelist))\n\nNumber of edges: 1110766\n\n\nThis edgelist contains over one million edges. Note, however, that many of them are duplicates as we looped over all ‚ÄúFahrten‚Äù of a given day. As in Part 1, we will now aggregate all duplicate edges, counting the number of connections and the average travel time between any two nodes.\n\n# Empty dict\nedges = {}\n\n# Loop over elements in edgelist\nfor i in edgelist:\n    # Create key\n    key = (i[0], i[1])\n    # Get previous entries in dict (if there are any)\n    prev = edges.get(key, (0, 0))\n    # Update values in dict\n    edges[key] = (prev[0] + 1, prev[1] + i[2])\n\n# Divide summed up travel times by number of trips\nedges = {k: (v[0], round(v[1]/v[0], 2)) for k, v in edges.items()}\n\nprint(\"Number of edges:\", len(edges))\n\nNumber of edges: 37947\n\n\nWe are left with 37‚Äô947 directed and weighted edges that are currently stored in a dict called edges. Let‚Äôs see how long it takes to get from Olten to Winterthur and how many connections there are on a given day:\n\n# Test\nedges[(\"Olten\", \"Winterthur\")]\n\n(25, 63.84)\n\n\nI can travel from Olten to Winterthur 25 times per day (without having to change trains) and the trip takes a bit more than an hour.\nNow, there is still a small problem (which I acutally only found out about after creating a network with networkX): there are two self-loops!\n\nprint(edges[(\"Monthey-En Place\", \"Monthey-En Place\")])\nprint(edges[(\"Les Planches (Aigle)\", \"Les Planches (Aigle)\")])\n\n(47, 9.87)\n(36, 9.22)\n\n\nI checked the trips in which these stations occur and the trips actually do visit the same station twice. So, our code did the right thing, these are just two odd trips. I decided to remove those two edges:\n\n# Remove the two self-loops\nedges.pop((\"Les Planches (Aigle)\", \"Les Planches (Aigle)\"))\nedges.pop((\"Monthey-En Place\", \"Monthey-En Place\"))\n\n(47, 9.87)\n\n\nNow, we import the nodelist from Part 1 so that we can replace the station names in the edges by the BPUIC identifiers.\n\n# Load the nodelist.\nnodes = pd.read_csv(\"nodelist.csv\", sep = \";\")\n\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\nAfter changing all stations names to BPUIC numbers we create a dataframe that can then be exported as a CSV file. Yay, we‚Äôre done!\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[k[0]], node_dict[k[1]], v[0], v[1]] for k,v in edges.items()]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','NUM_CONNECTIONS','AVG_DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist_SoCha.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC1\nBPUIC2\nNUM_CONNECTIONS\nAVG_DURATION\n\n\n\n\n0\n8503000\n8500010\n88\n63.81\n\n\n1\n8503000\n8500090\n14\n81.36\n\n\n2\n8500010\n8500090\n67\n6.07\n\n\n3\n8500090\n8500010\n67\n6.39\n\n\n4\n8500090\n8503000\n10\n100.00\n\n\n\n\n\n\n\nYou can download the result here: Edgelist space-of-changes (CSV).\n\n\nSpace-of-stations\nFor the space-of-stations graph representation we make use of the fact that the space-of-stations graph should be a subgraph of the space-of-stops graph that we extracted in Part 1 with the latter containing additional edges that represent shortcuts. For example, the space-of-stops graph contains a directed edge from Olten to Basel SBB as there are nonstop trains between these two stations. However, there are also smaller, regional trains which stop at all stations in between. The key idea (also nicely shown by Kurant and Thiran) is to go through all edges in the space-of-stops graph and identify the ones that are shortcuts.\nWe first load the (space-of-stops) edgelist from Part 1 and add the station names.\n\n# Load the space-of-stops edgelist.\nedges = pd.read_csv(\"edgelist_SoSto.csv\", sep = \";\")\n\n# Create a node dict with station names as values.\nnode_dict = dict(zip(nodes.BPUIC, nodes.STATION_NAME))\n\n# Add actual station names.\nedges[\"STATION1\"] = [node_dict[v] for v in edges[\"BPUIC1\"]]\nedges[\"STATION2\"] = [node_dict[v] for v in edges[\"BPUIC2\"]]\n\n# Check out the dataframe.\nprint(edges.head())\n\nprint(\"Number of edges in space-of-stops representation:\", edges.shape[0])\n\n    BPUIC1   BPUIC2  NUM_CONNECTIONS  AVG_DURATION      STATION1      STATION2\n0  8503000  8500010               36         54.00     Z√ºrich HB     Basel SBB\n1  8500010  8500090               67          6.07     Basel SBB  Basel Bad Bf\n2  8500090  8500010               67          6.39  Basel Bad Bf     Basel SBB\n3  8500010  8503000               39         57.87     Basel SBB     Z√ºrich HB\n4  8506286  8506271               34          3.00     Appenzell     Gontenbad\nNumber of edges in space-of-stops representation: 4211\n\n\nFor the space-of-stations representation, undirected edges make the most sense. Thus, we need to make the directed edges from the space-of-stops representation undirected and remove all duplicates that this introduces (e.g., ‚ÄòOlten - Basel SBB‚Äô and ‚ÄòBasel SBB - Olten‚Äô). With a little help by ChatGPT I found an elegant solution to achieve just that.\nMore concretely, we iterate over the zip object containing the node pairs of all edges. The min() and max() functions applied to the station names will sort the station names alphabetically so that, for example, ‚ÄòOlten - Basel SBB‚Äô and ‚ÄòBasel SBB - Olten‚Äô are both transformed to ‚ÄòBasel SBB - Olten‚Äô. Finally, the set() function will get rid of all duplicates.\n\n# Get a list of unique undirected edges.\nunique_undirected_edges = list(set((min(e1, e2), max(e1, e2)) for e1, e2 in zip(edges[\"STATION1\"], edges[\"STATION2\"])))\n\nprint(\"Number of unique undirected edges:\", len(unique_undirected_edges))\n\nNumber of unique undirected edges: 2152\n\n\nThis step leaves us with 2‚Äô152 undirected, unique edges.\n\nData preprocessing for improved efficiency\nIn order to make the procedure further below more efficient, we extract here all unique ‚ÄúFahrten‚Äù. More specifically, we create a dictionary fahrten with the sequence of station names as key and the FAHRT_BEZEICHNER as value. Note that if a sequence of station names already exists as a key in the dict, then the value belonging to that key will be overwritten with the new FAHRT_BEZEICHNER but that doesn‚Äôt bother us since we just want to be able to extract one example ‚ÄúFahrt‚Äù per unique sequence of stops.\n\n# Empty dict\nfahrten = {}\n\n# Loop over grouped df.\n# If the same key (sequence of stops) reappears, the value will be overwritte.\n# But that behavior is desired: we only want to keep one FAHRT_BEZEICHNER per key.\nfor fahrt, group in df.groupby('FAHRT_BEZEICHNER'):\n    fahrten[tuple(group['STATION_NAME'])] = fahrt\n\nprint(\"Number of unique 'Fahrten':\", len(fahrten))\nprint(\"Number of 'Fahrten' in whole dataframe:\", df['FAHRT_BEZEICHNER'].nunique())\n\nNumber of unique 'Fahrten': 1686\nNumber of 'Fahrten' in whole dataframe: 15005\n\n\nWe can see from the above output that this step drastically reduces the ‚ÄúFahrten‚Äù that we will iterate over later.\nIn the following code chunk we filter the ‚ÄúIst-Daten‚Äù (df) loaded earlier so that only the unique ‚ÄúFahrten‚Äù are left.\n\n# Reduce the dataframe to the 'Fahrten' in list of values of dict.\ndf = df[df['FAHRT_BEZEICHNER'].isin(list(fahrten.values()))]\n\nprint(\"Remaining number of rows:\", df.shape[0])\n\nRemaining number of rows: 17603\n\n\nAnother little trick to make things more efficent later is to create a dictionary with station names as keys and a list with all FAHRT_BEZEICHNER strings a station name is part of as values (kind of an inverted index).\n\n# defaultdict with lists\nresult_dict = defaultdict(list)\n\n# Iterate over rows\nfor _, row in df.iterrows():\n    # Create a dict with stations as keys and FAHRT_BEZEICHNER as values.\n    result_dict[row['STATION_NAME']].append(row['FAHRT_BEZEICHNER'])\n\n# Convert back to normal dict.\nresult_dict = dict(result_dict)\n\n\n\nIdentify shortcuts\nNext, we perform the key step in extracting the edges for the space-of-stations representation: we need to identify all edges that are shortcuts, passing train stations without stopping.\nWe first define a custom function that determines whether any two station names a and b are adjacent in a sequence (list) of station names lst.\n\n# Function to check whether elements a and b are NOT adjacent in lst.\ndef is_shortcut(lst, a, b):\n    return not any((x, y) == (a, b) or (x, y) == (b, a) for x, y in zip(lst, lst[1:]))\n\nThen, we iterate over all undirected, unique edges that we prepared above. For each edge we go through the following steps:\n\nWe get the FAHRT_BEZEICHNER strings for all ‚ÄúFahrten‚Äù which both nodes of the edge are part of. For this we use the inverted index-style dictionary we created above.\nThen we perform an inner loop over the ‚ÄúFahrten‚Äù extracted in the first step.\n\nWe first extract the sequence of stations of a ‚ÄúFahrt‚Äù.\nWe use our custom function from above to check whether the two nodes are adjacent in the sequence of stations.\nIf they are not adjacent, i.e., the edge represents a shortcut, then we save that edge and break the inner loop and move on to the next edge.\n\n\n\n# Empty list for shortcuts.\nshortcut_edges = []\n\n# Loop over list of undirected edges.\nfor idx, edge in enumerate(unique_undirected_edges):\n    # Find all 'Fahrten' in which both stations of the edge appear.\n    intersection = list(set(result_dict[edge[0]]) & set(result_dict[edge[1]]))\n    # Initialize shortcut to False\n    shortcut = False\n    # Loop over 'Fahrten' in which both stations of the edge appear.\n    for fahrt in intersection:\n        # Get the sequence of stations in current 'Fahrt'.\n        seq_of_stations = df.loc[df['FAHRT_BEZEICHNER'] == fahrt, 'STATION_NAME'].tolist()\n        # Check whether the edge represents a shortcut in that sequence.\n        shortcut = is_shortcut(seq_of_stations, edge[0], edge[1])\n        # If it is a shortcut, we add it to the list and break the inner loop.\n        if shortcut:\n            # Add to list and break the loop.\n            shortcut_edges.append((fahrt, edge))\n            break\n\nprint(\"Number of shortcut edges:\", len(shortcut_edges))\n\nNumber of shortcut edges: 443\n\n\nA total of 443 edges are identified as shortcuts. Let‚Äôs have a look at the first one:\n\n# Check first shortcut.\nprint(shortcut_edges[0])\n\n# Check the 'Fahrt' in which it was detected as a shortcut.\ndf.loc[df['FAHRT_BEZEICHNER'] == shortcut_edges[0][0], 'STATION_NAME']\n\n('ch:1:sjyid:100015:15793-002', ('Gampelen', 'Marin-Epagnier'))\n\n\n84149         Neuch√¢tel\n84150     St-Blaise-Lac\n84151    Marin-Epagnier\n84152        Zihlbr√ºcke\n84153          Gampelen\n84154               Ins\n84155      M√ºntschemier\n84156           Kerzers\nName: STATION_NAME, dtype: object\n\n\nFrom the whole sequence of stations, we can see that the edge identified as a shortcut is, in fact, a connection that is not consecutive.\nFinally, we remove the FAHRT_BEZEICHNER from shortcut_edges and create the final edge list without shortcuts.\n\n# Extract only edges\nshortcut_edges_clean = [i[1] for i in shortcut_edges]\n\n# Get the final list of non-shortcut edges.\nfinal_edges = [e for e in unique_undirected_edges if e not in shortcut_edges_clean]\n\nprint(\"Number of edges:\", len(final_edges))\n\nNumber of edges: 1709\n\n\nWe have a final number of edges of \\(2152-443=1709\\).\n\n\nValidate with ‚ÄúLiniendaten‚Äù\nThe extraction of the edges in the space-of-stations representation was a bit more complex than for space-of-changes or space-of-stops. That‚Äôs why I would like to run some checks.\nWe can validate some of the edges we extracted with another dataset from the Open Data Portal of SBB. The dataset Linie (Betriebspunkte) contains all railway ‚Äúlines‚Äù maintained by SBB with all ‚ÄúBetriebspunkte‚Äù (including stations) that are located along these lines. Let‚Äôs load this dataset:\n\n# Load the data about \"Linien mit Betriebspunkten\"\nlinien = pd.read_csv('linie-mit-betriebspunkten.csv', sep = \";\")\n\n# Reduce to relevant columns\nlinien = linien[[\"Name Haltestelle\",\"Linie\",\"KM\",\"Linien Text\",\"BPUIC\"]]\n\nprint(\"Shape of dataframe:\", linien.shape)\n\nShape of dataframe: (1884, 5)\n\n\nLet‚Äôs have a look:\n\n# Have a look at the dataframe\nlinien.head()\n\n\n\n\n\n\n\n\nName Haltestelle\nLinie\nKM\nLinien Text\nBPUIC\n\n\n\n\n0\nAarau\n649\n41.50577\nAarau - Woschnau Tunnel alt\n8502113\n\n\n1\nAarberg\n251\n95.49304\nPalezieux Est - Lyss Nord\n8504404\n\n\n2\nAesch BL\n230\n113.00006\nDelemont Est - Basel SBB Ost\n8500117\n\n\n3\nAespli\n455\n92.76586\nUnterhard BE - Aespli\n8515299\n\n\n4\nAigle\n100\n39.31241\nLausanne - Simplon Tunnel I - Iselle\n8501400\n\n\n\n\n\n\n\nThe rows in that dataset are not just stations but also other ‚ÄúBetriebspunkte‚Äù (important locations that are needed to run the infrastructure). But we can identify the stations among the ‚ÄúBetriebspunkte‚Äù by joining the nodes dataframe on BPUIC and only keeping the entries for which there was a matching row in nodes.\n\n# Join the rows of nodelist based on BPUIC.\nlinien = pd.merge(linien, nodes[[\"BPUIC\",\"STATION_NAME\"]], on = 'BPUIC', how = 'left')\n\n# How many entries have a missing value aka are not stations?\nprint(\"Number of non-stations:\", linien[\"STATION_NAME\"].isna().sum())\n\n# Drop all rows that are not stations.\nlinien = linien.dropna(subset = [\"STATION_NAME\"])\n\nprint(\"Number of remaining rows:\", linien.shape[0])\n\nNumber of non-stations: 979\nNumber of remaining rows: 905\n\n\nNext, we group the rows by 'Linie' and sort them in ascending order by 'KM' (where along the line is the ‚ÄúBetriebspunkt‚Äù located, in terms of kilometres) so that the stations for each line are sorted in the right order.\n\n# Function to sort entries within a group in ascending order of KM\ndef sort_data(group):\n    return group.sort_values('KM', ascending = True)\n\n# Sort for each group\nlinien_sorted = linien.groupby('Linie', group_keys=False).apply(sort_data)\n\n# Let's have a look at Linie 290.\nlinien_sorted.loc[linien_sorted['Linie'] == 290]\n\n\n\n\n\n\n\n\nName Haltestelle\nLinie\nKM\nLinien Text\nBPUIC\nSTATION_NAME\n\n\n\n\n1351\nOstermundigen\n290\n110.76500\nBern Wylerfeld - Thun\n8507002\nOstermundigen\n\n\n865\nGumligen\n290\n113.95844\nBern Wylerfeld - Thun\n8507003\nG√ºmligen\n\n\n270\nRubigen\n290\n119.03812\nBern Wylerfeld - Thun\n8507005\nRubigen\n\n\n1333\nMunsingen\n290\n122.13149\nBern Wylerfeld - Thun\n8507006\nM√ºnsingen\n\n\n695\nWichtrach\n290\n125.73250\nBern Wylerfeld - Thun\n8507007\nWichtrach\n\n\n892\nKiesen\n290\n128.30300\nBern Wylerfeld - Thun\n8507008\nKiesen\n\n\n1055\nUttigen\n290\n131.09513\nBern Wylerfeld - Thun\n8507009\nUttigen\n\n\n\n\n\n\n\nWe see here for one example (Line 290) that the stations are now nicely sorted in ascending order of 'KM'.\nNow, we can create a new column that always contains the station name of the next row using the handy shift() method. We then do the same with the KM column and compute the distance between any subsequent stations. We will use those distances later on as edge weights for this representation.\nThe last row within a group will always have a missing value for those new columns as there is no next station at the end of a line. So, we drop the last row of each line.\n\n# Create a new column that for each row contains the next stop within the group.\nlinien_sorted[\"NEXT_STATION\"] = linien_sorted.groupby(\"Linie\")[\"STATION_NAME\"].shift(-1)\n\n# Do the same for KM.\nlinien_sorted[\"NEXT_STATION_KM\"] = linien_sorted.groupby(\"Linie\")[\"KM\"].shift(-1)\n\n# Compute distance.\nlinien_sorted[\"DISTANCE\"] = linien_sorted[\"NEXT_STATION_KM\"] - linien_sorted[\"KM\"]\n\n# Drop all rows where 'NEXT_STATION' is missing\nlinien_sorted = linien_sorted.dropna(subset = [\"NEXT_STATION\"])\n\nWe now extract the values of the columns STATION_NAME and NEXT_STATION and ignore the distances for now. We will use this to validate our approach. Importantly, we sort the node pairs in each edge in the same way as before (alphabetically).\n\n# Now let's extract the edges\nlinien_edges = list(zip(linien_sorted['STATION_NAME'], linien_sorted['NEXT_STATION']))\n\n# Make sure the tuples are arranged in the same way as above (and unique).\nlinien_edges = list(set((min(e[0], e[1]), max(e[0], e[1])) for e in linien_edges))\n\nAs for the validation, we first want to check whether there are edges in linien_edges that are neither a shortcut nor in the final edgelist from above.\n\n# Check which edges are in linien_edges but neither in final_edges nor in shortcut_edges_clean.\n[x for x in linien_edges if x not in final_edges and x not in shortcut_edges_clean]\n\n[('Neuhausen Rheinfall', 'Rafz'),\n ('B√ºren an der Aare', 'Solothurn'),\n ('Dietfurt', 'Kaltbrunn'),\n ('Koblenz', 'Laufenburg'),\n ('Solothurn', 'Zollikofen'),\n ('Concise', 'Grandson'),\n ('Langenthal', 'Niederbipp'),\n ('Chambrelien', 'Corcelles-Peseux'),\n ('Bauma', 'Hinwil'),\n ('Niederbipp', 'Solothurn'),\n ('La Chaux-de-Fonds', 'Tavannes')]\n\n\nThere are some candidate edges but I checked all of them manually in the train schedule and none of them seem to have direct train connections. It could be that some of these are old train lines that are not active anymore.\nAre there any edges in linien_edges that were classified as shortcuts?\n\n# Are there any edges that I classified as shortcuts?\n[x for x in linien_edges if x in shortcut_edges_clean]\n\n[('Chur', 'Landquart'), ('Brig', 'Visp'), ('Niederbipp', 'Oensingen')]\n\n\nYes, but the three connections are in fact shortcuts. Between Niederbipp and Oensingen there is a small station called Niederbipp Industrie. Between Brig and Visp there is a small station called Eyholz. Between Chur and Landquart there are several smaller stations. Note, however, that it could be that even though the train tracks between Chur and Landquart actually pass those smaller stations there is no infrastructure for trains to actually stop.\nA manual check of the edges reveals that there are other shortcuts that our procedure was not able to identify. For example, the edge (Bern, Zofingen) cannot be identified because there is no other ‚ÄúFahrt‚Äù that contains these two stations and stops somewhere in between. We manually remove such edges. In addition, we add some edges for which I know that there is actually infrastructure (tunnels, high-speed routes) that directly connects the two nodes involved.\n\n# Manually remove edges.\nfinal_edges.remove(('Bern', 'Zofingen'))\nfinal_edges.remove(('Bern Wankdorf', 'Z√ºrich HB'))\nfinal_edges.remove(('Morges', 'Yverdon-les-Bains'))\nfinal_edges.remove(('Aarau', 'Sissach'))\nfinal_edges.remove(('Berg√ºn/Bravuogn', 'Pontresina'))\nfinal_edges.remove(('Interlaken West', 'Spiez'))\nfinal_edges.remove(('Biel/Bienne', 'Grenchen Nord'))\nfinal_edges.remove(('Chambrelien', 'Neuch√¢tel'))\nfinal_edges.remove(('Concise', 'Yverdon-les-Bains'))\nfinal_edges.remove(('Etoy', 'Rolle'))\nfinal_edges.remove(('Klosters Platz', 'Susch')) # Avoid several edges representing the Vereina tunnel\n\n# Manually add edges.\nfinal_edges.append(('Biasca', 'Erstfeld')) # New Gotthard tunnel\nfinal_edges.append(('Bern Wankdorf', 'Rothrist')) # Bahn-2000\nfinal_edges.append(('Chambrelien', 'Corcelles-Peseux')) # Connector that was missing\nfinal_edges.append(('Concise', 'Grandson')) # Connector that was missing\nfinal_edges.append(('Immensee', 'Rotkreuz')) # Connector that was missing\nfinal_edges.append(('Olten', 'Rothrist')) # Connector not going through Aarburg-Oftringen\nfinal_edges.append(('Rothrist', 'Solothurn')) # Bahn-2000\nfinal_edges.append(('Aarau', 'D√§niken SO')) # Eppenberg tunnel\nfinal_edges.append(('Liestal', 'Muttenz')) # Adler tunnel\nfinal_edges.append(('Thalwil', 'Z√ºrich HB')) # Zimmerberg tunnel\nfinal_edges.append(('Z√ºrich Altstetten', 'Z√ºrich HB')) # Separate infrastructure connecting the two stations\n\nFunny thing is that the two edges, ('Chambrelien', 'Corcelles-Peseux') and ('Concise', 'Grandson'), that our validation procedure proposed as missing edges did actually need to be added upon further inspection. They were missing connectors when I visually inspected the network.\nAfter these final modifications of the edgelist, the total number of edges is 1‚Äô709, since we add and remove exactly 11 edges (which is a coincidence).\n\n\nEdge weights\nAs edge weights, we will compute the distances between stations. We will have exact distances for the lines maintained by SBB (we already computed them above based on the dataset Linie (Betriebspunkte)). For all other edges, we will simply compute the direct distance based on the coordinates of the stations.\nIn a first step, we augment every edge in our edgelist with the direct distance:\n\n# New column with coordinates in the same column.\nnodes['coord'] = list(zip(nodes.LATITUDE, nodes.LONGITUDE))\n\n# Define a function to compute direct distance.\ndef compute_distance(station1, station2):\n    return geopy.distance.geodesic(\n        nodes.loc[nodes['STATION_NAME'] == station1, \"coord\"].item(), \n        nodes.loc[nodes['STATION_NAME'] == station2, \"coord\"].item()).km\n\n# Compute direct distances between node pairs.\nfinal_edges = [(e[0], e[1], compute_distance(e[0], e[1])) for e in final_edges]\n\nThen, in a second step we modify every edge that appears in the validation data (based on the dataset Linie (Betriebspunkte)) and fill in the exact distance. For this we transform the validation data into a dictionary for fast lookups. The get() method then allows for easy replacement of distances:\n\n# List of edges including distances.\nlinien_edges = list(zip(linien_sorted['STATION_NAME'], linien_sorted['NEXT_STATION'], linien_sorted['DISTANCE']))\n\n# Make sure the tuples are arranged in the same way as above (and unique).\nlinien_edges = list(set((min(e[0], e[1]), max(e[0], e[1]), e[2]) for e in linien_edges))\n\n# Convert to a dict.\nlinien_edges_dict = {(e[0], e[1]): e[2] for e in linien_edges}\n\n# Add exact distance for edges that exist in dict with exact distance.\nfinal_edges = [(n1, n2, dist, linien_edges_dict.get((n1, n2), np.nan)) for n1, n2, dist in final_edges]\n\n\n\nExport the edgelist\nFinally, we can export the edges as before for the other representations. Note that before we export the data we correct one small mistake in the edge Baar Lindenpark - Zug. The exact distance derived from the dataset Linie (Betriebspunkte) is way too large and thus we simply impute the geodesic distance for this one edge.\n\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n# Transform edge dict to nested list and replace all station names with their BPUIC.\n# Also, round the distances to 4 decimal points.\nedges = [[node_dict[e[0]], node_dict[e[1]], round(e[2], 4), round(e[3], 4)] for e in final_edges]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','DISTANCE_GEODESIC','DISTANCE_EXACT'])\n\n# Correct mistake in edge between Baar Lindenpark and Zug.\nedges.loc[(edges['BPUIC1'] == 8515993) & (edges['BPUIC2'] == 8502204), 'DISTANCE_EXACT'] = 1.0593\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist_SoSta.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC1\nBPUIC2\nDISTANCE_GEODESIC\nDISTANCE_EXACT\n\n\n\n\n0\n8503234\n8503233\n1.8435\n1.9635\n\n\n1\n8506311\n8506322\n1.2191\n1.2216\n\n\n2\n8505004\n8502205\n6.0461\nNaN\n\n\n3\n8501169\n8587727\n0.8863\nNaN\n\n\n4\n8500021\n8517131\n1.6846\nNaN\n\n\n\n\n\n\n\n\n\n\nThe Swiss railway network with a geographic layout, Space-of-Stations representation (created using Gephi).\n\n\nYou can download the result here: Edgelist space-of-stations (CSV).\n\n\n\nReferences\nKurant, M., & Thiran, P. (2006). Extraction and analysis of traffic and topologies of transportation networks. Physical Review E, 74(3), 036114. https://doi.org/10.1103/PhysRevE.74.036114\nThe title image has been created by Wikimedia user JoachimKohler-HB and is licensed under Creative Commons."
  },
  {
    "objectID": "blog/sbb_nw3/index.html",
    "href": "blog/sbb_nw3/index.html",
    "title": "SBB Network Analysis - Part 3",
    "section": "",
    "text": "After Part 1 and Part 2, where I demonstrated how to create different versions of static networks, I now want to show how to construct a temporal network representation of the Swiss railway network. If you‚Äôve followed along with the first two parts of this series, the code here should be easy to understand.\nThe temporal network representation I develop here is based on the space-of-changes approach. In this representation, a directed edge connects each station to all subsequent stations for a given ‚ÄúFahrt.‚Äù Instead of aggregating edges between the same pairs of stations, we retain all edges at different points in time, storing both the start time of each edge, \\(t\\), and the time required to traverse it, \\(\\delta t\\). This is just one possible way to represent temporal edges (see, for instance, the 2012 overview paper by Petter Holme and Jari Saram√§ki).\nWith this temporal network model, finding time-respecting paths between any two nodes closely mirrors what the SBB (Swiss railway) app does when searching for the fastest connections between stations.\nBut let‚Äôs start the practical part now.\n\nimport pandas as pd\nfrom collections import Counter, defaultdict\n\n# Check versions of libraries.\nprint(\"Pandas version:\", pd.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n\nPandas version: 2.1.4\n\n\n\nTemporal edgelist\nAs for the space-of-changes representation, we start by loading the already processed ‚ÄúIst-Daten‚Äù from Part 1 and transform all date-time elements into the right format. Also, we only need a few of the columns, so we reduce the dataframe drastically to only 4 columns.\n\n# Load the processed IST-DATEN.\ndf = pd.read_csv('ist-daten.csv', sep=\";\", low_memory=False)\n\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'])\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'])\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'])\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'])\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'])\n\n# Reduce to relevant columns.\ndf = df[[\"FAHRT_BEZEICHNER\",\"STATION_NAME\",\"ANKUNFTSZEIT\",\"ABFAHRTSZEIT\"]]\n\n# Check the dataframe.\ndf.head()\n\n\n\n\n\n\n\n\nFAHRT_BEZEICHNER\nSTATION_NAME\nANKUNFTSZEIT\nABFAHRTSZEIT\n\n\n\n\n0\n80:800631:17230:000\nBasel Bad Bf\n2025-03-05 04:59:00\nNaT\n\n\n1\n80:800631:17233:000\nBasel Bad Bf\nNaT\n2025-03-05 06:07:00\n\n\n2\n80:800631:17234:000\nBasel Bad Bf\n2025-03-05 05:56:00\nNaT\n\n\n3\n80:800631:17235:000\nBasel Bad Bf\nNaT\n2025-03-05 06:43:00\n\n\n4\n80:800631:17236:000\nBasel Bad Bf\n2025-03-05 06:31:00\nNaT\n\n\n\n\n\n\n\nWe now use almost the same function as for the space-of-changes representation in order to extract the edges between any station and all its subsequent stations in a given ‚ÄúFahrt‚Äù.\nThe only difference is that we extract, as the third element of an edge, the start time measured in minutes since the start of the day (2025-03-05 00:00:00). So, a train that departs at one minute past midnight will have the start time 1, as the following code demonstrates:\n\n(pd.to_datetime(\"2025-03-05 00:01:00\") - pd.to_datetime(\"2025-03-05 00:00:00\")).total_seconds() / 60\n\n1.0\n\n\nHere now the function that we will use to iterate over the ‚ÄúFahrten‚Äù:\n\n# Function to compute (directed) edges according to spaces-of-changes principle.\ndef get_edges_in_groups(group):\n    # Empty list for results of a group.\n    results = []\n    # Loop over all rows in group.\n    for i in range(len(group)):\n        # Nested loop over all subsequent rows.\n        for j in range(i + 1, len(group)):\n            # Now, append edge to results list.\n            results.append((\n                group.iloc[i][\"STATION_NAME\"], # Station of origin\n                group.iloc[j][\"STATION_NAME\"], # Station of destination\n                # Time of departure in minutes since the day began.\n                (group.iloc[i][\"ABFAHRTSZEIT\"] - pd.to_datetime(\"2025-03-05 00:00:00\")).total_seconds() / 60,\n                # Duration in minutes.\n                (group.iloc[j]['ANKUNFTSZEIT'] - group.iloc[i]['ABFAHRTSZEIT']).total_seconds() / 60\n            ))\n    # Return list.\n    return results\n\nThis function is applied as before for the space-of-changes representation:\n\n# Now apply that function group-wise.\nedges_series = df.groupby(\"FAHRT_BEZEICHNER\", group_keys=False).apply(get_edges_in_groups)\n\nWe can check the same ‚ÄúFahrt‚Äù between Yverdon-les-Bains and Ste-Croix again.\n\n# Let's check out one FAHRT.\nedges_series[\"85:97:9:000\"]\n\n[('Yverdon-les-Bains', 'Vuiteboeuf', 333.0, 10.0),\n ('Yverdon-les-Bains', 'Baulmes', 333.0, 14.0),\n ('Yverdon-les-Bains', 'Six-Fontaines', 333.0, 18.0),\n ('Yverdon-les-Bains', 'Ste-Croix', 333.0, 33.0),\n ('Vuiteboeuf', 'Baulmes', 343.0, 4.0),\n ('Vuiteboeuf', 'Six-Fontaines', 343.0, 8.0),\n ('Vuiteboeuf', 'Ste-Croix', 343.0, 23.0),\n ('Baulmes', 'Six-Fontaines', 347.0, 4.0),\n ('Baulmes', 'Ste-Croix', 347.0, 19.0),\n ('Six-Fontaines', 'Ste-Croix', 351.0, 15.0)]\n\n\nThat train starts at 333 minutes past midnight (which is 05:33). The durations are the same as before in the space-of-changes representation.\nThe final step before getting the data ready for the export is to flatten all the edges that are currently organized in the form of a Pandas series of lists.\n\n# Flatten the result into one edgelist.\nedgelist = [x for l in edges_series.values for x in l]\n\nprint(\"Number of edges:\", len(edgelist))\n\nNumber of edges: 1110766\n\n\nThe space-of-changes representation code now aggregated duplicate edges. Crucially, this step is omitted here as we want to keep the temporal represenation of edges. Thus, our temporal representation of the network will have 1‚Äô110‚Äô766 edges.\nThe final steps are easy: we change the station names to their BPUIC numbers, we convert both the start time and the duration of an edge to integer values, and we export the dataframe as a CSV file.\n\n# Load the nodelist.\nnodes = pd.read_csv(\"nodelist.csv\", sep = \";\")\n\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[e[0]], node_dict[e[1]], int(e[2]), int(e[3])] for e in edgelist]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','START','DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist_temporal.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC1\nBPUIC2\nSTART\nDURATION\n\n\n\n\n0\n8503000\n8500010\n1294\n54\n\n\n1\n8503000\n8500090\n1294\n105\n\n\n2\n8500010\n8500090\n1393\n6\n\n\n3\n8500090\n8500010\n1813\n7\n\n\n4\n8500090\n8503000\n1813\n112\n\n\n\n\n\n\n\nYou can download the result here: Temporal Edgelist (CSV).\n\n\nReferences\nHolme, P., & Saram√§ki, J. (2012). Temporal networks. Physics Reports, 519(3), 97-125. https://doi.org/10.1016/j.physrep.2012.03.001\nThe title image has been created by Wikimedia user JoachimKohler-HB and is licensed under Creative Commons."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching at the University of Applied Sciences and Arts Northwestern Switzerland (FHNW) centers around three key modules, outlined below.\n\nMachine Learning\nThis module is one of four within the Managerial Data Science specialization, available to both Business and International Management students at FHNW during the final year of their Bachelor‚Äôs studies.\nMany students in this specialization have little to no prior programming experience and only a modest background in mathematics and statistics. Teaching a subject rooted in advanced concepts from both fields might seem daunting in this context. However, I view it as an exciting challenge. Over the years, several key decisions have proven particularly effective.\nThe first was to clearly position this module as applied Machine Learning. Our primary goal is to solve real-world problems using off-the-shelf models and algorithms. Every topic is introduced through practical examples‚Äîpreferably from a business context‚Äîand accompanied by R code to demonstrate each model or technique.\nThe second decision was to avoid making this a no-math course. Instead, we explore mathematical concepts when they provide deeper insights into a model‚Äôs inner workings or enhance intuitive understanding. I believe that even students with a minimal math background can grasp complex topics if they are broken down into univariate or bivariate examples, where visualization is straightforward and the (linear) algebra remains simple.\nFinally, I adopted a flipped classroom approach. Based on my experience, students learn best when they can explore new topics at their own pace, in a setting of their choice. To support this, I developed interactive online tutorials using the learnr package, featuring text, Shiny apps, visualizations, R code snippets, and occasional multiple-choice questions. In-class sessions are then dedicated to deepening the material, working through exercises together, and addressing questions or areas of particular interest.\nBelow are two visualizations I created to help clarify topics that students often find challenging.\n\n\n\nHow R transforms a linear scale into a log scale.\n\n\nThe first illustrates the effect of using a log scale (base 10) versus a linear scale in R. It shows how three values (10, 500, and 100,000) are represented differently when plotted on a log scale. The figure should be interpreted from bottom to top.\n\n\n\nHow a simple convolution filter works on a simplified MNIST image.\n\n\nThe second visualization explains how a simple convolution filter operates using a simplified (binary) MNIST image of a handwritten digit. I manually designed an image of the number 7 to clearly highlight the pixel-level computations during the convolution process.\n\n\nApplied Data Science\nThe current topic of the Applied Data Science course (a week-long intensive course) is data visualization. This elective course is offered annually in November. In the course, we use the tidyverse framework, with a particular focus on the ggplot2 package to create visualizations.\nTo demonstrate the key principles of high-quality visualization, I wrote a bunch of code to generate engaging plots, all based on real data from the Swiss canton of Basel-Landschaft. This data is available on the canton‚Äôs open government data portal. The goal was to walk students through the essential steps of creating effective visualizations using a practical case study.\nSome results from this case study are shown below. The first figure is a scatterplot illustrating the relationship between two quantitative variables‚Äîtax rates and land prices‚Äîand demonstrates how additional information can be integrated into the plot using point color and size.\n\n\n\nVisualizing the relationship between two quantitative variables.\n\n\nThe second figure presents CO2 emissions (in tons per capita) across all communities in the canton. This figure serves as an example of geographic visualization using a choropleth map.\n\n\n\nVisualizing a quantitative variable geographically.\n\n\nFinally, the third figure showcases how to visualize time series data. Here, we display the daily measurements of nitrogen dioxide and ozone concentrations, as recorded by the monitoring station in Sissach-B√ºtzenen.\n\n\n\nVisualizing two time series.\n\n\n\n\nBusiness Analytics\nBusiness Analytics (BA) is one of two components of the core module Empirische Methoden und Business Analytics, which is mandatory for all Business students at FHNW.\nThe BA section is structured around two main areas of focus:\n\nIntroduction to R and Data Handling: Students begin by working with R and RStudio, gaining a basic understanding of key data structures such as vectors, factors, and data frames. They learn how to import data, manipulate these basic structures, and apply indexing. The course also covers the calculation of simple descriptive statistics and introduces R‚Äôs base plotting capabilities for data visualization.\nFundamentals of Linear Regression: The second part of the course delves into simple and multiple linear regression. Students explore the intuition behind least-squares coefficients and learn how the coefficient of determination can be used for model evaluation and comparison. In the context of multiple linear regression, the course covers topics such as dummy encoding for categorical variables and the use of interaction terms, among other techniques.\n\nThis structure ensures that students not only develop a basic technical proficiency in data analysis using R but also gain a solid understanding of essential statistical modeling concepts."
  }
]