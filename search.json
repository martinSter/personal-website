[
  {
    "objectID": "blog/sbb_nw/index.html",
    "href": "blog/sbb_nw/index.html",
    "title": "SBB Network Analysis - Part 1",
    "section": "",
    "text": "Update 10.03.2025: I updated the analysis in this blog so that it runs on more recent data. More precisely, I use the train traffic data from March 5, 2025 to construct the network. Moreover, I now properly reference the source data and I have added a bunch of additional node attributes. The most interesting new node attributes are average passenger frequency data for all stations.\nFor quite some time I have been wondering if there are some interesting Swiss data that would serve as the basis for some fun network analysis. As a fan of public transportation and a long-time owner of a Swiss train pass (‚ÄúGA‚Äù), the answer should have been obvious much sooner: the Swiss railway network.\nI wanted to create a (static) network in which each node corresponds to a train station and each directed edge between any two nodes, A and B, means there is at least one train going nonstop from A to B. Ideally, the edge would also be attributed with some weight representing the importance of the edge (e.g., how many trains go nonstop from A to B on a given day).\nThe structure of this post is as follows. I will first introduce the three datasets that I used to create the network. I will then show how to load and preprocess each one of them and how to join them. Finally, I will present how to transform those data into a form that is suitable for network analysis. The following image shows a visualization of the network data resulting from this post.\n\n\n\nThe Swiss railway network with a geographic layout (created using Gephi).\n\n\nThis is the first part of a series that will cover all kinds of fun network analysis based on the Swiss railway network.\n\nData sources\nIt was not that obvious how a network with nodes and edges following the definitions given above could be constructed based on data from the Swiss Federal Railways (abbreviated by the German speakers in Switzerland as SBB). With some help from SBB Experts and the Open Data Plattform Mobility Switzerland, I finally found the right data.\nThe first and most important dataset is called Ist-Daten and, for a given day, contains all regular stops of all trains in Switzerland with their planned and effective arrival and departure times. From this data, we can infer all nonstop stretches of any train in Switzerland. A description of this dataset can be found here.\nNote that the ‚ÄúIst-Daten‚Äù not only contain the data for trains but also for all other public transport (buses, trams, and even boats). To keep things simple we will focus on the train network.\nThe second dataset is the Dienststellen-Daten which basically allows to add node attributes such as the geographic coordinates of a node (i.e., a train station). A description of this dataset can be found here.\nThe third dataset is a statistic of the average number of passengers boarding and alighting. It will allow us to add further interesting node attributes.\n\n\nLoad and preprocess ‚ÄúIst-Daten‚Äù\nHere, we will load and preprocess the ‚ÄúIst-Daten‚Äù from which we can derive the edges of our network. First, I import some Python libraries and print their version number for better reproducibility of this code.\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Check versions of libraries.\nprint(\"NumPy version:\", np.__version__)\nprint(\"Pandas version:\", pd.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n\nNumPy version: 1.26.4\nPandas version: 2.1.4\n\n\nLet‚Äôs now load the data. You can see in the filename that I downloaded the ‚ÄúIst-Daten‚Äù from the SBB portal for March 5, 2025. You can get the data for any day you want here.\n\n# Load the data\ndf = pd.read_csv('2025-03-05_istdaten.csv', sep=\";\", low_memory=False)\n\nTo get a feeling for the data, let‚Äôs check the number of rows and columns.\n\n# Number of rows and columns\nprint(df.shape)\n\n(2510290, 21)\n\n\nOk, it‚Äôs actually a pretty big dataset: it has over 2.5 million rows. That makes sense as this file contains every stop of every vehicle involved in public transport on a given day. Thus, every row corresponds to a stop of a train, bus, or any other vehicle of public transport.\n\n# Missing values per column\ndf.isna().sum()\n\nBETRIEBSTAG                  0\nFAHRT_BEZEICHNER             0\nBETREIBER_ID                 0\nBETREIBER_ABK                0\nBETREIBER_NAME               0\nPRODUKT_ID                  31\nLINIEN_ID                    0\nLINIEN_TEXT                  0\nUMLAUF_ID              1362009\nVERKEHRSMITTEL_TEXT          0\nZUSATZFAHRT_TF               0\nFAELLT_AUS_TF                0\nBPUIC                        0\nHALTESTELLEN_NAME       169114\nANKUNFTSZEIT            149415\nAN_PROGNOSE             156828\nAN_PROGNOSE_STATUS      149200\nABFAHRTSZEIT            149426\nAB_PROGNOSE             157273\nAB_PROGNOSE_STATUS      149115\nDURCHFAHRT_TF                0\ndtype: int64\n\n\nWe can see that some columns contain many missing values. The only one I worry about for now is the column PRODUKT_ID. If you look through these rows (I don‚Äôt show that here), you can see that they should all be of type ‚ÄúZug‚Äù (train). Thus, we impute accordingly:\n\n# Impute 'Zug'\ndf.loc[df[\"PRODUKT_ID\"].isna(), \"PRODUKT_ID\"] = 'Zug'\n\nThere are quite a few date-timestamp columns that are not yet in the proper format. Thus, we now convert them to datetime formats:\n\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'], format = \"%d.%m.%Y\")\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\n\nNow is a good time to finally have a look at the dataframe:\n\n# Let's look at first few rows\ndf.head()\n\n\n\n\n\n\n\n\nBETRIEBSTAG\nFAHRT_BEZEICHNER\nBETREIBER_ID\nBETREIBER_ABK\nBETREIBER_NAME\nPRODUKT_ID\nLINIEN_ID\nLINIEN_TEXT\nUMLAUF_ID\nVERKEHRSMITTEL_TEXT\nZUSATZFAHRT_TF\nFAELLT_AUS_TF\nBPUIC\nHALTESTELLEN_NAME\nANKUNFTSZEIT\nAN_PROGNOSE\nAN_PROGNOSE_STATUS\nABFAHRTSZEIT\nAB_PROGNOSE\nAB_PROGNOSE_STATUS\nDURCHFAHRT_TF\n\n\n\n\n0\n2025-03-05\n80:800631:17230:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17230\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2025-03-05 04:59:00\n2025-03-05 04:59:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n1\n2025-03-05\n80:800631:17233:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17233\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\nNaT\nNaT\nNaN\n2025-03-05 06:07:00\n2025-03-05 06:08:00\nPROGNOSE\nFalse\n\n\n2\n2025-03-05\n80:800631:17234:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17234\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2025-03-05 05:56:00\n2025-03-05 06:02:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n3\n2025-03-05\n80:800631:17235:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17235\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\nNaT\nNaT\nNaN\n2025-03-05 06:43:00\n2025-03-05 06:53:00\nPROGNOSE\nFalse\n\n\n4\n2025-03-05\n80:800631:17236:000\n80:800631\nDB\nDB Regio AG Baden-W√ºrttemberg\nZug\n17236\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2025-03-05 06:31:00\n2025-03-05 06:34:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n\n\n\n\n\nBut what do all these columns mean? I have browsed the metadata a bit and found the following explanations (that I hopefully accurately reproduce in English):\n\nBETRIEBSTAG: Simply the day on which the data were recorded.\nFAHRT_BEZEICHNER: This is some elaborate identifier in the format [UIC-Countrycode]:[GO-Number]:[VM-Number]:[Extended Reference].\nBETREIBER_ID: [UIC-Countrycode]:[GO-Number]. GO is short for ‚ÄúGesch√§ftsorganisation‚Äù. For foreign organizations it is not a GO-Number but a TU-Number with TU meaning ‚ÄúTransportunternehmen‚Äù. It is basically an ID for the company running that particular train.\nBETREIBER_ABK: The abbreviation for the company running the train.\nBETREIBER_NAME: The full name of the company running the train.\nPRODUKT_ID: Type of public transport.\nLINIEN_ID: The ID for the route of that train.\nLINIEN_TEXT: The public ID for the route of that train.\nUMLAUF_ID: An ID for a ‚ÄúUmlauf‚Äù which describes the period starting with the vehicle leaving the garage and ending with the vehicle being deposited back in the garage.\nZUSATZFAHRT_TF: Is true if it is an extraordinary (not usually scheduled) trip.\nFAELLT_AUS_TF: Is true if the trip is cancelled.\nBPUIC: The ID of the station.\nHALTESTELLEN_NAME: The name of the station.\nANKUNFTSZEIT: Planned time of arrival at the station.\nAN_PROGNOSE: Prediction of time of arrival at the station.\nAN_PROGNOSE_STATUS: Status of that prediction. Possible values are: ‚ÄúUNBEKANNT‚Äù, ‚Äúleer‚Äù, ‚ÄúPROGNOSE‚Äù, ‚ÄúGESCHAETZT‚Äù, ‚ÄúREAL‚Äù. If the value of that column is ‚ÄúREAL‚Äù, it means that the predicted time of arrival is the time the train actually arrived at the station.\nABFAHRTSZEIT, AB_PROGNOSE, AB_PROGNOSE_STATUS: Same definitions as for arrival but here for departure from the station.\nDURCHFAHRT_TF: Is true if the vehicle does not stop even if a stop was scheduled.\n\nLet‚Äôs now have a look at the values in the column PRODUKT_ID:\n\n# Look at PRODUKT_ID\ndf[\"PRODUKT_ID\"].value_counts()\n\nPRODUKT_ID\nBus            1965207\nTram            249408\nZug             163649\nBUS             124171\nMetro             4936\nZahnradbahn       1944\nSchiff             975\nName: count, dtype: int64\n\n\nWe can see that trains are only the third most frequent category in this data. However, as mentioned before, we want to keep it simple and now reduce the dataset to only trains.\n\n# First we reduce to only trains\ndf = df[df['PRODUKT_ID'] == \"Zug\"]\n\nIn a next step, we remove all rows where the corresponding train has been cancelled.\n\n# Filter out all entries with FAELLT_AUS_TF == True\ndf = df[df['FAELLT_AUS_TF'] == False]\n\nLet‚Äôs explore the data a bit more before we move to the second dataset. Let‚Äôs check out the most frequent values that occur in the column BETREIBER_NAME:\n\n# Look at BETREIBER_NAME\ndf[\"BETREIBER_NAME\"].value_counts().head()\n\nBETREIBER_NAME\nSchweizerische Bundesbahnen SBB    63850\nBLS AG (bls)                       16256\nTHURBO                             13017\nAargau Verkehr AG                   7131\nSchweizerische S√ºdostbahn (sob)     6083\nName: count, dtype: int64\n\n\nAs expected, SBB is the company serving the largest number of stations. What about the column VERKEHRSMITTEL_TEXT?\n\n# Look at VERKEHRSMITTEL_TEXT\ndf[\"VERKEHRSMITTEL_TEXT\"].value_counts().head()\n\nVERKEHRSMITTEL_TEXT\nS     103331\nR      34435\nRE      9730\nIR      7532\nIC      3059\nName: count, dtype: int64\n\n\nWe can see that the most frequent type of trains are S-Bahns (S). Finally, let‚Äôs check the most frequent train stations that occur in the data:\n\n# Look at HALTESTELLEN_NAME\ndf[\"HALTESTELLEN_NAME\"].value_counts().head()\n\nHALTESTELLEN_NAME\nZ√ºrich HB          2239\nBern               1709\nWinterthur          955\nZ√ºrich Oerlikon     919\nLuzern              843\nName: count, dtype: int64\n\n\nUnsurprisingly, Z√ºrich and Bern are the most frequent values occuring in the data.\n\n\nLoad and preprocess ‚ÄúDienststellen-Daten‚Äù\nFortunately, we can go through the second dataset a bit more quickly. We again start by loading it and checking the dimensions of the dataframe.\n\n# Load the data\nds = pd.read_csv('actual_date-swiss-only-service_point-2025-03-06.csv', sep = \";\", low_memory = False)\n\n# Number of rows and columns\nprint(ds.shape)\n\n(55308, 55)\n\n\nThe data contains a column validTo that allows us to filter out all stations that are not valid anymore (closed down?). We check the values that appear in this column and see that all stations should be valid as of March 6, 2025. This is no surprise as we use the dataset of currently valid stations.\n\n# Check 'validTo' values.\nds['validTo'].unique()\n\narray(['9999-12-31', '2025-12-13', '2025-04-06', '2026-12-12',\n       '2025-08-29', '2027-12-11', '2025-03-16', '2025-05-30',\n       '2099-12-31', '2028-12-09', '2025-09-30', '2030-12-14',\n       '2025-06-30', '2050-12-31', '2029-12-09', '2025-08-31',\n       '2025-03-31', '2025-04-12', '2025-05-16', '2025-03-06'],\n      dtype=object)\n\n\nLet‚Äôs also quickly make sure that we have unique rows (based on ‚Äònumber‚Äô).\n\n# Is the number of unique 'number' (= BPUIC) values equal to the number of rows?\nlen(pd.unique(ds['number'])) == ds.shape[0]\n\nTrue\n\n\nFinally, we keep only the columns we need (identifier, official name, and geo coordinates).\n\n# Keep only the relevant columns\nds = ds[[\"number\",\"designationOfficial\",\"cantonName\",\"municipalityName\",\"businessOrganisationDescriptionEn\",\"wgs84East\",\"wgs84North\",\"height\"]]\n\n# Show first few rows\nds.head()\n\n\n\n\n\n\n\n\nnumber\ndesignationOfficial\ncantonName\nmunicipalityName\nbusinessOrganisationDescriptionEn\nwgs84East\nwgs84North\nheight\n\n\n\n\n0\n1322001\nAntronapiana\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.113620\n46.060120\n0.0\n\n\n1\n1322002\nAnzola d'Ossola\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.345715\n45.989869\n0.0\n\n\n2\n1322003\nBaceno\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.319256\n46.261501\n0.0\n\n\n3\n1322012\nCastiglione\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.214886\n46.020588\n0.0\n\n\n4\n1322013\nCeppo Morelli\nNaN\nNaN\nAutoservizi Comazzi S.R.L.\n8.069922\n45.971036\n0.0\n\n\n\n\n\n\n\n\n\nLoad and preprocess average traffic data\nThis part is also fairly easy. We load the data and check the dimensions, as always.\n\n# Load the data\nds_freq = pd.read_csv('t01x-sbb-cff-ffs-frequentia-2023.csv', sep = \";\", low_memory = False)\n\n# Number of rows and columns\nprint(ds_freq.shape)\n\n(3479, 14)\n\n\nIf you actually have a look at the data, you see that many stations have several measurements made at different times (and the times of measurements are identified by Jahr_Annee_Anno). We only want to keep the most recent measurements for every station:\n\n# For every station, we only keep the most recent measurements.\nds_freq = ds_freq.loc[ds_freq.groupby('UIC')['Jahr_Annee_Anno'].idxmax()]\n\nChecking the data types of all columns reveals that there is still a problem with the measurement columns DTV_TJM_TGM, DWV_TMJO_TFM, and DNWV_TMJNO_TMGNL. They are currently of type object because they contain the thousand separator ‚Äô. We thus remove all instances of this characters and transform these columns to integers.\n\n# Data types of columns\nds_freq.dtypes\n\n# Remove thousand separator and make integers out of it.\nds_freq['DTV_TJM_TGM'] = ds_freq['DTV_TJM_TGM'].str.replace('‚Äô', '').astype(int)\nds_freq['DWV_TMJO_TFM'] = ds_freq['DWV_TMJO_TFM'].str.replace('‚Äô', '').astype(int)\nds_freq['DNWV_TMJNO_TMGNL'] = ds_freq['DNWV_TMJNO_TMGNL'].str.replace('‚Äô', '').astype(int)\n\nFinally, we keep only the relevant columns.\n\n# Keep only the relevant columns\nds_freq = ds_freq[[\"UIC\",\"DTV_TJM_TGM\",\"DWV_TMJO_TFM\",\"DNWV_TMJNO_TMGNL\"]]\n\n\n# Show first few rows\nds_freq.head()\n\n\n\n\n\n\n\n\nUIC\nDTV_TJM_TGM\nDWV_TMJO_TFM\nDNWV_TMJNO_TMGNL\n\n\n\n\n411\n8500010\n98600\n105900\n81900\n\n\n423\n8500016\n90\n100\n60\n\n\n2024\n8500020\n5700\n7000\n2800\n\n\n2294\n8500021\n8500\n9900\n5200\n\n\n1072\n8500022\n3600\n4100\n2300\n\n\n\n\n\n\n\nBut what exactly are these three measurement variables? The source dataset provides the following definitions:\n\nDTV_TJM_TGM: ‚ÄúAverage daily traffic (Monday to Sunday).‚Äù\nDWV_TMJO_TFM: ‚ÄúAverage traffic on weekdays (Monday to Friday).‚Äù\nDNWV_TMJNO_TMGNL: ‚ÄúAverage non-work day traffic (Saturdays, Sundays and public holidays).‚Äù\n\nIt is further mentioned that all passengers boarding and exiting the trains are counted. That also means that passengers who switch trains are counted twice. For larger stations, the data may not cover all trains arriving and departing at the corresponding station. For example, the numbers for Bern do not include the traffic generated by the regional train company RBS.\n\n\nCombine the three datasets\nWe first merge the traffic data to the ‚ÄúDienststellen-Daten‚Äù:\n\n# Join to 'ds'\nds = pd.merge(ds, ds_freq, left_on = 'number', right_on = 'UIC', how = 'left')\n\n# Drop 'UIC'\nds = ds.drop('UIC', axis=1)\n\n# Better column names\nds.columns = ['BPUIC','STATION_NAME','CANTON','MUNICIPALITY','COMPANY',\n              'LONGITUDE','LATITUDE','ELEVATION','AVG_DAILY_TRAFFIC',\n              'AVG_DAILY_TRAFFIC_WEEKDAYS','AVG_DAILY_TRAFFIC_WEEKENDS']\n\nThen we merge the ‚ÄúDienststellen-Daten‚Äù to the ‚ÄúIst-Daten‚Äù via the BPUIC variable:\n\n# Left-join with station names and coordinates\ndf = pd.merge(df, ds, on = 'BPUIC', how = 'left')\n\nUnfortunately, there are some rows (18) for which HALTESTELLEN_NAME is missing. But fortunately, we know which stations are affected based on the STATION_NAME column that we have just merged from ds.\n\n# There are 18 missing values for 'HALTESTELLEN_NAME' which we impute from 'STATION_NAME'.\ndf.loc[df['HALTESTELLEN_NAME'].isna(), \"HALTESTELLEN_NAME\"] = df.loc[df['HALTESTELLEN_NAME'].isna(), \"STATION_NAME\"]\n\nNow, we are finally ready to start extracting the network from this data!\n\n\nConvert it to a network\nAs I mentioned several times, every row corresponds to a stop of a train at a train station. One train ride from some initial station to some end station (called ‚ÄúFahrt‚Äù in German) then typically consists of several stops along the way. However, there are some ‚ÄúFahrten‚Äù with only one entry. Presumably these are mostly foreign trains that have their final destination at some border station. I decided to remove those entries:\n\n# First group by FAHRT_BEZEICHNER and then filter out all groups with only one entry\n# It's mostly trains that stop at a place at the border (I think)\ndf_filtered = df.groupby('FAHRT_BEZEICHNER').filter(lambda g: len(g) &gt; 1)\n\n# How many rows do we loose with that?\nprint(df.shape[0] - df_filtered.shape[0])\n\n420\n\n\nThis preprocessing step removes 420 rows.\nNow we group the rows by FAHRT_BEZEICHNER so that each group is one ‚ÄúFahrt‚Äù. In every group we sort the stops along the way in an ascending order of the departure time.\n\n# Function to sort entries within a group in ascending order of ABFAHRTSZEIT\ndef sort_data(group):\n    return group.sort_values('ABFAHRTSZEIT', ascending = True)\n\n# Sort for each group\ndf_sorted = df_filtered.groupby('FAHRT_BEZEICHNER', group_keys=True).apply(sort_data)\n\nLet‚Äôs have a look at one ‚ÄúFahrt‚Äù to get a better idea:\n\n# Look at one example Fahrt\ndf_sorted.loc[['85:22:1083:000'],['BETREIBER_NAME','LINIEN_TEXT','HALTESTELLEN_NAME','ABFAHRTSZEIT']]\n\n\n\n\n\n\n\n\n\nBETREIBER_NAME\nLINIEN_TEXT\nHALTESTELLEN_NAME\nABFAHRTSZEIT\n\n\nFAHRT_BEZEICHNER\n\n\n\n\n\n\n\n\n\n85:22:1083:000\n64346\nAppenzeller Bahnen (ab)\nS23\nGossau SG\n2025-03-05 08:21:00\n\n\n64347\nAppenzeller Bahnen (ab)\nS23\nHerisau\n2025-03-05 08:28:00\n\n\n64348\nAppenzeller Bahnen (ab)\nS23\nHerisau Wilen\n2025-03-05 08:30:00\n\n\n64349\nAppenzeller Bahnen (ab)\nS23\nWaldstatt\n2025-03-05 08:34:00\n\n\n64350\nAppenzeller Bahnen (ab)\nS23\nZ√ºrchersm√ºhle\n2025-03-05 08:39:00\n\n\n64351\nAppenzeller Bahnen (ab)\nS23\nUrn√§sch\n2025-03-05 08:43:00\n\n\n64352\nAppenzeller Bahnen (ab)\nS23\nJakobsbad\n2025-03-05 08:48:00\n\n\n64353\nAppenzeller Bahnen (ab)\nS23\nGonten\n2025-03-05 08:50:00\n\n\n64354\nAppenzeller Bahnen (ab)\nS23\nGontenbad\n2025-03-05 08:52:00\n\n\n64355\nAppenzeller Bahnen (ab)\nS23\nAppenzell\nNaT\n\n\n\n\n\n\n\nThis is a train that goes from Gossau to Appenzell with many stops in-between. In Appenzell the ABFAHRTSZEIT is missing as that ‚ÄúFahrt‚Äù ends there (the train will most likely go back in the other direction, but that will be a new ‚ÄúFahrt‚Äù).\nWe now have enough knowledge about the data that we can extract the edges in a for loop. Basically, what we do is to loop over the rows of a given ‚ÄúFahrt‚Äù, starting with the second row and extracting the edges as\n(previous station, current station, travel time between stations).\nThe Python code for this looks as follows:\n\n# Empty list\nedgelist = []\n\n# Variables to store previous row and its index\nprev_row = None\nprev_idx = None\n\n# Loop over rows of dataframe\nfor i, row in df_sorted.iterrows():\n    # Only start with second row\n    # Only if the two rows belong to the same Fahrt\n    if prev_idx is not None and prev_idx == i[0]:\n        # Add edge to edgelist assuming it's a directed edge\n        edgelist.append((prev_row['STATION_NAME'], \n                         row['STATION_NAME'], \n                         (row['ANKUNFTSZEIT'] - prev_row['ABFAHRTSZEIT']).total_seconds() / 60))\n    # Set current row and row index to previous ones\n    prev_idx = i[0]\n    prev_row = row\n\nTo get a better idea, let‚Äôs have a look at the first list element:\n\n# First list element\nedgelist[0]\n\n('Z√ºrich HB', 'Basel SBB', 54.0)\n\n\nWe are still not quite done yet. The problem is that the edgelist contains many duplicated entries as, for example, the stretch Z√ºrich HB - Basel SBB is served by many different trains on a given day.\nWhat we want to do is to go through all possible edges and sum up the number of times they occur. In addition, we would like to average the travel time between a given pair of stations over all trips between the two stations. The following code does exactly that and saves the result in the form of a dictionary.\n\n# Empty dict\nedges = {}\n\n# Loop over elements in edgelist\nfor i in edgelist:\n    # Create key\n    key = (i[0], i[1])\n    # Get previous entries in dict (if there are any)\n    prev = edges.get(key, (0, 0))\n    # Update values in dict\n    edges[key] = (prev[0] + 1, prev[1] + i[2])\n\n# Divide summed up travel times by number of trips\nedges = {k: (v[0], round(v[1]/v[0], 2)) for k, v in edges.items()}\n\nLet‚Äôs look at the entry for the stretch between Schaffhausen and Basel Badischer Bahnhof again:\n\n# Look at some element in dict\nedges[('Z√ºrich HB', 'Basel SBB')]\n\n(36, 54.0)\n\n\nThere are 36 trips between these two stations (in this direction) and they take 54 minutes on average.\nWe are now ready to create the final node list (and export it). First, we reduce ds to the train stations that actually appear in the edges (it still contains many bus and tram stops and other things).\n\n# Set of stations that appear in edgelist\nstations_in_edgelist = set(sum(list(edges.keys()), ()))\n\n# Reduces nodes dataframe to only places in edgelist\nnodes = ds[ds['STATION_NAME'].isin(stations_in_edgelist)]\n\nSecond, we quickly check the number of missing values again.\n\n# Missing values per column\nnodes.isna().sum()\n\nBPUIC                           0\nSTATION_NAME                    0\nCANTON                         21\nMUNICIPALITY                   21\nCOMPANY                         0\nLONGITUDE                       0\nLATITUDE                        0\nELEVATION                       1\nAVG_DAILY_TRAFFIC             500\nAVG_DAILY_TRAFFIC_WEEKDAYS    500\nAVG_DAILY_TRAFFIC_WEEKENDS    500\ndtype: int64\n\n\nThere are still some issues here. The one we can solve is the missing elevation. The station Tirano (in Italy) has no value for this column. We simply impute manually (Tirano‚Äôs elevation is approximately 441m).\n\n# Impute missing elevation for Tirano\nnodes.loc[nodes['STATION_NAME'] == \"Tirano\", \"ELEVATION\"] = 441\n\nThe missing values for CANTON and MUNICIPALITY concern municipalities abroad (in Germany and Italy mostly). The 500 missing values in the traffic columns are stations are run by smaller companies or stations abroad. There is nothing we can do about all these missing values.\n\n# Have a look\nnodes.head()\n\n# Export node list\n# nodes.sort_values(\"BPUIC\").to_csv(\"nodelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC\nSTATION_NAME\nCANTON\nMUNICIPALITY\nCOMPANY\nLONGITUDE\nLATITUDE\nELEVATION\nAVG_DAILY_TRAFFIC\nAVG_DAILY_TRAFFIC_WEEKDAYS\nAVG_DAILY_TRAFFIC_WEEKENDS\n\n\n\n\n12683\n8500100\nTavannes\nBern\nTavannes\nSwiss Federal Railways SBB\n7.201645\n47.219845\n754.17\n1400.0\n1600.0\n810.0\n\n\n12684\n8500121\nCourfaivre\nJura\nHaute-Sorne\nSwiss Federal Railways SBB\n7.291166\n47.335083\n450.99\n420.0\n480.0\n280.0\n\n\n12685\n8500103\nSorvilier\nBern\nSorvilier\nSwiss Federal Railways SBB\n7.305794\n47.239354\n681.07\n60.0\n70.0\n49.0\n\n\n12688\n8500120\nCourt√©telle\nJura\nCourt√©telle\nSwiss Federal Railways SBB\n7.317943\n47.342829\n436.90\n840.0\n970.0\n550.0\n\n\n12689\n8500102\nMalleray-B√©vilard\nBern\nValbirse\nSwiss Federal Railways SBB\n7.275946\n47.238714\n698.18\n630.0\n780.0\n280.0\n\n\n\n\n\n\n\nBefore we export the edges, we change the station names in the edgelist to the BPUIC to make the edges more compact. Then we transform the dictionary into a dataframe which can finally be exported.\n\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[k[0]], node_dict[k[1]], v[0], v[1]] for k,v in edges.items()]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','NUM_CONNECTIONS','AVG_DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC1\nBPUIC2\nNUM_CONNECTIONS\nAVG_DURATION\n\n\n\n\n0\n8503000\n8500010\n36\n54.00\n\n\n1\n8500010\n8500090\n67\n6.07\n\n\n2\n8500090\n8500010\n67\n6.39\n\n\n3\n8500010\n8503000\n39\n57.87\n\n\n4\n8503424\n8500090\n17\n73.18\n\n\n\n\n\n\n\nFeel free to download the final results: Nodelist (CSV) and Edgelist (CSV).\nThe title image has been created by Wikimedia user JoachimKohler-HB and is licensed under Creative Commons."
  },
  {
    "objectID": "news/snf_epi/index.html",
    "href": "news/snf_epi/index.html",
    "title": "New SNSF project",
    "section": "",
    "text": "In March 2024, my colleagues Lorenz Hilfiker, Matthias Templ, and I submitted a project proposal to the Swiss National Science Foundation‚Äôs (SNSF) second Health and Wellbeing Call.\nIn November 2024, we received the exciting news that our project has been awarded funding! With a grant of CHF 391,938, we will be able to hire a full-time PhD student to support our research. Additionally, Lorenz will join Matthias and me at Fachhochschule Nordwestschweiz (FHNW) in 2025, starting in a part-time position.\nOver the next four years, our team‚Äîincluding the PhD student‚Äîwill work on developing new methods to trace the origins of epidemic outbreaks in contact networks. While researchers have been exploring epidemic source detection for about 15 years, many existing studies focus on simplified models and small networks, limiting their practical applications. Our project aims to address these challenges by pursuing two key goals:\n\nScalability: Developing methods that can be applied to large-scale contact networks.\nReal-world applicability: Designing techniques that account for uncertainty in key parameters, making them more suitable for practical use.\n\nOur research is not limited to human pandemics‚Äîit also has significant applications in the agricultural and livestock industries, where regulatory requirements often ensure the availability of contact data.\nUltimately, we hope our methods will become valuable tools for preventing future outbreaks, whether in human or veterinary health. We are thrilled to embark on this journey and look forward to the discoveries ahead!"
  },
  {
    "objectID": "news/blog/index.html",
    "href": "news/blog/index.html",
    "title": "First blogpost",
    "section": "",
    "text": "I‚Äôve started a blog right here on this website (see here). I know the internet is already overflowing with blogs and other content, and the world certainly wasn‚Äôt waiting for yet another person eager to share their thoughts.\nBut honestly, this blog is, above all, a creative outlet for me. Writing my first post was a lot of fun, and if others find it interesting‚Äîgreat! If not, no hard feelings. üòâ\nThat said, this first post took a surprising amount of work (a full workday, if we‚Äôre being honest‚Äîdon‚Äôt tell my boss). So, I won‚Äôt be posting on a fixed schedule. Instead, whenever I have an interesting analysis that isn‚Äôt quite enough for a formal publication, a cool data visualization, or a neat tutorial on a model or algorithm‚Äîand, crucially, some spare time‚ÄîI‚Äôll share it here.\nSpeaking of which, my first post explores how to use Swiss public transport data to create a directed network. I plan to turn this into a series of blogposts, applying different network science concepts to that network.\nThe title image has been created by Wikimedia user Cortega9 and is licensed under Creative Commons."
  },
  {
    "objectID": "code/index.html",
    "href": "code/index.html",
    "title": "Code",
    "section": "",
    "text": "Shiny Apps\nAs part of my Postdoc employment with Prof.¬†Dr.¬†Carolin Strobl at the University of Zurich, I developed two Shiny apps to help psychology students understand the assumptions of (simple) linear regression models. In the process, I explored base R‚Äôs plotting capabilities and learned a great deal about how to make visually appealing plots using only base R.\nThe first app visually demonstrates how the distribution of predictors (\\(x_i\\)) and error terms (\\(\\epsilon_i\\)) influence both the marginal and conditional distributions of the dependent variable (\\(y_i\\)). The key takeaway we attempt to convey to students is that linear regression models impose distributional assumptions on the error terms (\\(\\epsilon_i\\)), not directly on \\(y_i\\).\n\n\n\nKey figure in the first app\n\n\nThe second app is more comprehensive and illustrates the five key assumptions of linear regression, as typically taught in statistics courses for psychology students (at least at the University of Zurich). The core idea is to explore each assumption by:\n\nDemonstrating how it may be violated and what that visually looks like.\nShowing the consequences of a violation.\nPresenting possible remedies to address the violation of the assumption.\n\nFor example, consider the assumption of homoscedasticity. The app first provides a visual representation of how a violation appears in typical regression plots. A small simulation experiment then reveals that, when this assumption is violated, the coverage probability of 95% confidence intervals drops to 85%. Finally, two potential remedies are introduced: robust standard errors and weighted least squares. The app allows the user to test the effect of those remedies.\n\n\n\nResiduals vs.¬†fitted values in case of heteroscedasticity"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "News",
    "section": "",
    "text": "First blogpost\n\n\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nNetSci 2025, here we come\n\n\n\n\n\n\nConferences\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nNew SNSF project\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Journals\n\nSterchi, M., Hilfiker, L., Gr√ºtter, R., Bernstein, A. (2023). Active Querying Approach to Epidemic Source Detection on Contact Networks. Scientific Reports, 13(1). https://doi.org/10.1038/s41598-023-38282-8\nSterchi, M., Sarasua, C., Gr√ºtter, R., Bernstein, A. (2021). Outbreak detection for temporal contact data. Applied Network Science, 6(17). https://doi.org/10.1007/s41109-021-00360-z\nSterchi, M., Faverjon, C., Sarasua, C., Vargas, ME., Berezowski, J., Bernstein, A., Gr√ºtter, R., Nathues, H. (2019). The pig transport network in Switzerland: Structure, patterns, and implications for the transmission of infectious diseases between animal holdings. PLOS ONE, 14(5): e0217974. https://doi.org/10.1371/journal.pone.0217974\nFaverjon, C., Bernstein, A., Gr√ºtter, R., Nathues, C., Nathues, H., Sarasua, C., Sterchi, M., Vargas, ME., Berezowski, J. (2019). A Transdisciplinary Approach Supporting the Implementation of a Big Data Project in Livestock Production: An Example From the Swiss Pig Production Industry. Frontiers in Veterinary Science, 6(215). https://doi.org/10.3389/fvets.2019.00215\nSt√∂ckli, S., Messner, C., Sterchi, M., Dorn, M. (2019). Unreliable is Better: Theoretical and Practical Impulses for Performance Management. Die Unternehmung, 73(2), 167-180. https://www.jstor.org/stable/26696764\nHulliger, B. & Sterchi, M. (2018). A survey-based design of a pricing system for psychotherapy. Health Economics Review, 8(29), 2-11. https://doi.org/10.1186/s13561-018-0213-7\n\n\n\nProceedings\n\nSterchi, M., Sarasua, C., Gr√ºtter, R., Bernstein, A. (2019). Maximizing the Likelihood of Detecting Outbreaks in Temporal Networks. In: Proceedings of the 8th International Conference on Complex Networks and Their Applications, COMPLEX NETWORKS, 2019. Oral Presentation. https://doi.org/10.1007/978-3-030-36683-4_39\nSterchi, M. & Wolf, M. (2017). Weighted Least Squares and Adaptive Least Squares: Further Empirical Evidence. In: Kreinovic, Vladik; Sriboonchitta, Songsak; Huynh, Van-Nam. Robustness in Econometrics. Cham: Springer, 135-167. https://doi.org/10.1007/978-3-319-50742-2_9\n\n\n\nAbstracts\n\nSterchi, M., Hilfiker, L. (2025). Evaluating Graph Neural Networks for Epidemic Source Detection: A Benchmark Study (Abstract). International School and Conference on Network Science, NetSci, (upcoming, June 2025). Poster Presentation.\nSterchi, M., Hilfiker, L., Gr√ºtter, R., Bernstein, A. (2023). Active learning for epidemic source detection (Extended Abstract). Book of Abstracts of the 12th International Conference on Complex Networks and their Applications, COMPLEX NETWORKS, 2023. Oral Presentation.\nHilfiker, L., Sterchi, M. (2021). Covid‚Äë19 superspreading: lessons from simulations on an empirical contact network (Extended Abstract). Book of Abstracts of the 10th International Conference on Complex Networks and their Applications, COMPLEX NETWORKS, 2021. Poster Presentation."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "I am a lecturer at the School of Business at the University of Applied Sciences and Arts Northwestern Switzerland, or short FHNW. Additionally, I hold a part-time Postdoctoral position at the Chair of Psychological Methods, Evaluation and Statistics at the University of Zurich, led by Prof.¬†Dr.¬†Carolin Strobl.\nI have a background in economics, having studied at the Universities of Fribourg and Zurich, and hold a PhD in Computer Science from the University of Zurich. My doctoral research, titled Computational Approaches to Epidemic Prevention on Contact Networks, focused on developing methods to mitigate the spread of a disease when at least partial knowledge of the underlying contact network is available.\nAt FHNW I teach the following modules (more info):\n\nMachine Learning: This course is designed for business students, emphasizing the practical application of ML to real-world problems. We primarily use the tidymodels framework, building on students‚Äô prior knowledge of R.\nApplied Data Science: Currently, this week-long intensive course focuses on teaching students data visualization skills in R using ggplot2.\nBusiness Analytics: In this core module, I introduce students to programming in R and guide them through the fundamentals of linear regression.\n\nMy research interests are as follows:\n\nDynamical processes on networks: My primary research interest lies in epidemic processes, which was the focus of my PhD and remains a central theme in my current work.\nMonte Carlo simulations: For many problems, analytical solutions do not exist, and as a computer scientist, I am not reluctant to use simulations to find approximate solutions.\nGraph Neural Networks: Network science has not escaped the Deep Learning revolution‚Äîand naturally, GNNs have caught my attention. Specifically, I am curious if GNNs can help address the challenge of source detection.\nExplainability: Recently, I have begun exploring model explainability, primarily through visualizations‚Äîanother passion of mine. I plan to further investigate this topic, particularly in the context of models on networks.\n\nI enjoy implementing Machine Learning algorithms from scratch to deepen my understanding at the implementation level. Moreover, I can spend hours refining data visualizations‚Äîwhether in base R, ggplot2, or matplotlib‚Äîstriving for a balance of aesthetics, clarity, and engagement.\nI live in Switzerland together with my partner and our two adorable‚Äîyet incredibly demanding‚Äîcats.\n\n\n\nTwo bundles of joy"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching at the University of Applied Sciences and Arts Northwestern Switzerland (FHNW) centers around three key modules, outlined below.\n\nMachine Learning\nThis module is one of four within the Managerial Data Science specialization, available to both Business and International Management students at FHNW during the final year of their Bachelor‚Äôs studies.\nMany students in this specialization have little to no prior programming experience and only a modest background in mathematics and statistics. Teaching a subject rooted in advanced concepts from both fields might seem daunting in this context. However, I view it as an exciting challenge. Over the years, several key decisions have proven particularly effective.\nThe first was to clearly position this module as applied Machine Learning. Our primary goal is to solve real-world problems using off-the-shelf models and algorithms. Every topic is introduced through practical examples‚Äîpreferably from a business context‚Äîand accompanied by R code to demonstrate each model or technique.\nThe second decision was to avoid making this a no-math course. Instead, we explore mathematical concepts when they provide deeper insights into a model‚Äôs inner workings or enhance intuitive understanding. I believe that even students with a minimal math background can grasp complex topics if they are broken down into univariate or bivariate examples, where visualization is straightforward and the (linear) algebra remains simple.\nFinally, I adopted a flipped classroom approach. Based on my experience, students learn best when they can explore new topics at their own pace, in a setting of their choice. To support this, I developed interactive online tutorials using the learnr package, featuring text, Shiny apps, visualizations, R code snippets, and occasional multiple-choice questions. In-class sessions are then dedicated to deepening the material, working through exercises together, and addressing questions or areas of particular interest.\nBelow are two visualizations I created to help clarify topics that students often find challenging.\n\n\n\nHow R transforms a linear scale into a log scale.\n\n\nThe first illustrates the effect of using a log scale (base 10) versus a linear scale in R. It shows how three values (10, 500, and 100,000) are represented differently when plotted on a log scale. The figure should be interpreted from bottom to top.\n\n\n\nHow a simple convolution filter works on a simplified MNIST image.\n\n\nThe second visualization explains how a simple convolution filter operates using a simplified (binary) MNIST image of a handwritten digit. I manually designed an image of the number 7 to clearly highlight the pixel-level computations during the convolution process.\n\n\nApplied Data Science\nThe current topic of the Applied Data Science course (a week-long intensive course) is data visualization. This elective course is offered annually in November. In the course, we use the tidyverse framework, with a particular focus on the ggplot2 package to create visualizations.\nTo demonstrate the key principles of high-quality visualization, I wrote a bunch of code to generate engaging plots, all based on real data from the Swiss canton of Basel-Landschaft. This data is available on the canton‚Äôs open government data portal. The goal was to walk students through the essential steps of creating effective visualizations using a practical case study.\nSome results from this case study are shown below. The first figure is a scatterplot illustrating the relationship between two quantitative variables‚Äîtax rates and land prices‚Äîand demonstrates how additional information can be integrated into the plot using point color and size.\n\n\n\nVisualizing the relationship between two quantitative variables.\n\n\nThe second figure presents CO2 emissions (in tons per capita) across all communities in the canton. This figure serves as an example of geographic visualization using a choropleth map.\n\n\n\nVisualizing a quantitative variable geographically.\n\n\nFinally, the third figure showcases how to visualize time series data. Here, we display the daily measurements of nitrogen dioxide and ozone concentrations, as recorded by the monitoring station in Sissach-B√ºtzenen.\n\n\n\nVisualizing two time series.\n\n\n\n\nBusiness Analytics\nBusiness Analytics (BA) is one of two components of the core module Empirische Methoden und Business Analytics, which is mandatory for all Business students at FHNW.\nThe BA section is structured around two main areas of focus:\n\nIntroduction to R and Data Handling: Students begin by working with R and RStudio, gaining a basic understanding of key data structures such as vectors, factors, and data frames. They learn how to import data, manipulate these basic structures, and apply indexing. The course also covers the calculation of simple descriptive statistics and introduces R‚Äôs base plotting capabilities for data visualization.\nFundamentals of Linear Regression: The second part of the course delves into simple and multiple linear regression. Students explore the intuition behind least-squares coefficients and learn how the coefficient of determination can be used for model evaluation and comparison. In the context of multiple linear regression, the course covers topics such as dummy encoding for categorical variables and the use of interaction terms, among other techniques.\n\nThis structure ensures that students not only develop a basic technical proficiency in data analysis using R but also gain a solid understanding of essential statistical modeling concepts."
  },
  {
    "objectID": "news/netsci25/index.html",
    "href": "news/netsci25/index.html",
    "title": "NetSci 2025, here we come",
    "section": "",
    "text": "My colleague Lorenz Hilfiker and I have been accepted for a poster presentation at NetSci2025! We look forward to traveling to Maastricht (NL) this June to share our work with experts in the field.\nOur research explores the potential of Graph Neural Networks (GNNs) for epidemic source detection‚Äîidentifying the origin of an outbreak given a contact network and the epidemic states of all nodes.\nOur (still preliminary) findings suggest that GNNs may not yet live up to their promise, as they are consistently outperformed by more traditional source detection methods. However, we have only explored a small region of the GNN design space so far, so it is too early to rule them out as a valuable tool for this problem.\nIf you‚Äôre interested, check out our abstract below‚Äîand stay tuned for a full paper on this topic!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "SBB Network Analysis - Part 1\n\n\n\n\n\n\nNetworks\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\nNo matching items"
  }
]