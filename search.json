[
  {
    "objectID": "blog/sbb_nw/index.html",
    "href": "blog/sbb_nw/index.html",
    "title": "SBB Network Analysis - Part 1",
    "section": "",
    "text": "For quite some time I have been wondering if there are some interesting Swiss data that would serve as the basis for some fun network analysis. As a fan of public transportation and a long-time owner of a Swiss train pass (“GA”), the answer should have been obvious much sooner: the Swiss public transport network.\nI wanted to create a (static) network in which each node corresponds to a train station and each directed edge between any two nodes, A and B, means there is at least one train going nonstop from A to B. Ideally, the edge would also be attributed with some weight representing the importance of the edge (e.g., how many trains go nonstop from A to B on a given day).\nThe structure of this post is as follows. I will first introduce the two datasets that I used to create the network. I will then show how to load and preprocess each one of them and how to join them. Finally, I will present how to transform those data into a form that is suitable for network analysis.\nThis is the first part of a series that will cover all kinds of fun network analysis based on the Swiss railway network.\n\nData sources\nIt was not that obvious how a network with nodes and edges following the definitions given above could be constructed based on data from the Swiss Federal Railways (abbreviated by the German speakers in Switzerland as SBB). With some help from SBB Experts and the SBB Open Data Plattform, I finally found the right data.\nThe first dataset is called “Ist-Daten” and, for a given day, contains all regular stops of all trains in Switzerland with their planned and effective arrival and departure times. From this data, we can infer all nonstop stretches of any train in Switzerland.\nNote that the “Ist-Daten” not only contain the data for trains but also for all other public transport (buses, trams, and even boats). To keep things simple we will focus on the train network.\nThe second dataset is the “Dienststellen-Daten” which basically allows to add node attributes such as the geographic coordinates of a node (i.e., a train station).\n\n\nLoad and preprocess “Ist-Daten”\nHere, we will load and preprocess the “Ist-Daten” from which we can derive the edges of our network. First, I import some Python libraries and print their version number for better reproducibility of this code.\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Check versions of libraries.\nprint(\"NumPy version:\", np.__version__)\nprint(\"Pandas version:\", pd.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n\nNumPy version: 1.26.4\nPandas version: 2.1.4\n\n\nLet’s now load the data. You can see in the filename that I downloaded the “Ist-Daten” from the SBB portal for June 6, 2023. You can get the data for any day you want here.\n\n# Load the data\ndf = pd.read_csv('2023-06-06_istdaten.csv', sep=\";\", low_memory=False) \n\nTo get a feeling for the data, let’s check the number of rows and columns.\n\n# Number of rows and columns\nprint(df.shape)\n\n(2309817, 21)\n\n\nOk, it’s actually a pretty big dataset: it has over 2.3 million rows. That makes sense as this file contains every stop of every vehicle involved in public transport on a given day. Thus, every row corresponds to a stop of a train, bus, or any other vehicle of public transport.\n\n# Missing values per column\ndf.isna().sum()\n\nBETRIEBSTAG                  0\nFAHRT_BEZEICHNER             0\nBETREIBER_ID                 0\nBETREIBER_ABK                0\nBETREIBER_NAME               0\nPRODUKT_ID                 233\nLINIEN_ID                    0\nLINIEN_TEXT                  0\nUMLAUF_ID              1196819\nVERKEHRSMITTEL_TEXT          0\nZUSATZFAHRT_TF               0\nFAELLT_AUS_TF                0\nBPUIC                        0\nHALTESTELLEN_NAME       158400\nANKUNFTSZEIT            138774\nAN_PROGNOSE             164786\nAN_PROGNOSE_STATUS      138560\nABFAHRTSZEIT            138604\nAB_PROGNOSE             165447\nAB_PROGNOSE_STATUS      138351\nDURCHFAHRT_TF                0\ndtype: int64\n\n\nWe can see that some columns contain many missing values. The only one I worry about for now is the column PRODUKT_ID. If you look through these rows (I don’t show that here), you can see that they should all be of type “Zug” (train). Thus, we impute accordingly:\n\n# Impute 'Zug'\ndf.loc[df[\"PRODUKT_ID\"].isna(), \"PRODUKT_ID\"] = 'Zug'\n\nThere are quite a few date-timestamp columns that are not yet in the proper format. Thus, we now convert them to datetime formats:\n\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'], format = \"%d.%m.%Y\")\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\n\nNow is a good time to finally have a look at the dataframe:\n\n# Let's look at first few rows\ndf.head()\n\n\n\n\n\n\n\n\nBETRIEBSTAG\nFAHRT_BEZEICHNER\nBETREIBER_ID\nBETREIBER_ABK\nBETREIBER_NAME\nPRODUKT_ID\nLINIEN_ID\nLINIEN_TEXT\nUMLAUF_ID\nVERKEHRSMITTEL_TEXT\nZUSATZFAHRT_TF\nFAELLT_AUS_TF\nBPUIC\nHALTESTELLEN_NAME\nANKUNFTSZEIT\nAN_PROGNOSE\nAN_PROGNOSE_STATUS\nABFAHRTSZEIT\nAB_PROGNOSE\nAB_PROGNOSE_STATUS\nDURCHFAHRT_TF\n\n\n\n\n0\n2023-06-06\n80:800631:17230:000\n80:800631\nDB\nDB Regio AG Baden-Württemberg\nZug\n17230\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2023-06-06 04:59:00\n2023-06-06 04:59:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n1\n2023-06-06\n80:800631:17233:000\n80:800631\nDB\nDB Regio AG Baden-Württemberg\nZug\n17233\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\nNaT\nNaT\nNaN\n2023-06-06 06:07:00\n2023-06-06 06:07:00\nPROGNOSE\nFalse\n\n\n2\n2023-06-06\n80:800631:17234:000\n80:800631\nDB\nDB Regio AG Baden-Württemberg\nZug\n17234\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2023-06-06 05:56:00\n2023-06-06 05:58:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n3\n2023-06-06\n80:800631:17235:000\n80:800631\nDB\nDB Regio AG Baden-Württemberg\nZug\n17235\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\nNaT\nNaT\nNaN\n2023-06-06 06:43:00\n2023-06-06 06:43:00\nPROGNOSE\nFalse\n\n\n4\n2023-06-06\n80:800631:17236:000\n80:800631\nDB\nDB Regio AG Baden-Württemberg\nZug\n17236\nRB\nNaN\nRB\nFalse\nFalse\n8500090\nBasel Bad Bf\n2023-06-06 06:33:00\n2023-06-06 06:34:00\nPROGNOSE\nNaT\nNaT\nNaN\nFalse\n\n\n\n\n\n\n\nBut what do all these columns mean? I have browsed the metadata a bit and found the following explanations (that I hopefully accurately reproduce in English):\n\nBETRIEBSTAG: Simply the day on which the data were recorded.\nFAHRT_BEZEICHNER: This is some elaborate identifier in the format [UIC-Countrycode]:[GO-Number]:[VM-Number]:[Extended Reference].\nBETREIBER_ID: [UIC-Countrycode]:[GO-Number]. GO is short for “Geschäftsorganisation”. For foreign organizations it is not a GO-Number but a TU-Number with TU meaning “Transportunternehmen”. It is basically an ID for the company running that particular train.\nBETREIBER_ABK: The abbreviation for the company running the train.\nBETREIBER_NAME: The full name of the company running the train.\nPRODUKT_ID: Type of public transport.\nLINIEN_ID: The ID for the route of that train.\nLINIEN_TEXT: The public ID for the route of that train.\nUMLAUF_ID: An ID for a “Umlauf” which describes the period starting with the vehicle leaving the garage and ending with the vehicle being deposited back in the garage.\nZUSATZFAHRT_TF: Is true if it is an extraordinary (not usually scheduled) trip.\nFAELLT_AUS_TF: Is true if the trip is cancelled.\nBPUIC: The ID of the station.\nHALTESTELLEN_NAME: The name of the station.\nANKUNFTSZEIT: Planned time of arrival at the station.\nAN_PROGNOSE: Prediction of time of arrival at the station.\nAN_PROGNOSE_STATUS: Status of that prediction. Possible values are: “UNBEKANNT”, “leer”, “PROGNOSE”, “GESCHAETZT”, “REAL”. If the value of that column is “REAL”, it means that the predicted time of arrival is the time the train actually arrived at the station.\nABFAHRTSZEIT, AB_PROGNOSE, AB_PROGNOSE_STATUS: Same definitions as for arrival but here for departure from the station.\nDURCHFAHRT_TF: Is true if the vehicle does not stop even if a stop was scheduled.\n\nLet’s now have a look at the values in the column PRODUKT_ID:\n\n# Look at PRODUKT_ID\ndf[\"PRODUKT_ID\"].value_counts()\n\nPRODUKT_ID\nBus            1746016\nTram            247305\nZug             162038\nBUS             147531\nMetro             4304\nZahnradbahn       1965\nSchiff             658\nName: count, dtype: int64\n\n\nWe can see that trains are only the third most frequent category in this data. However, as mentioned before, we want to keep it simple and now reduce the dataset to only trains.\n\n# First we reduce to only trains\ndf = df[df['PRODUKT_ID'] == \"Zug\"]\n\nIn a next step, we remove all rows where the corresponding train has been cancelled.\n\n# Filter out all entries with FAELLT_AUS_TF == True\ndf = df[df['FAELLT_AUS_TF'] == False]\n\nLet’s explore the data a bit more before we move to the second dataset. Let’s check out the most frequent values that occur in the column BETREIBER_NAME:\n\n# Look at BETREIBER_NAME\ndf[\"BETREIBER_NAME\"].value_counts().head()\n\nBETREIBER_NAME\nSchweizerische Bundesbahnen SBB    61798\nBLS AG (bls)                       15842\nTHURBO                             13250\nAargau Verkehr AG                   7220\nRhätische Bahn                      5366\nName: count, dtype: int64\n\n\nAs expected, SBB is the company serving the largest number of stations. What about the column VERKEHRSMITTEL_TEXT?\n\n# Look at VERKEHRSMITTEL_TEXT\ndf[\"VERKEHRSMITTEL_TEXT\"].value_counts().head()\n\nVERKEHRSMITTEL_TEXT\nS     105288\nR      31987\nRE      8551\nIR      7104\nIC      2891\nName: count, dtype: int64\n\n\nWe can see that the most frequent type of trains are S-Bahns (S). Finally, let’s check the most frequent train stations that occur in the data:\n\n# Look at HALTESTELLEN_NAME\ndf[\"HALTESTELLEN_NAME\"].value_counts().head()\n\nHALTESTELLEN_NAME\nZürich HB          1914\nBern               1706\nWinterthur          948\nZürich Oerlikon     918\nLuzern              814\nName: count, dtype: int64\n\n\nUnsurprisingly, Zürich and Bern are the most frequent values occuring in the data.\n\n\nLoad and preprocess “Dienststellen-Daten”\nFortunately, we can go through the second dataset a bit more quickly. We again start by loading it and checking the dimensions of the dataframe.\n\n# Load the data\nds = pd.read_csv('dienststellen_full.csv', sep = \";\", low_memory=False)\n\n# Number of rows and columns\nprint(ds.shape)\n\n(152433, 78)\n\n\nThe data contains a column GUELTIG_BIS that allows us to filter out all stations that are not valid anymore (closed down?). But first we need to transform it into the proper format.\n\n# GUELTIG_BIS as datetime\nds['GUELTIG_BIS'] = pd.to_datetime(ds['GUELTIG_BIS'], format = \"%Y-%m-%d\")\n\n# Keep only currently valid entries\nds = ds[ds['GUELTIG_BIS'] == \"2099-12-31\"]\n\nFinally, we keep only the columns we need (identifier, official name, and geo coordinates).\n\n# Keep only the relevant columns\nds = ds[[\"BPUIC\",\"BEZEICHNUNG_OFFIZIELL\",\"E_WGS84\",\"N_WGS84\",\"Z_WGS84\"]]\n\n# Show first few rows\nds.head()\n\n\n\n\n\n\n\n\nBPUIC\nBEZEICHNUNG_OFFIZIELL\nE_WGS84\nN_WGS84\nZ_WGS84\n\n\n\n\n26\n8531284\nSamnaun-Ravaisch (Talst. I)\n10.375228\n46.951317\n1775.0\n\n\n99\n8530938\nFräkmüntegg (3. Sekt. Talst.)\n8.251436\n46.990475\n1413.0\n\n\n114\n1400013\nAnnecy, Pont Neuf\n6.114629\n45.897774\n-9999.0\n\n\n115\n8530879\nFoppa (Naraus)\n9.267387\n46.846531\n1418.0\n\n\n134\n8584977\nEssence Tamoil Collombey\n6.954193\n46.266124\n392.5\n\n\n\n\n\n\n\n\n\nCombine the two datasets\nWe now merge the “Dienststellen-Daten” to the first dataset via the BPUIC variable.\n\n# Left-join with station names and coordinates\ndf = pd.merge(df, ds, on = 'BPUIC', how = 'left')\n\nUnfortunately, there are some rows for which there is no matching entry in the “Dienststellen-Daten”. But fortunately, we know which stations are affected based on the HALTESTELLEN_NAME column.\n\n# There are still some missings after left-join (Oberkulm Post and Borgnone-Cadanza)\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'].isna(), ['HALTESTELLEN_NAME','BEZEICHNUNG_OFFIZIELL']]\n\n\n\n\n\n\n\n\nHALTESTELLEN_NAME\nBEZEICHNUNG_OFFIZIELL\n\n\n\n\n101255\nBorgnone-Cadanza\nNaN\n\n\n101347\nBorgnone-Cadanza\nNaN\n\n\n101384\nBorgnone-Cadanza\nNaN\n\n\n101387\nBorgnone-Cadanza\nNaN\n\n\n101408\nBorgnone-Cadanza\nNaN\n\n\n...\n...\n...\n\n\n156746\nOberkulm Post\nNaN\n\n\n156763\nOberkulm Post\nNaN\n\n\n156786\nOberkulm Post\nNaN\n\n\n156803\nOberkulm Post\nNaN\n\n\n156823\nOberkulm Post\nNaN\n\n\n\n\n156 rows × 2 columns\n\n\n\nThe first part of the fix consists of imputing the names of the stations in the column BEZEICHNUNG_OFFIZIELL.\n\n# But they have data in the original data, so let's impute those\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'].isna(), \"BEZEICHNUNG_OFFIZIELL\"] = df.loc[df['BEZEICHNUNG_OFFIZIELL'].isna(), \"HALTESTELLEN_NAME\"]\n\nThe second part of the fix is to manually add the geo coordinates for the missing two stations (Oberkulm Post and Borgnone-Cadanza).\n\n# Impute geo coordinates and elevation for those missing\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'] == \"Oberkulm Post\", [\"E_WGS84\",\"N_WGS84\",\"Z_WGS84\"]] = (8.11970, 47.30414, 483)\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'] == \"Borgnone-Cadanza\", [\"E_WGS84\",\"N_WGS84\",\"Z_WGS84\"]] = (8.62254, 46.15853, 713)\n\nNow, we are finally ready to start extracting the network from this data!\n\n\nConvert it to a network\nAs I mentioned several times, every row corresponds to a stop of a train at a train station. One train ride from some initial station to some end station (called “Fahrt” in German) then typically consists of several stops along the way. However, there are some “Fahrten” with only one entry. Presumably these are mostly foreign trains that have their end destination at some border station. I decided to remove those entries:\n\n# First group by FAHRT_BEZEICHNER and then filter out all groups with only one entry\n# It's mostly trains that stop at a place at the border (I think)\ndf_filtered = df.groupby('FAHRT_BEZEICHNER').filter(lambda g: len(g) &gt; 1)\n\n# How many rows do we loose with that?\nprint(df.shape[0] - df_filtered.shape[0])\n\n566\n\n\nThis preprocessing step removes 566 rows.\nNow we group the rows by FAHRT_BEZEICHNER so that each group is one “Fahrt”. In every group we sort the stops along the way in an ascending order of the departure time.\n\n# Function to sort entries within a group in ascending order of ABFAHRTSZEIT\ndef sort_data(group):\n    return group.sort_values('ABFAHRTSZEIT', ascending = True)\n\n# Sort for each group\ndf_sorted = df_filtered.groupby('FAHRT_BEZEICHNER', group_keys=True).apply(sort_data)\n\nLet’s have a look at one “Fahrt” to get a better idea:\n\n# Look at one example Fahrt\ndf_sorted.loc[['85:11:1511:003'],['BETREIBER_ABK','LINIEN_TEXT','BEZEICHNUNG_OFFIZIELL','ABFAHRTSZEIT']]\n\n\n\n\n\n\n\n\n\nBETREIBER_ABK\nLINIEN_TEXT\nBEZEICHNUNG_OFFIZIELL\nABFAHRTSZEIT\n\n\nFAHRT_BEZEICHNER\n\n\n\n\n\n\n\n\n\n85:11:1511:003\n1758\nSBB\nIC5\nLausanne\n2023-06-06 06:15:00\n\n\n1759\nSBB\nIC5\nYverdon-les-Bains\n2023-06-06 06:37:00\n\n\n1760\nSBB\nIC5\nNeuchâtel\n2023-06-06 06:58:00\n\n\n1761\nSBB\nIC5\nBiel/Bienne\n2023-06-06 07:17:00\n\n\n1762\nSBB\nIC5\nGrenchen Süd\n2023-06-06 07:26:00\n\n\n1763\nSBB\nIC5\nSolothurn\n2023-06-06 07:34:00\n\n\n1764\nSBB\nIC5\nOensingen\n2023-06-06 07:46:00\n\n\n1765\nSBB\nIC5\nOlten\n2023-06-06 07:59:00\n\n\n1766\nSBB\nIC5\nZürich HB\n2023-06-06 08:33:00\n\n\n1767\nSBB\nIC5\nZürich Flughafen\n2023-06-06 08:43:00\n\n\n1768\nSBB\nIC5\nWinterthur\n2023-06-06 08:59:00\n\n\n1769\nSBB\nIC5\nSt. Gallen\nNaT\n\n\n\n\n\n\n\nThis is a train that goes from Lausanne to St.Gallen with many stops in-between. In St.Gallen the ABFAHRTSZEIT is missing as that “Fahrt” ends there (the train will most likely go back in the other direction, but that will be a new “Fahrt”).\nWe now have enough knowledge about the data that we can extract the edges in a for loop. Basically, what we do is to loop over the rows of a given “Fahrt”, starting with the second row and extracting the edges as\n(previous station, current station, travel time between stations).\nThe Python code for this looks as follows:\n\n# Empty list\nedgelist = []\n\n# Variables to store previous row and its index\nprev_row = None\nprev_idx = None\n\n# Loop over rows of dataframe\nfor i, row in df_sorted.iterrows():\n    # Only start with second row\n    # Only if the two rows belong to the same Fahrt\n    if prev_idx is not None and prev_idx == i[0]:\n        # Add edge to edgelist assuming it's a directed edge\n        edgelist.append((prev_row['BEZEICHNUNG_OFFIZIELL'], \n                         row['BEZEICHNUNG_OFFIZIELL'], \n                         (row['ANKUNFTSZEIT'] - prev_row['ABFAHRTSZEIT']).total_seconds() / 60))\n    # Set current row and row index to previous ones\n    prev_idx = i[0]\n    prev_row = row\n\nTo get a better idea, let’s have a look at the first list element:\n\n# First list element\nedgelist[0]\n\n('Schaffhausen', 'Basel Bad Bf', 75.0)\n\n\nWe are still not quite done yet. The problem is that the edgelist contains many duplicated entries as, for example, the stretch Olten - Zürich HB is served by many different trains on a given day.\nWhat we want to do is to go through all possible edges and sum up the number of times they occur. In addition, we would like to average the travel time between a given pair of stations over all trips between the two stations. The following code does exactly that and saves the result in the form of a dictionary.\n\n# Empty dict\nedges = {}\n\n# Loop over elements in edgelist\nfor i in edgelist:\n    # Create key\n    key = (i[0], i[1])\n    # Get previous entries in dict (if there are any)\n    prev = edges.get(key, (0, 0))\n    # Update values in dict\n    edges[key] = (prev[0] + 1, prev[1] + i[2])\n\n# Divide summed up travel times by number of trips\nedges = {k: (v[0], round(v[1]/v[0], 2)) for k, v in edges.items()}\n\nLet’s look at the entry for the stretch between Schaffhausen and Basel Badischer Bahnhof again:\n\n# Look at some element in dict\nedges[('Schaffhausen', 'Basel Bad Bf')]\n\n(17, 73.18)\n\n\nThere are 17 trips between these two stations (in this direction) and they take 73 minutes on average.\nWe are now ready to create the final node list (and export it). First, we add the two missing stations to the dataframe ds (above we only added them to the merged df, not ds). Then we reduce ds to the train stations that appear in the edges (it still contains many bus and tram stops and other things). Finally, we give it nicer column names.\n\n# Add two missing places to ds\nds.loc[len(ds)] = [8502183, 'Oberkulm Post', 8.1197, 47.30414, 483.0]\nds.loc[len(ds)] = [8505498, 'Borgnone-Cadanza', 8.62254, 46.15853, 713.0]\n\n# Set of stations that appear in edgelist\nstations_in_edgelist = set(sum(list(edges.keys()), ()))\n\n# Reduces nodes dataframe to only places in edgelist\nnodes = ds[ds['BEZEICHNUNG_OFFIZIELL'].isin(stations_in_edgelist)]\n\n# Better column names\nnodes.columns = ['BPUIC','STATION_NAME','LONGITUDE','LATITUDE','ELEVATION']\n\n# Have a look\nnodes.head()\n\n# Export node list\n# nodes.sort_values(\"BPUIC\").to_csv(\"nodelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC\nSTATION_NAME\nLONGITUDE\nLATITUDE\nELEVATION\n\n\n\n\n2375\n8500329\nKoblenz\n8.227050\n47.600338\n320.3\n\n\n2431\n8503093\nZürich Manegg\n8.519751\n47.338009\n430.0\n\n\n3816\n8508208\nTrubschachen\n7.846142\n46.921700\n732.0\n\n\n4013\n8503290\nBiberegg\n8.669443\n47.093610\n933.0\n\n\n4102\n8502271\nWohlen Oberdorf\n8.286887\n47.346852\n433.0\n\n\n\n\n\n\n\nBefore we export the edges, we change the station names in the edgelist to the BPUIC to make the edges more compact. Then we transform the dictionary into a dataframe which can finally be exported.\n\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[k[0]], node_dict[k[1]], v[0], v[1]] for k,v in edges.items()]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','NUM_CONNECTIONS','AVG_DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n\n\n\n\n\n\n\n\nBPUIC1\nBPUIC2\nNUM_CONNECTIONS\nAVG_DURATION\n\n\n\n\n0\n8503424\n8500090\n17\n73.18\n\n\n1\n8500090\n8503424\n18\n72.39\n\n\n2\n8503000\n8500010\n35\n54.00\n\n\n3\n8500010\n8500090\n68\n6.06\n\n\n4\n8500090\n8500010\n73\n6.33\n\n\n\n\n\n\n\nFeel free to download the final results: Nodelist (CSV) and Edgelist (CSV).\n\n\n\nThe Swiss railway network with a geographic layout (created using Gephi)."
  },
  {
    "objectID": "news/snf_epi/index.html",
    "href": "news/snf_epi/index.html",
    "title": "New SNSF project",
    "section": "",
    "text": "In March 2024, my colleagues Lorenz Hilfiker, Matthias Templ, and I submitted a project proposal to the Swiss National Science Foundation’s (SNSF) second Health and Wellbeing Call.\nIn November 2024, we received the exciting news that our project has been awarded funding! With a grant of CHF 391,938, we will be able to hire a full-time PhD student to support our research. Additionally, Lorenz will join Matthias and me at Fachhochschule Nordwestschweiz (FHNW) in 2025, starting in a part-time position.\nOver the next four years, our team—including the PhD student—will work on developing new methods to trace the origins of epidemic outbreaks in contact networks. While researchers have been exploring epidemic source detection for about 15 years, many existing studies focus on simplified models and small networks, limiting their practical applications. Our project aims to address these challenges by pursuing two key goals:\n\nScalability: Developing methods that can be applied to large-scale contact networks.\nReal-world applicability: Designing techniques that account for uncertainty in key parameters, making them more suitable for practical use.\n\nOur research is not limited to human pandemics—it also has significant applications in the agricultural and livestock industries, where regulatory requirements often ensure the availability of contact data.\nUltimately, we hope our methods will become valuable tools for preventing future outbreaks, whether in human or veterinary health. We are thrilled to embark on this journey and look forward to the discoveries ahead!"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching at the University of Applied Sciences and Arts Northwestern Switzerland (FHNW) centers around three key modules, outlined below.\n\nMachine Learning\nThis module is one of four within the Managerial Data Science specialization, available to both Business and International Management students at FHNW during the final year of their Bachelor’s studies.\nMany students in this specialization have little to no prior programming experience and only a modest background in mathematics and statistics. Teaching a subject rooted in advanced concepts from both fields might seem daunting in this context. However, I view it as an exciting challenge. Over the years, several key decisions have proven particularly effective.\nThe first was to clearly position this module as applied Machine Learning. Our primary goal is to solve real-world problems using off-the-shelf models and algorithms. Every topic is introduced through practical examples—preferably from a business context—and accompanied by R code to demonstrate each model or technique.\nThe second decision was to avoid making this a no-math course. Instead, we explore mathematical concepts when they provide deeper insights into a model’s inner workings or enhance intuitive understanding. I believe that even students with a minimal math background can grasp complex topics if they are broken down into univariate or bivariate examples, where visualization is straightforward and the (linear) algebra remains simple.\nFinally, I adopted a flipped classroom approach. Based on my experience, students learn best when they can explore new topics at their own pace, in a setting of their choice. To support this, I developed interactive online tutorials using the learnr package, featuring text, Shiny apps, visualizations, R code snippets, and occasional multiple-choice questions. In-class sessions are then dedicated to deepening the material, working through exercises together, and addressing questions or areas of particular interest.\nBelow are two visualizations I created to help clarify topics that students often find challenging.\n\n\n\nHow R transforms a linear scale into a log scale.\n\n\nThe first illustrates the effect of using a log scale (base 10) versus a linear scale in R. It shows how three values (10, 500, and 100,000) are represented differently when plotted on a log scale. The figure should be interpreted from bottom to top.\n\n\n\nHow a simple convolution filter works on a simplified MNIST image.\n\n\nThe second visualization explains how a simple convolution filter operates using a simplified (binary) MNIST image of a handwritten digit. I manually designed an image of the number 7 to clearly highlight the pixel-level computations during the convolution process.\n\n\nApplied Data Science\nThe current topic of the Applied Data Science course (a week-long intensive course) is data visualization. This elective course is offered annually in November. In the course, we use the tidyverse framework, with a particular focus on the ggplot2 package to create visualizations.\nTo demonstrate the key principles of high-quality visualization, I wrote a bunch of code to generate engaging plots, all based on real data from the Swiss canton of Basel-Landschaft. This data is available on the canton’s open government data portal. The goal was to walk students through the essential steps of creating effective visualizations using a practical case study.\nSome results from this case study are shown below. The first figure is a scatterplot illustrating the relationship between two quantitative variables—tax rates and land prices—and demonstrates how additional information can be integrated into the plot using point color and size.\n\n\n\nVisualizing the relationship between two quantitative variables.\n\n\nThe second figure presents CO2 emissions (in tons per capita) across all communities in the canton. This figure serves as an example of geographic visualization using a choropleth map.\n\n\n\nVisualizing a quantitative variable geographically.\n\n\nFinally, the third figure showcases how to visualize time series data. Here, we display the daily measurements of nitrogen dioxide and ozone concentrations, as recorded by the monitoring station in Sissach-Bützenen.\n\n\n\nVisualizing two time series.\n\n\n\n\nBusiness Analytics\nBusiness Analytics (BA) is one of two components of the core module Empirische Methoden und Business Analytics, which is mandatory for all Business students at FHNW.\nThe BA section is structured around two main areas of focus:\n\nIntroduction to R and Data Handling: Students begin by working with R and RStudio, gaining a basic understanding of key data structures such as vectors, factors, and data frames. They learn how to import data, manipulate these basic structures, and apply indexing. The course also covers the calculation of simple descriptive statistics and introduces R’s base plotting capabilities for data visualization.\nFundamentals of Linear Regression: The second part of the course delves into simple and multiple linear regression. Students explore the intuition behind least-squares coefficients and learn how the coefficient of determination can be used for model evaluation and comparison. In the context of multiple linear regression, the course covers topics such as dummy encoding for categorical variables and the use of interaction terms, among other techniques.\n\nThis structure ensures that students not only develop a basic technical proficiency in data analysis using R but also gain a solid understanding of essential statistical modeling concepts."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "I am a lecturer at the School of Business at the University of Applied Sciences and Arts Northwestern Switzerland, or short FHNW. Additionally, I hold a part-time Postdoctoral position at the Chair of Psychological Methods, Evaluation and Statistics at the University of Zurich, led by Prof. Dr. Carolin Strobl.\nI have a background in economics, having studied at the Universities of Fribourg and Zurich, and hold a PhD in Computer Science from the University of Zurich. My doctoral research, titled Computational Approaches to Epidemic Prevention on Contact Networks, focused on developing methods to mitigate the spread of a disease when at least partial knowledge of the underlying contact network is available.\nAt FHNW I teach the following modules (more info):\n\nMachine Learning: This course is designed for business students, emphasizing the practical application of ML to real-world problems. We primarily use the tidymodels framework, building on students’ prior knowledge of R.\nApplied Data Science: Currently, this week-long intensive course focuses on teaching students data visualization skills in R using ggplot2.\nBusiness Analytics: In this core module, I introduce students to programming in R and guide them through the fundamentals of linear regression.\n\nMy research interests are as follows:\n\nDynamical processes on networks: My primary research interest lies in epidemic processes, which was the focus of my PhD and remains a central theme in my current work.\nMonte Carlo simulations: For many problems, analytical solutions do not exist, and as a computer scientist, I am not reluctant to use simulations to find approximate solutions.\nGraph Neural Networks: Network science has not escaped the Deep Learning revolution—and naturally, GNNs have caught my attention. Specifically, I am curious if GNNs can help address the challenge of source detection.\nExplainability: Recently, I have begun exploring model explainability, primarily through visualizations—another passion of mine. I plan to further investigate this topic, particularly in the context of models on networks.\n\nI enjoy implementing Machine Learning algorithms from scratch to deepen my understanding at the implementation level. Moreover, I can spend hours refining data visualizations—whether in base R, ggplot2, or matplotlib—striving for a balance of aesthetics, clarity, and engagement.\nI live in Switzerland together with my partner and our two adorable—yet incredibly demanding—cats.\n\n\n\nTwo bundles of joy"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Journals\n\nSterchi, M., Hilfiker, L., Grütter, R., Bernstein, A. (2023). Active Querying Approach to Epidemic Source Detection on Contact Networks. Scientific Reports, 13(1). https://doi.org/10.1038/s41598-023-38282-8\nSterchi, M., Sarasua, C., Grütter, R., Bernstein, A. (2021). Outbreak detection for temporal contact data. Applied Network Science, 6(17). https://doi.org/10.1007/s41109-021-00360-z\nSterchi, M., Faverjon, C., Sarasua, C., Vargas, ME., Berezowski, J., Bernstein, A., Grütter, R., Nathues, H. (2019). The pig transport network in Switzerland: Structure, patterns, and implications for the transmission of infectious diseases between animal holdings. PLOS ONE, 14(5): e0217974. https://doi.org/10.1371/journal.pone.0217974\nFaverjon, C., Bernstein, A., Grütter, R., Nathues, C., Nathues, H., Sarasua, C., Sterchi, M., Vargas, ME., Berezowski, J. (2019). A Transdisciplinary Approach Supporting the Implementation of a Big Data Project in Livestock Production: An Example From the Swiss Pig Production Industry. Frontiers in Veterinary Science, 6(215). https://doi.org/10.3389/fvets.2019.00215\nStöckli, S., Messner, C., Sterchi, M., Dorn, M. (2019). Unreliable is Better: Theoretical and Practical Impulses for Performance Management. Die Unternehmung, 73(2), 167-180. https://www.jstor.org/stable/26696764\nHulliger, B. & Sterchi, M. (2018). A survey-based design of a pricing system for psychotherapy. Health Economics Review, 8(29), 2-11. https://doi.org/10.1186/s13561-018-0213-7\n\n\n\nProceedings\n\nSterchi, M., Sarasua, C., Grütter, R., Bernstein, A. (2019). Maximizing the Likelihood of Detecting Outbreaks in Temporal Networks. In: Proceedings of the 8th International Conference on Complex Networks and Their Applications, COMPLEX NETWORKS, 2019. Oral Presentation. https://doi.org/10.1007/978-3-030-36683-4_39\nSterchi, M. & Wolf, M. (2017). Weighted Least Squares and Adaptive Least Squares: Further Empirical Evidence. In: Kreinovic, Vladik; Sriboonchitta, Songsak; Huynh, Van-Nam. Robustness in Econometrics. Cham: Springer, 135-167. https://doi.org/10.1007/978-3-319-50742-2_9\n\n\n\nAbstracts\n\nSterchi, M., Hilfiker, L. (2025). Evaluating Graph Neural Networks for Epidemic Source Detection: A Benchmark Study (Abstract). International School and Conference on Network Science, NetSci, (upcoming, June 2025). Poster Presentation.\nSterchi, M., Hilfiker, L., Grütter, R., Bernstein, A. (2023). Active learning for epidemic source detection (Extended Abstract). Book of Abstracts of the 12th International Conference on Complex Networks and their Applications, COMPLEX NETWORKS, 2023. Oral Presentation.\nHilfiker, L., Sterchi, M. (2021). Covid‑19 superspreading: lessons from simulations on an empirical contact network (Extended Abstract). Book of Abstracts of the 10th International Conference on Complex Networks and their Applications, COMPLEX NETWORKS, 2021. Poster Presentation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "News",
    "section": "",
    "text": "NetSci 2025, here we come\n\n\n\n\n\n\nConferences\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\n\n\n\n\n\n\nNew SNSF project\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code/index.html",
    "href": "code/index.html",
    "title": "Code",
    "section": "",
    "text": "Shiny Apps\nAs part of my Postdoc employment with Prof. Dr. Carolin Strobl at the University of Zurich, I developed two Shiny apps to help psychology students understand the assumptions of (simple) linear regression models. In the process, I explored base R’s plotting capabilities and learned a great deal about how to make visually appealing plots using only base R.\nThe first app visually demonstrates how the distribution of predictors (\\(x_i\\)) and error terms (\\(\\epsilon_i\\)) influence both the marginal and conditional distributions of the dependent variable (\\(y_i\\)). The key takeaway we attempt to convey to students is that linear regression models impose distributional assumptions on the error terms (\\(\\epsilon_i\\)), not directly on \\(y_i\\).\n\n\n\nKey figure in the first app\n\n\nThe second app is more comprehensive and illustrates the five key assumptions of linear regression, as typically taught in statistics courses for psychology students (at least at the University of Zurich). The core idea is to explore each assumption by:\n\nDemonstrating how it may be violated and what that visually looks like.\nShowing the consequences of a violation.\nPresenting possible remedies to address the violation of the assumption.\n\nFor example, consider the assumption of homoscedasticity. The app first provides a visual representation of how a violation appears in typical regression plots. A small simulation experiment then reveals that, when this assumption is violated, the coverage probability of 95% confidence intervals drops to 85%. Finally, two potential remedies are introduced: robust standard errors and weighted least squares. The app allows the user to test the effect of those remedies.\n\n\n\nResiduals vs. fitted values in case of heteroscedasticity"
  },
  {
    "objectID": "news/netsci25/index.html",
    "href": "news/netsci25/index.html",
    "title": "NetSci 2025, here we come",
    "section": "",
    "text": "My colleague Lorenz Hilfiker and I are excited to have been accepted for a poster presentation at NetSci2025! We look forward to traveling to Maastricht (NL) this June to share our work with experts in the field.\nOur research explores the potential of Graph Neural Networks (GNNs) for epidemic source detection—identifying the origin of an outbreak given a contact network and the epidemic states of all nodes.\nOur (still preliminary) findings suggest that GNNs may not yet live up to their promise, as they are consistently outperformed by more traditional source detection methods. However, we have only explored a small region of the GNN design space so far, so it is too early to rule them out as a valuable tool for this problem.\nIf you’re interested, check out our abstract below—and stay tuned for a full paper on this topic!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "SBB Network Analysis - Part 1\n\n\n\n\n\n\nNetworks\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nMartin Sterchi\n\n\n\n\n\n\nNo matching items"
  }
]