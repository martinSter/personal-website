{
  "hash": "e03e619545f4017db814dcab73fffbdc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SBB Network Analysis - Part 1\"\nauthor:\n  - name: Martin Sterchi\n    email: martin.sterchi@fhnw.ch\ndate: 2025-02-28\ncategories: [\"Networks\"]\nimage: sbb_hb.jpg\nformat:\n  html:\n    df-print: paged\n    toc: true\ngoogle-scholar: false\n---\n\n\n*Update 10.03.2025: I updated the analysis in this blog so that it runs on more recent data. More precisely, I use the train traffic data from March 5, 2025 to construct the network. Moreover, I now properly reference the source data and I have added a bunch of additional node attributes. The most interesting new node attributes are average passenger frequency data for all stations.*\n\nFor quite some time I have been wondering if there are some interesting Swiss data that would serve as the basis for some fun network analysis. As a fan of public transportation and a long-time owner of a Swiss train pass (\"GA\"), the answer should have been obvious much sooner: the **Swiss railway network**.\n\nI wanted to create a (static) network in which *each node corresponds to a train station* and *each directed edge between any two nodes, A and B, means there is at least one train going nonstop from A to B*. Ideally, the edge would also be attributed with some weight representing the importance of the edge (e.g., how many trains go nonstop from A to B on a given day).\n\nThe structure of this post is as follows. I will first introduce the three datasets that I used to create the network. I will then show how to load and preprocess each one of them and how to join them. Finally, I will present how to transform those data into a form that is suitable for network analysis. The following image shows a visualization of the network data resulting from this post.\n\n![The Swiss railway network with a geographic layout (created using Gephi).](network.svg){width=\"850\"}\n\nThis is the first part of a series that will cover all kinds of fun network analysis based on the Swiss railway network.\n\n### Data sources\n\nIt was not that obvious how a network with nodes and edges following the definitions given above could be constructed based on data from the Swiss Federal Railways (abbreviated by the German speakers in Switzerland as **SBB**). With some help from SBB Experts and the [Open Data Plattform Mobility Switzerland](https://opentransportdata.swiss/en/), I finally found the right data.\n\nThe first and most important dataset is called [Ist-Daten](https://data.opentransportdata.swiss/de/dataset/istdaten) and, for a given day, contains all regular stops of all trains in Switzerland with their planned and effective arrival and departure times. From this data, we can infer all nonstop stretches of any train in Switzerland. A description of this dataset can be found [here](https://opentransportdata.swiss/de/cookbook/actual-data/).\n\nNote that the \"Ist-Daten\" not only contain the data for trains but also for all other public transport (buses, trams, and even boats). To keep things simple we will focus on the train network.\n\nThe second dataset is the [Dienststellen-Daten](https://data.opentransportdata.swiss/de/dataset/service-points-actual-date) which basically allows to add node attributes such as the geographic coordinates of a node (i.e., a train station). A description of this dataset can be found [here](https://opentransportdata.swiss/de/cookbook/service-points/).\n\nThe third dataset is a [statistic of the average number of passengers boarding and alighting](https://data.opentransportdata.swiss/en/dataset/einundaus). It will allow us to add further interesting node attributes.\n\n### Load and preprocess \"Ist-Daten\"\n\nHere, we will load and preprocess the \"Ist-Daten\" from which we can derive the edges of our network. First, I import some Python libraries and print their version number for better reproducibility of this code.\n\n::: {#a2166f64 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Check versions of libraries.\nprint(\"NumPy version:\", np.__version__)\nprint(\"Pandas version:\", pd.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumPy version: 1.26.4\nPandas version: 2.1.4\n```\n:::\n:::\n\n\nLet's now load the data. You can see in the filename that I downloaded the \"Ist-Daten\" from the SBB portal for March 5, 2025. You can get the data for any day you want [here](https://data.opentransportdata.swiss/de/dataset/istdaten).\n\n::: {#04dc93d0 .cell execution_count=2}\n``` {.python .cell-code}\n# Load the data\ndf = pd.read_csv('2025-03-05_istdaten.csv', sep=\";\", low_memory=False)\n```\n:::\n\n\nTo get a feeling for the data, let's check the number of rows and columns.\n\n::: {#8c0f7cb3 .cell execution_count=3}\n``` {.python .cell-code}\n# Number of rows and columns\nprint(df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(2510290, 21)\n```\n:::\n:::\n\n\nOk, it's actually a pretty big dataset: it has over 2.5 million rows. That makes sense as this file contains every stop of every vehicle involved in public transport on a given day. Thus, every row corresponds to a stop of a train, bus, or any other vehicle of public transport.\n\n::: {#bafe8f67 .cell execution_count=4}\n``` {.python .cell-code}\n# Missing values per column\ndf.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nBETRIEBSTAG                  0\nFAHRT_BEZEICHNER             0\nBETREIBER_ID                 0\nBETREIBER_ABK                0\nBETREIBER_NAME               0\nPRODUKT_ID                  31\nLINIEN_ID                    0\nLINIEN_TEXT                  0\nUMLAUF_ID              1362009\nVERKEHRSMITTEL_TEXT          0\nZUSATZFAHRT_TF               0\nFAELLT_AUS_TF                0\nBPUIC                        0\nHALTESTELLEN_NAME       169114\nANKUNFTSZEIT            149415\nAN_PROGNOSE             156828\nAN_PROGNOSE_STATUS      149200\nABFAHRTSZEIT            149426\nAB_PROGNOSE             157273\nAB_PROGNOSE_STATUS      149115\nDURCHFAHRT_TF                0\ndtype: int64\n```\n:::\n:::\n\n\nWe can see that some columns contain many missing values. The only one I worry about for now is the column `PRODUKT_ID`. If you look through these rows (I don't show that here), you can see that they should all be of type \"Zug\" (train). Thus, we impute accordingly:\n\n::: {#b6419c37 .cell execution_count=5}\n``` {.python .cell-code}\n# Impute 'Zug'\ndf.loc[df[\"PRODUKT_ID\"].isna(), \"PRODUKT_ID\"] = 'Zug'\n```\n:::\n\n\nThere are quite a few date-timestamp columns that are not yet in the proper format. Thus, we now convert them to datetime formats:\n\n::: {#4c8d40ab .cell execution_count=6}\n``` {.python .cell-code}\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'], format = \"%d.%m.%Y\")\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\n```\n:::\n\n\nNow is a good time to finally have a look at the dataframe:\n\n::: {#0441e23a .cell execution_count=7}\n``` {.python .cell-code}\n# Let's look at first few rows\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BETRIEBSTAG</th>\n      <th>FAHRT_BEZEICHNER</th>\n      <th>BETREIBER_ID</th>\n      <th>BETREIBER_ABK</th>\n      <th>BETREIBER_NAME</th>\n      <th>PRODUKT_ID</th>\n      <th>LINIEN_ID</th>\n      <th>LINIEN_TEXT</th>\n      <th>UMLAUF_ID</th>\n      <th>VERKEHRSMITTEL_TEXT</th>\n      <th>ZUSATZFAHRT_TF</th>\n      <th>FAELLT_AUS_TF</th>\n      <th>BPUIC</th>\n      <th>HALTESTELLEN_NAME</th>\n      <th>ANKUNFTSZEIT</th>\n      <th>AN_PROGNOSE</th>\n      <th>AN_PROGNOSE_STATUS</th>\n      <th>ABFAHRTSZEIT</th>\n      <th>AB_PROGNOSE</th>\n      <th>AB_PROGNOSE_STATUS</th>\n      <th>DURCHFAHRT_TF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-03-05</td>\n      <td>80:800631:17230:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17230</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>2025-03-05 04:59:00</td>\n      <td>2025-03-05 04:59:00</td>\n      <td>PROGNOSE</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-03-05</td>\n      <td>80:800631:17233:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17233</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>2025-03-05 06:07:00</td>\n      <td>2025-03-05 06:08:00</td>\n      <td>PROGNOSE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-03-05</td>\n      <td>80:800631:17234:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17234</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>2025-03-05 05:56:00</td>\n      <td>2025-03-05 06:02:00</td>\n      <td>PROGNOSE</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-03-05</td>\n      <td>80:800631:17235:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17235</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>2025-03-05 06:43:00</td>\n      <td>2025-03-05 06:53:00</td>\n      <td>PROGNOSE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-03-05</td>\n      <td>80:800631:17236:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17236</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>2025-03-05 06:31:00</td>\n      <td>2025-03-05 06:34:00</td>\n      <td>PROGNOSE</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBut what do all these columns mean? I have browsed the metadata a bit and found the following explanations (that I hopefully accurately reproduce in English):\n\n-   `BETRIEBSTAG`: Simply the day on which the data were recorded.\n-   `FAHRT_BEZEICHNER`: This is some elaborate identifier in the format \\[UIC-Countrycode\\]:\\[GO-Number\\]:\\[VM-Number\\]:\\[Extended Reference\\].\n-   `BETREIBER_ID`: \\[UIC-Countrycode\\]:\\[GO-Number\\]. GO is short for \"Geschäftsorganisation\". For foreign organizations it is not a GO-Number but a TU-Number with TU meaning \"Transportunternehmen\". It is basically an ID for the company running that particular train.\n-   `BETREIBER_ABK`: The abbreviation for the company running the train.\n-   `BETREIBER_NAME`: The full name of the company running the train.\n-   `PRODUKT_ID`: Type of public transport.\n-   `LINIEN_ID`: The ID for the route of that train.\n-   `LINIEN_TEXT`: The public ID for the route of that train.\n-   `UMLAUF_ID`: An ID for a \"Umlauf\" which describes the period starting with the vehicle leaving the garage and ending with the vehicle being deposited back in the garage.\n-   `ZUSATZFAHRT_TF`: Is true if it is an extraordinary (not usually scheduled) trip.\n-   `FAELLT_AUS_TF`: Is true if the trip is cancelled.\n-   `BPUIC`: The ID of the station.\n-   `HALTESTELLEN_NAME`: The name of the station.\n-   `ANKUNFTSZEIT`: Planned time of arrival at the station.\n-   `AN_PROGNOSE`: Prediction of time of arrival at the station.\n-   `AN_PROGNOSE_STATUS`: Status of that prediction. Possible values are: \"UNBEKANNT\", \"leer\", \"PROGNOSE\", \"GESCHAETZT\", \"REAL\". If the value of that column is \"REAL\", it means that the predicted time of arrival is the time the train actually arrived at the station.\n-   `ABFAHRTSZEIT`, `AB_PROGNOSE`, `AB_PROGNOSE_STATUS`: Same definitions as for arrival but here for departure from the station.\n-   `DURCHFAHRT_TF`: Is true if the vehicle does not stop even if a stop was scheduled.\n\nLet's now have a look at the values in the column `PRODUKT_ID`:\n\n::: {#0b765cfb .cell execution_count=8}\n``` {.python .cell-code}\n# Look at PRODUKT_ID\ndf[\"PRODUKT_ID\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nPRODUKT_ID\nBus            1965207\nTram            249408\nZug             163649\nBUS             124171\nMetro             4936\nZahnradbahn       1944\nSchiff             975\nName: count, dtype: int64\n```\n:::\n:::\n\n\nWe can see that trains are only the third most frequent category in this data. However, as mentioned before, we want to keep it simple and now reduce the dataset to only trains.\n\n::: {#73fcc005 .cell execution_count=9}\n``` {.python .cell-code}\n# First we reduce to only trains\ndf = df[df['PRODUKT_ID'] == \"Zug\"]\n```\n:::\n\n\nIn a next step, we remove all rows where the corresponding train has been cancelled.\n\n::: {#f93e7496 .cell execution_count=10}\n``` {.python .cell-code}\n# Filter out all entries with FAELLT_AUS_TF == True\ndf = df[df['FAELLT_AUS_TF'] == False]\n```\n:::\n\n\nLet's explore the data a bit more before we move to the second dataset. Let's check out the most frequent values that occur in the column `BETREIBER_NAME`:\n\n::: {#41131d5a .cell execution_count=11}\n``` {.python .cell-code}\n# Look at BETREIBER_NAME\ndf[\"BETREIBER_NAME\"].value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nBETREIBER_NAME\nSchweizerische Bundesbahnen SBB    63850\nBLS AG (bls)                       16256\nTHURBO                             13017\nAargau Verkehr AG                   7131\nSchweizerische Südostbahn (sob)     6083\nName: count, dtype: int64\n```\n:::\n:::\n\n\nAs expected, SBB is the company serving the largest number of stations. What about the column `VERKEHRSMITTEL_TEXT`?\n\n::: {#47180f84 .cell execution_count=12}\n``` {.python .cell-code}\n# Look at VERKEHRSMITTEL_TEXT\ndf[\"VERKEHRSMITTEL_TEXT\"].value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nVERKEHRSMITTEL_TEXT\nS     103331\nR      34435\nRE      9730\nIR      7532\nIC      3059\nName: count, dtype: int64\n```\n:::\n:::\n\n\nWe can see that the most frequent type of trains are S-Bahns (`S`). Finally, let's check the most frequent train stations that occur in the data:\n\n::: {#a848d451 .cell execution_count=13}\n``` {.python .cell-code}\n# Look at HALTESTELLEN_NAME\ndf[\"HALTESTELLEN_NAME\"].value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nHALTESTELLEN_NAME\nZürich HB          2239\nBern               1709\nWinterthur          955\nZürich Oerlikon     919\nLuzern              843\nName: count, dtype: int64\n```\n:::\n:::\n\n\nUnsurprisingly, Zürich and Bern are the most frequent values occuring in the data.\n\n### Load and preprocess \"Dienststellen-Daten\"\n\nFortunately, we can go through the second dataset a bit more quickly. We again start by loading it and checking the dimensions of the dataframe.\n\n::: {#326502d6 .cell execution_count=14}\n``` {.python .cell-code}\n# Load the data\nds = pd.read_csv('actual_date-swiss-only-service_point-2025-03-06.csv', sep = \";\", low_memory = False)\n\n# Number of rows and columns\nprint(ds.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(55308, 55)\n```\n:::\n:::\n\n\nThe data contains a column `validTo` that allows us to filter out all stations that are not valid anymore (closed down?). We check the values that appear in this column and see that all stations should be valid as of March 6, 2025. This is no surprise as we use the dataset of currently valid stations.\n\n::: {#eb122627 .cell execution_count=15}\n``` {.python .cell-code}\n# Check 'validTo' values.\nds['validTo'].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray(['9999-12-31', '2025-12-13', '2025-04-06', '2026-12-12',\n       '2025-08-29', '2027-12-11', '2025-03-16', '2025-05-30',\n       '2099-12-31', '2028-12-09', '2025-09-30', '2030-12-14',\n       '2025-06-30', '2050-12-31', '2029-12-09', '2025-08-31',\n       '2025-03-31', '2025-04-12', '2025-05-16', '2025-03-06'],\n      dtype=object)\n```\n:::\n:::\n\n\nLet's also quickly make sure that we have unique rows (based on 'number').\n\n::: {#c2c964e9 .cell execution_count=16}\n``` {.python .cell-code}\n# Is the number of unique 'number' (= BPUIC) values equal to the number of rows?\nlen(pd.unique(ds['number'])) == ds.shape[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nTrue\n```\n:::\n:::\n\n\nFinally, we keep only the columns we need (identifier, official name, and geo coordinates).\n\n::: {#79863576 .cell execution_count=17}\n``` {.python .cell-code}\n# Keep only the relevant columns\nds = ds[[\"number\",\"designationOfficial\",\"cantonName\",\"municipalityName\",\"businessOrganisationDescriptionEn\",\"wgs84East\",\"wgs84North\",\"height\"]]\n\n# Show first few rows\nds.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number</th>\n      <th>designationOfficial</th>\n      <th>cantonName</th>\n      <th>municipalityName</th>\n      <th>businessOrganisationDescriptionEn</th>\n      <th>wgs84East</th>\n      <th>wgs84North</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1322001</td>\n      <td>Antronapiana</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Autoservizi Comazzi S.R.L.</td>\n      <td>8.113620</td>\n      <td>46.060120</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1322002</td>\n      <td>Anzola d'Ossola</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Autoservizi Comazzi S.R.L.</td>\n      <td>8.345715</td>\n      <td>45.989869</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1322003</td>\n      <td>Baceno</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Autoservizi Comazzi S.R.L.</td>\n      <td>8.319256</td>\n      <td>46.261501</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1322012</td>\n      <td>Castiglione</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Autoservizi Comazzi S.R.L.</td>\n      <td>8.214886</td>\n      <td>46.020588</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1322013</td>\n      <td>Ceppo Morelli</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Autoservizi Comazzi S.R.L.</td>\n      <td>8.069922</td>\n      <td>45.971036</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Load and preprocess average traffic data\n\nThis part is also fairly easy. We load the data and check the dimensions, as always.\n\n::: {#1ece9761 .cell execution_count=18}\n``` {.python .cell-code}\n# Load the data\nds_freq = pd.read_csv('t01x-sbb-cff-ffs-frequentia-2023.csv', sep = \";\", low_memory = False)\n\n# Number of rows and columns\nprint(ds_freq.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(3479, 14)\n```\n:::\n:::\n\n\nIf you actually have a look at the data, you see that many stations have several measurements made at different times (and the times of measurements are identified by `Jahr_Annee_Anno`). We only want to keep the most recent measurements for every station:\n\n::: {#7349fee9 .cell execution_count=19}\n``` {.python .cell-code}\n# For every station, we only keep the most recent measurements.\nds_freq = ds_freq.loc[ds_freq.groupby('UIC')['Jahr_Annee_Anno'].idxmax()]\n```\n:::\n\n\nChecking the data types of all columns reveals that there is still a problem with the measurement columns `DTV_TJM_TGM`, `DWV_TMJO_TFM`, and `DNWV_TMJNO_TMGNL`. They are currently of type `object` because they contain the thousand separator `’`. We thus remove all instances of this characters and transform these columns to integers.\n\n::: {#4a35565b .cell execution_count=20}\n``` {.python .cell-code}\n# Data types of columns\nds_freq.dtypes\n\n# Remove thousand separator and make integers out of it.\nds_freq['DTV_TJM_TGM'] = ds_freq['DTV_TJM_TGM'].str.replace('’', '').astype(int)\nds_freq['DWV_TMJO_TFM'] = ds_freq['DWV_TMJO_TFM'].str.replace('’', '').astype(int)\nds_freq['DNWV_TMJNO_TMGNL'] = ds_freq['DNWV_TMJNO_TMGNL'].str.replace('’', '').astype(int)\n```\n:::\n\n\nFinally, we keep only the relevant columns.\n\n::: {#388b131d .cell execution_count=21}\n``` {.python .cell-code}\n# Keep only the relevant columns\nds_freq = ds_freq[[\"UIC\",\"DTV_TJM_TGM\",\"DWV_TMJO_TFM\",\"DNWV_TMJNO_TMGNL\"]]\n```\n:::\n\n\n::: {#4ea02a36 .cell execution_count=22}\n``` {.python .cell-code}\n# Show first few rows\nds_freq.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UIC</th>\n      <th>DTV_TJM_TGM</th>\n      <th>DWV_TMJO_TFM</th>\n      <th>DNWV_TMJNO_TMGNL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>411</th>\n      <td>8500010</td>\n      <td>98600</td>\n      <td>105900</td>\n      <td>81900</td>\n    </tr>\n    <tr>\n      <th>423</th>\n      <td>8500016</td>\n      <td>90</td>\n      <td>100</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>2024</th>\n      <td>8500020</td>\n      <td>5700</td>\n      <td>7000</td>\n      <td>2800</td>\n    </tr>\n    <tr>\n      <th>2294</th>\n      <td>8500021</td>\n      <td>8500</td>\n      <td>9900</td>\n      <td>5200</td>\n    </tr>\n    <tr>\n      <th>1072</th>\n      <td>8500022</td>\n      <td>3600</td>\n      <td>4100</td>\n      <td>2300</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBut what exactly are these three measurement variables? The source dataset provides the following definitions:\n\n* `DTV_TJM_TGM`: \"Average daily traffic (Monday to Sunday).\"\n* `DWV_TMJO_TFM`: \"Average traffic on weekdays (Monday to Friday).\"\n* `DNWV_TMJNO_TMGNL`: \"Average non-work day traffic (Saturdays, Sundays and public holidays).\"\n\nIt is further mentioned that all passengers boarding and exiting the trains are counted. That also means that passengers who switch trains are counted twice. For larger stations, the data may not cover all trains arriving and departing at the corresponding station. For example, the numbers for Bern do not include the traffic generated by the regional train company RBS.\n\n### Combine the three datasets\n\nWe first merge the traffic data to the \"Dienststellen-Daten\":\n\n::: {#8fdfa4ff .cell execution_count=23}\n``` {.python .cell-code}\n# Join to 'ds'\nds = pd.merge(ds, ds_freq, left_on = 'number', right_on = 'UIC', how = 'left')\n\n# Drop 'UIC'\nds = ds.drop('UIC', axis=1)\n\n# Better column names\nds.columns = ['BPUIC','STATION_NAME','CANTON','MUNICIPALITY','COMPANY',\n              'LONGITUDE','LATITUDE','ELEVATION','AVG_DAILY_TRAFFIC',\n              'AVG_DAILY_TRAFFIC_WEEKDAYS','AVG_DAILY_TRAFFIC_WEEKENDS']\n```\n:::\n\n\nThen we merge the \"Dienststellen-Daten\" to the \"Ist-Daten\" via the `BPUIC` variable:\n\n::: {#05ba214a .cell execution_count=24}\n``` {.python .cell-code}\n# Left-join with station names and coordinates\ndf = pd.merge(df, ds, on = 'BPUIC', how = 'left')\n```\n:::\n\n\nUnfortunately, there are some rows (18) for which `HALTESTELLEN_NAME` is missing. But fortunately, we know which stations are affected based on the `STATION_NAME` column that we have just merged from `ds`.\n\n::: {#24f362f0 .cell execution_count=25}\n``` {.python .cell-code}\n# There are 18 missing values for 'HALTESTELLEN_NAME' which we impute from 'STATION_NAME'.\ndf.loc[df['HALTESTELLEN_NAME'].isna(), \"HALTESTELLEN_NAME\"] = df.loc[df['HALTESTELLEN_NAME'].isna(), \"STATION_NAME\"]\n```\n:::\n\n\nNow, we are finally ready to start extracting the network from this data!\n\n### Convert it to a network\n\nAs I mentioned several times, every row corresponds to a stop of a train at a train station. One train ride from some initial station to some end station (called \"Fahrt\" in German) then typically consists of several stops along the way. However, there are some \"Fahrten\" with only one entry. Presumably these are mostly foreign trains that have their final destination at some border station. I decided to remove those entries:\n\n::: {#e75c61e1 .cell execution_count=26}\n``` {.python .cell-code}\n# First group by FAHRT_BEZEICHNER and then filter out all groups with only one entry\n# It's mostly trains that stop at a place at the border (I think)\ndf_filtered = df.groupby('FAHRT_BEZEICHNER').filter(lambda g: len(g) > 1)\n\n# How many rows do we loose with that?\nprint(df.shape[0] - df_filtered.shape[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n420\n```\n:::\n:::\n\n\nThis preprocessing step removes 420 rows.\n\nNow we group the rows by `FAHRT_BEZEICHNER` so that each group is one \"Fahrt\". In every group we sort the stops along the way in an ascending order of the departure time.\n\n::: {#0a953ea4 .cell execution_count=27}\n``` {.python .cell-code}\n# Function to sort entries within a group in ascending order of ABFAHRTSZEIT\ndef sort_data(group):\n    return group.sort_values('ABFAHRTSZEIT', ascending = True)\n\n# Sort for each group\ndf_sorted = df_filtered.groupby('FAHRT_BEZEICHNER', group_keys=True).apply(sort_data)\n```\n:::\n\n\nLet's have a look at one \"Fahrt\" to get a better idea:\n\n::: {#34006720 .cell execution_count=28}\n``` {.python .cell-code}\n# Look at one example Fahrt\ndf_sorted.loc[['85:22:1083:000'],['BETREIBER_NAME','LINIEN_TEXT','HALTESTELLEN_NAME','ABFAHRTSZEIT']]\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>BETREIBER_NAME</th>\n      <th>LINIEN_TEXT</th>\n      <th>HALTESTELLEN_NAME</th>\n      <th>ABFAHRTSZEIT</th>\n    </tr>\n    <tr>\n      <th>FAHRT_BEZEICHNER</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">85:22:1083:000</th>\n      <th>64346</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Gossau SG</td>\n      <td>2025-03-05 08:21:00</td>\n    </tr>\n    <tr>\n      <th>64347</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Herisau</td>\n      <td>2025-03-05 08:28:00</td>\n    </tr>\n    <tr>\n      <th>64348</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Herisau Wilen</td>\n      <td>2025-03-05 08:30:00</td>\n    </tr>\n    <tr>\n      <th>64349</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Waldstatt</td>\n      <td>2025-03-05 08:34:00</td>\n    </tr>\n    <tr>\n      <th>64350</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Zürchersmühle</td>\n      <td>2025-03-05 08:39:00</td>\n    </tr>\n    <tr>\n      <th>64351</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Urnäsch</td>\n      <td>2025-03-05 08:43:00</td>\n    </tr>\n    <tr>\n      <th>64352</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Jakobsbad</td>\n      <td>2025-03-05 08:48:00</td>\n    </tr>\n    <tr>\n      <th>64353</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Gonten</td>\n      <td>2025-03-05 08:50:00</td>\n    </tr>\n    <tr>\n      <th>64354</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Gontenbad</td>\n      <td>2025-03-05 08:52:00</td>\n    </tr>\n    <tr>\n      <th>64355</th>\n      <td>Appenzeller Bahnen (ab)</td>\n      <td>S23</td>\n      <td>Appenzell</td>\n      <td>NaT</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis is a train that goes from Gossau to Appenzell with many stops in-between. In Appenzell the `ABFAHRTSZEIT` is missing as that \"Fahrt\" ends there (the train will most likely go back in the other direction, but that will be a new \"Fahrt\").\n\nWe now have enough knowledge about the data that we can extract the edges in a for loop. Basically, what we do is to loop over the rows of a given \"Fahrt\", starting with the second row and extracting the edges as\n\n`(previous station, current station, travel time between stations)`.\n\nThe Python code for this looks as follows:\n\n::: {#8f92c366 .cell execution_count=29}\n``` {.python .cell-code}\n# Empty list\nedgelist = []\n\n# Variables to store previous row and its index\nprev_row = None\nprev_idx = None\n\n# Loop over rows of dataframe\nfor i, row in df_sorted.iterrows():\n    # Only start with second row\n    # Only if the two rows belong to the same Fahrt\n    if prev_idx is not None and prev_idx == i[0]:\n        # Add edge to edgelist assuming it's a directed edge\n        edgelist.append((prev_row['STATION_NAME'], \n                         row['STATION_NAME'], \n                         (row['ANKUNFTSZEIT'] - prev_row['ABFAHRTSZEIT']).total_seconds() / 60))\n    # Set current row and row index to previous ones\n    prev_idx = i[0]\n    prev_row = row\n```\n:::\n\n\nTo get a better idea, let's have a look at the first list element:\n\n::: {#4cd0dd66 .cell execution_count=30}\n``` {.python .cell-code}\n# First list element\nedgelist[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n('Zürich HB', 'Basel SBB', 54.0)\n```\n:::\n:::\n\n\nWe are still not quite done yet. The problem is that the `edgelist` contains many duplicated entries as, for example, the stretch Zürich HB - Basel SBB is served by many different trains on a given day.\n\nWhat we want to do is to go through all possible edges and sum up the number of times they occur. In addition, we would like to average the travel time between a given pair of stations over all trips between the two stations. The following code does exactly that and saves the result in the form of a dictionary.\n\n::: {#764e7c1c .cell execution_count=31}\n``` {.python .cell-code}\n# Empty dict\nedges = {}\n\n# Loop over elements in edgelist\nfor i in edgelist:\n    # Create key\n    key = (i[0], i[1])\n    # Get previous entries in dict (if there are any)\n    prev = edges.get(key, (0, 0))\n    # Update values in dict\n    edges[key] = (prev[0] + 1, prev[1] + i[2])\n\n# Divide summed up travel times by number of trips\nedges = {k: (v[0], round(v[1]/v[0], 2)) for k, v in edges.items()}\n```\n:::\n\n\nLet's look at the entry for the stretch between Schaffhausen and Basel Badischer Bahnhof again:\n\n::: {#98a7baf1 .cell execution_count=32}\n``` {.python .cell-code}\n# Look at some element in dict\nedges[('Zürich HB', 'Basel SBB')]\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n(36, 54.0)\n```\n:::\n:::\n\n\nThere are 36 trips between these two stations (in this direction) and they take 54 minutes on average.\n\nWe are now ready to create the final node list (and export it). First, we reduce `ds` to the train stations that actually appear in the edges (it still contains many bus and tram stops and other things).\n\n::: {#52fb1c17 .cell execution_count=33}\n``` {.python .cell-code}\n# Set of stations that appear in edgelist\nstations_in_edgelist = set(sum(list(edges.keys()), ()))\n\n# Reduces nodes dataframe to only places in edgelist\nnodes = ds[ds['STATION_NAME'].isin(stations_in_edgelist)]\n```\n:::\n\n\nSecond, we quickly check the number of missing values again.\n\n::: {#2a4073b2 .cell execution_count=34}\n``` {.python .cell-code}\n# Missing values per column\nnodes.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\nBPUIC                           0\nSTATION_NAME                    0\nCANTON                         21\nMUNICIPALITY                   21\nCOMPANY                         0\nLONGITUDE                       0\nLATITUDE                        0\nELEVATION                       1\nAVG_DAILY_TRAFFIC             500\nAVG_DAILY_TRAFFIC_WEEKDAYS    500\nAVG_DAILY_TRAFFIC_WEEKENDS    500\ndtype: int64\n```\n:::\n:::\n\n\nThere are still some issues here. The one we can solve is the missing elevation. The station Tirano (in Italy) has no value for this column. We simply impute manually (Tirano's elevation is approximately 441m).\n\n::: {#982b0fab .cell execution_count=35}\n``` {.python .cell-code}\n# Impute missing elevation for Tirano\nnodes.loc[nodes['STATION_NAME'] == \"Tirano\", \"ELEVATION\"] = 441\n```\n:::\n\n\nThe missing values for `CANTON` and `MUNICIPALITY` concern municipalities abroad (in Germany and Italy mostly). The 500 missing values in the traffic columns are stations are run by smaller companies or stations abroad. There is nothing we can do about all these missing values.\n\n::: {#d8eb97b4 .cell execution_count=36}\n``` {.python .cell-code}\n# Have a look\nnodes.head()\n\n# Export node list\n# nodes.sort_values(\"BPUIC\").to_csv(\"nodelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BPUIC</th>\n      <th>STATION_NAME</th>\n      <th>CANTON</th>\n      <th>MUNICIPALITY</th>\n      <th>COMPANY</th>\n      <th>LONGITUDE</th>\n      <th>LATITUDE</th>\n      <th>ELEVATION</th>\n      <th>AVG_DAILY_TRAFFIC</th>\n      <th>AVG_DAILY_TRAFFIC_WEEKDAYS</th>\n      <th>AVG_DAILY_TRAFFIC_WEEKENDS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12683</th>\n      <td>8500100</td>\n      <td>Tavannes</td>\n      <td>Bern</td>\n      <td>Tavannes</td>\n      <td>Swiss Federal Railways SBB</td>\n      <td>7.201645</td>\n      <td>47.219845</td>\n      <td>754.17</td>\n      <td>1400.0</td>\n      <td>1600.0</td>\n      <td>810.0</td>\n    </tr>\n    <tr>\n      <th>12684</th>\n      <td>8500121</td>\n      <td>Courfaivre</td>\n      <td>Jura</td>\n      <td>Haute-Sorne</td>\n      <td>Swiss Federal Railways SBB</td>\n      <td>7.291166</td>\n      <td>47.335083</td>\n      <td>450.99</td>\n      <td>420.0</td>\n      <td>480.0</td>\n      <td>280.0</td>\n    </tr>\n    <tr>\n      <th>12685</th>\n      <td>8500103</td>\n      <td>Sorvilier</td>\n      <td>Bern</td>\n      <td>Sorvilier</td>\n      <td>Swiss Federal Railways SBB</td>\n      <td>7.305794</td>\n      <td>47.239354</td>\n      <td>681.07</td>\n      <td>60.0</td>\n      <td>70.0</td>\n      <td>49.0</td>\n    </tr>\n    <tr>\n      <th>12688</th>\n      <td>8500120</td>\n      <td>Courtételle</td>\n      <td>Jura</td>\n      <td>Courtételle</td>\n      <td>Swiss Federal Railways SBB</td>\n      <td>7.317943</td>\n      <td>47.342829</td>\n      <td>436.90</td>\n      <td>840.0</td>\n      <td>970.0</td>\n      <td>550.0</td>\n    </tr>\n    <tr>\n      <th>12689</th>\n      <td>8500102</td>\n      <td>Malleray-Bévilard</td>\n      <td>Bern</td>\n      <td>Valbirse</td>\n      <td>Swiss Federal Railways SBB</td>\n      <td>7.275946</td>\n      <td>47.238714</td>\n      <td>698.18</td>\n      <td>630.0</td>\n      <td>780.0</td>\n      <td>280.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBefore we export the edges, we change the station names in the edgelist to the BPUIC to make the edges more compact. Then we transform the dictionary into a dataframe which can finally be exported.\n\n::: {#07a62686 .cell execution_count=37}\n``` {.python .cell-code}\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[k[0]], node_dict[k[1]], v[0], v[1]] for k,v in edges.items()]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','NUM_CONNECTIONS','AVG_DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BPUIC1</th>\n      <th>BPUIC2</th>\n      <th>NUM_CONNECTIONS</th>\n      <th>AVG_DURATION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8503000</td>\n      <td>8500010</td>\n      <td>36</td>\n      <td>54.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8500010</td>\n      <td>8500090</td>\n      <td>67</td>\n      <td>6.07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8500090</td>\n      <td>8500010</td>\n      <td>67</td>\n      <td>6.39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8500010</td>\n      <td>8503000</td>\n      <td>39</td>\n      <td>57.87</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8503424</td>\n      <td>8500090</td>\n      <td>17</td>\n      <td>73.18</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFeel free to download the final results: <a href=\"nodelist.csv\" download>Nodelist (CSV)</a> and <a href=\"edgelist.csv\" download>Edgelist (CSV)</a>.\n\n*The title image has been created by Wikimedia user JoachimKohler-HB and is licensed under [Creative Commons](https://creativecommons.org/licenses/by-sa/4.0/deed.en).*\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}