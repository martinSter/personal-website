{
  "hash": "dd8e829cb204dca13d71c7609dd201dd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SBB Network Analysis - Part 1\"\nauthor:\n  - name: Martin Sterchi\n    email: martin.sterchi@fhnw.ch\ndate: 2025-02-28\ncategories: [\"Networks\"]\nimage: sbb_hb.jpg\nformat:\n  html:\n    df-print: paged\n    toc: true\ngoogle-scholar: false\n---\n\n\n\n\nFor quite some time I have been wondering if there are some interesting Swiss data that would serve as the basis for some fun network analysis. As a fan of public transportation and a long-time owner of a Swiss train pass (\"GA\"), the answer should have been obvious much sooner: the **Swiss public transport network**.\n\nI wanted to create a (static) network in which *each node corresponds to a train station* and *each directed edge between any two nodes, A and B, means there is at least one train going nonstop from A to B*. Ideally, the edge would also be attributed with some weight representing the importance of the edge (e.g., how many trains go nonstop from A to B on a given day).\n\nThe structure of this post is as follows. I will first introduce the two datasets that I used to create the network. I will then show how to load and preprocess each one of them and how to join them. Finally, I will present how to transform those data into a form that is suitable for network analysis. The following image shows a visualization of the network data resulting from this post.\n\n![The Swiss railway network with a geographic layout (created using Gephi).](network.svg){width=\"850\"}\n\nThis is the first part of a series that will cover all kinds of fun network analysis based on the Swiss railway network.\n\n### Data sources\n\nIt was not that obvious how a network with nodes and edges following the definitions given above could be constructed based on data from the Swiss Federal Railways (abbreviated by the German speakers in Switzerland as **SBB**). With some help from SBB Experts and the [SBB Open Data Plattform](https://data.sbb.ch/pages/home/), I finally found the right data.\n\nThe first dataset is called \"**Ist-Daten**\" and, for a given day, contains all regular stops of all trains in Switzerland with their planned and effective arrival and departure times. From this data, we can infer all nonstop stretches of any train in Switzerland.\n\nNote that the \"Ist-Daten\" not only contain the data for trains but also for all other public transport (buses, trams, and even boats). To keep things simple we will focus on the train network.\n\nThe second dataset is the \"**Dienststellen-Daten**\" which basically allows to add node attributes such as the geographic coordinates of a node (i.e., a train station).\n\n### Load and preprocess \"Ist-Daten\"\n\nHere, we will load and preprocess the \"Ist-Daten\" from which we can derive the edges of our network. First, I import some Python libraries and print their version number for better reproducibility of this code.\n\n::: {#04961d3d .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Check versions of libraries.\nprint(\"NumPy version:\", np.__version__)\nprint(\"Pandas version:\", pd.__version__)\n\n# Make sure there is no limit on the number of columns shown.\npd.set_option('display.max_columns', None)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumPy version: 1.26.4\nPandas version: 2.1.4\n```\n:::\n:::\n\n\nLet's now load the data. You can see in the filename that I downloaded the \"Ist-Daten\" from the SBB portal for June 6, 2023. You can get the data for any day you want [here](https://data.sbb.ch/explore/dataset/ist-daten-sbb/information/).\n\n::: {#92c4de2f .cell execution_count=2}\n``` {.python .cell-code}\n# Load the data\ndf = pd.read_csv('2023-06-06_istdaten.csv', sep=\";\", low_memory=False) \n```\n:::\n\n\nTo get a feeling for the data, let's check the number of rows and columns.\n\n::: {#27622ca9 .cell execution_count=3}\n``` {.python .cell-code}\n# Number of rows and columns\nprint(df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(2309817, 21)\n```\n:::\n:::\n\n\nOk, it's actually a pretty big dataset: it has over 2.3 million rows. That makes sense as this file contains every stop of every vehicle involved in public transport on a given day. Thus, every row corresponds to a stop of a train, bus, or any other vehicle of public transport.\n\n::: {#b9cc9ed2 .cell execution_count=4}\n``` {.python .cell-code}\n# Missing values per column\ndf.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nBETRIEBSTAG                  0\nFAHRT_BEZEICHNER             0\nBETREIBER_ID                 0\nBETREIBER_ABK                0\nBETREIBER_NAME               0\nPRODUKT_ID                 233\nLINIEN_ID                    0\nLINIEN_TEXT                  0\nUMLAUF_ID              1196819\nVERKEHRSMITTEL_TEXT          0\nZUSATZFAHRT_TF               0\nFAELLT_AUS_TF                0\nBPUIC                        0\nHALTESTELLEN_NAME       158400\nANKUNFTSZEIT            138774\nAN_PROGNOSE             164786\nAN_PROGNOSE_STATUS      138560\nABFAHRTSZEIT            138604\nAB_PROGNOSE             165447\nAB_PROGNOSE_STATUS      138351\nDURCHFAHRT_TF                0\ndtype: int64\n```\n:::\n:::\n\n\nWe can see that some columns contain many missing values. The only one I worry about for now is the column `PRODUKT_ID`. If you look through these rows (I don't show that here), you can see that they should all be of type \"Zug\" (train). Thus, we impute accordingly:\n\n::: {#db67f050 .cell execution_count=5}\n``` {.python .cell-code}\n# Impute 'Zug'\ndf.loc[df[\"PRODUKT_ID\"].isna(), \"PRODUKT_ID\"] = 'Zug'\n```\n:::\n\n\nThere are quite a few date-timestamp columns that are not yet in the proper format. Thus, we now convert them to datetime formats:\n\n::: {#6a8945d5 .cell execution_count=6}\n``` {.python .cell-code}\n# Convert BETRIEBSTAG to date format\ndf['BETRIEBSTAG'] = pd.to_datetime(df['BETRIEBSTAG'], format = \"%d.%m.%Y\")\n\n# Convert ANKUNFTSZEIT, AN_PROGNOSE, ABFAHRTSZEIT, AB_PROGNOSE to datetime format\ndf['ANKUNFTSZEIT'] = pd.to_datetime(df['ANKUNFTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AN_PROGNOSE'] = pd.to_datetime(df['AN_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\ndf['ABFAHRTSZEIT'] = pd.to_datetime(df['ABFAHRTSZEIT'], format = \"%d.%m.%Y %H:%M\")\ndf['AB_PROGNOSE'] = pd.to_datetime(df['AB_PROGNOSE'], format = \"%d.%m.%Y %H:%M:%S\")\n```\n:::\n\n\nNow is a good time to finally have a look at the dataframe:\n\n::: {#5bc8e507 .cell execution_count=7}\n``` {.python .cell-code}\n# Let's look at first few rows\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BETRIEBSTAG</th>\n      <th>FAHRT_BEZEICHNER</th>\n      <th>BETREIBER_ID</th>\n      <th>BETREIBER_ABK</th>\n      <th>BETREIBER_NAME</th>\n      <th>PRODUKT_ID</th>\n      <th>LINIEN_ID</th>\n      <th>LINIEN_TEXT</th>\n      <th>UMLAUF_ID</th>\n      <th>VERKEHRSMITTEL_TEXT</th>\n      <th>ZUSATZFAHRT_TF</th>\n      <th>FAELLT_AUS_TF</th>\n      <th>BPUIC</th>\n      <th>HALTESTELLEN_NAME</th>\n      <th>ANKUNFTSZEIT</th>\n      <th>AN_PROGNOSE</th>\n      <th>AN_PROGNOSE_STATUS</th>\n      <th>ABFAHRTSZEIT</th>\n      <th>AB_PROGNOSE</th>\n      <th>AB_PROGNOSE_STATUS</th>\n      <th>DURCHFAHRT_TF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-06-06</td>\n      <td>80:800631:17230:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17230</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>2023-06-06 04:59:00</td>\n      <td>2023-06-06 04:59:00</td>\n      <td>PROGNOSE</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-06-06</td>\n      <td>80:800631:17233:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17233</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>2023-06-06 06:07:00</td>\n      <td>2023-06-06 06:07:00</td>\n      <td>PROGNOSE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-06-06</td>\n      <td>80:800631:17234:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17234</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>2023-06-06 05:56:00</td>\n      <td>2023-06-06 05:58:00</td>\n      <td>PROGNOSE</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-06-06</td>\n      <td>80:800631:17235:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17235</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>2023-06-06 06:43:00</td>\n      <td>2023-06-06 06:43:00</td>\n      <td>PROGNOSE</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-06-06</td>\n      <td>80:800631:17236:000</td>\n      <td>80:800631</td>\n      <td>DB</td>\n      <td>DB Regio AG Baden-Württemberg</td>\n      <td>Zug</td>\n      <td>17236</td>\n      <td>RB</td>\n      <td>NaN</td>\n      <td>RB</td>\n      <td>False</td>\n      <td>False</td>\n      <td>8500090</td>\n      <td>Basel Bad Bf</td>\n      <td>2023-06-06 06:33:00</td>\n      <td>2023-06-06 06:34:00</td>\n      <td>PROGNOSE</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBut what do all these columns mean? I have browsed the metadata a bit and found the following explanations (that I hopefully accurately reproduce in English):\n\n-   `BETRIEBSTAG`: Simply the day on which the data were recorded.\n-   `FAHRT_BEZEICHNER`: This is some elaborate identifier in the format \\[UIC-Countrycode\\]:\\[GO-Number\\]:\\[VM-Number\\]:\\[Extended Reference\\].\n-   `BETREIBER_ID`: \\[UIC-Countrycode\\]:\\[GO-Number\\]. GO is short for \"Geschäftsorganisation\". For foreign organizations it is not a GO-Number but a TU-Number with TU meaning \"Transportunternehmen\". It is basically an ID for the company running that particular train.\n-   `BETREIBER_ABK`: The abbreviation for the company running the train.\n-   `BETREIBER_NAME`: The full name of the company running the train.\n-   `PRODUKT_ID`: Type of public transport.\n-   `LINIEN_ID`: The ID for the route of that train.\n-   `LINIEN_TEXT`: The public ID for the route of that train.\n-   `UMLAUF_ID`: An ID for a \"Umlauf\" which describes the period starting with the vehicle leaving the garage and ending with the vehicle being deposited back in the garage.\n-   `ZUSATZFAHRT_TF`: Is true if it is an extraordinary (not usually scheduled) trip.\n-   `FAELLT_AUS_TF`: Is true if the trip is cancelled.\n-   `BPUIC`: The ID of the station.\n-   `HALTESTELLEN_NAME`: The name of the station.\n-   `ANKUNFTSZEIT`: Planned time of arrival at the station.\n-   `AN_PROGNOSE`: Prediction of time of arrival at the station.\n-   `AN_PROGNOSE_STATUS`: Status of that prediction. Possible values are: \"UNBEKANNT\", \"leer\", \"PROGNOSE\", \"GESCHAETZT\", \"REAL\". If the value of that column is \"REAL\", it means that the predicted time of arrival is the time the train actually arrived at the station.\n-   `ABFAHRTSZEIT`, `AB_PROGNOSE`, `AB_PROGNOSE_STATUS`: Same definitions as for arrival but here for departure from the station.\n-   `DURCHFAHRT_TF`: Is true if the vehicle does not stop even if a stop was scheduled.\n\nLet's now have a look at the values in the column `PRODUKT_ID`:\n\n::: {#e147a71e .cell execution_count=8}\n``` {.python .cell-code}\n# Look at PRODUKT_ID\ndf[\"PRODUKT_ID\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nPRODUKT_ID\nBus            1746016\nTram            247305\nZug             162038\nBUS             147531\nMetro             4304\nZahnradbahn       1965\nSchiff             658\nName: count, dtype: int64\n```\n:::\n:::\n\n\nWe can see that trains are only the third most frequent category in this data. However, as mentioned before, we want to keep it simple and now reduce the dataset to only trains.\n\n::: {#178e5b88 .cell execution_count=9}\n``` {.python .cell-code}\n# First we reduce to only trains\ndf = df[df['PRODUKT_ID'] == \"Zug\"]\n```\n:::\n\n\nIn a next step, we remove all rows where the corresponding train has been cancelled.\n\n::: {#3f97d5d1 .cell execution_count=10}\n``` {.python .cell-code}\n# Filter out all entries with FAELLT_AUS_TF == True\ndf = df[df['FAELLT_AUS_TF'] == False]\n```\n:::\n\n\nLet's explore the data a bit more before we move to the second dataset. Let's check out the most frequent values that occur in the column `BETREIBER_NAME`:\n\n::: {#bbfaa256 .cell execution_count=11}\n``` {.python .cell-code}\n# Look at BETREIBER_NAME\ndf[\"BETREIBER_NAME\"].value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nBETREIBER_NAME\nSchweizerische Bundesbahnen SBB    61798\nBLS AG (bls)                       15842\nTHURBO                             13250\nAargau Verkehr AG                   7220\nRhätische Bahn                      5366\nName: count, dtype: int64\n```\n:::\n:::\n\n\nAs expected, SBB is the company serving the largest number of stations. What about the column `VERKEHRSMITTEL_TEXT`?\n\n::: {#a966c258 .cell execution_count=12}\n``` {.python .cell-code}\n# Look at VERKEHRSMITTEL_TEXT\ndf[\"VERKEHRSMITTEL_TEXT\"].value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nVERKEHRSMITTEL_TEXT\nS     105288\nR      31987\nRE      8551\nIR      7104\nIC      2891\nName: count, dtype: int64\n```\n:::\n:::\n\n\nWe can see that the most frequent type of trains are S-Bahns (`S`). Finally, let's check the most frequent train stations that occur in the data:\n\n::: {#df965f66 .cell execution_count=13}\n``` {.python .cell-code}\n# Look at HALTESTELLEN_NAME\ndf[\"HALTESTELLEN_NAME\"].value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nHALTESTELLEN_NAME\nZürich HB          1914\nBern               1706\nWinterthur          948\nZürich Oerlikon     918\nLuzern              814\nName: count, dtype: int64\n```\n:::\n:::\n\n\nUnsurprisingly, Zürich and Bern are the most frequent values occuring in the data.\n\n### Load and preprocess \"Dienststellen-Daten\"\n\nFortunately, we can go through the second dataset a bit more quickly. We again start by loading it and checking the dimensions of the dataframe.\n\n::: {#585f0146 .cell execution_count=14}\n``` {.python .cell-code}\n# Load the data\nds = pd.read_csv('dienststellen_full.csv', sep = \";\", low_memory=False)\n\n# Number of rows and columns\nprint(ds.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(152433, 78)\n```\n:::\n:::\n\n\nThe data contains a column `GUELTIG_BIS` that allows us to filter out all stations that are not valid anymore (closed down?). But first we need to transform it into the proper format.\n\n::: {#2cdbf4fc .cell execution_count=15}\n``` {.python .cell-code}\n# GUELTIG_BIS as datetime\nds['GUELTIG_BIS'] = pd.to_datetime(ds['GUELTIG_BIS'], format = \"%Y-%m-%d\")\n\n# Keep only currently valid entries\nds = ds[ds['GUELTIG_BIS'] == \"2099-12-31\"]\n```\n:::\n\n\nFinally, we keep only the columns we need (identifier, official name, and geo coordinates).\n\n::: {#927c7b62 .cell execution_count=16}\n``` {.python .cell-code}\n# Keep only the relevant columns\nds = ds[[\"BPUIC\",\"BEZEICHNUNG_OFFIZIELL\",\"E_WGS84\",\"N_WGS84\",\"Z_WGS84\"]]\n\n# Show first few rows\nds.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BPUIC</th>\n      <th>BEZEICHNUNG_OFFIZIELL</th>\n      <th>E_WGS84</th>\n      <th>N_WGS84</th>\n      <th>Z_WGS84</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26</th>\n      <td>8531284</td>\n      <td>Samnaun-Ravaisch (Talst. I)</td>\n      <td>10.375228</td>\n      <td>46.951317</td>\n      <td>1775.0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>8530938</td>\n      <td>Fräkmüntegg (3. Sekt. Talst.)</td>\n      <td>8.251436</td>\n      <td>46.990475</td>\n      <td>1413.0</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>1400013</td>\n      <td>Annecy, Pont Neuf</td>\n      <td>6.114629</td>\n      <td>45.897774</td>\n      <td>-9999.0</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>8530879</td>\n      <td>Foppa (Naraus)</td>\n      <td>9.267387</td>\n      <td>46.846531</td>\n      <td>1418.0</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>8584977</td>\n      <td>Essence Tamoil Collombey</td>\n      <td>6.954193</td>\n      <td>46.266124</td>\n      <td>392.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Combine the two datasets\n\nWe now merge the \"Dienststellen-Daten\" to the first dataset via the `BPUIC` variable.\n\n::: {#dfc6f283 .cell execution_count=17}\n``` {.python .cell-code}\n# Left-join with station names and coordinates\ndf = pd.merge(df, ds, on = 'BPUIC', how = 'left')\n```\n:::\n\n\nUnfortunately, there are some rows for which there is no matching entry in the \"Dienststellen-Daten\". But fortunately, we know which stations are affected based on the `HALTESTELLEN_NAME` column.\n\n::: {#712d744d .cell execution_count=18}\n``` {.python .cell-code}\n# There are still some missings after left-join (Oberkulm Post and Borgnone-Cadanza)\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'].isna(), ['HALTESTELLEN_NAME','BEZEICHNUNG_OFFIZIELL']]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>HALTESTELLEN_NAME</th>\n      <th>BEZEICHNUNG_OFFIZIELL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>101255</th>\n      <td>Borgnone-Cadanza</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>101347</th>\n      <td>Borgnone-Cadanza</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>101384</th>\n      <td>Borgnone-Cadanza</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>101387</th>\n      <td>Borgnone-Cadanza</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>101408</th>\n      <td>Borgnone-Cadanza</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156746</th>\n      <td>Oberkulm Post</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>156763</th>\n      <td>Oberkulm Post</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>156786</th>\n      <td>Oberkulm Post</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>156803</th>\n      <td>Oberkulm Post</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>156823</th>\n      <td>Oberkulm Post</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>156 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\nThe first part of the fix consists of imputing the names of the stations in the column `BEZEICHNUNG_OFFIZIELL`.\n\n::: {#d121b904 .cell execution_count=19}\n``` {.python .cell-code}\n# But they have data in the original data, so let's impute those\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'].isna(), \"BEZEICHNUNG_OFFIZIELL\"] = df.loc[df['BEZEICHNUNG_OFFIZIELL'].isna(), \"HALTESTELLEN_NAME\"]\n```\n:::\n\n\nThe second part of the fix is to manually add the geo coordinates for the missing two stations (Oberkulm Post and Borgnone-Cadanza).\n\n::: {#0acdb2be .cell execution_count=20}\n``` {.python .cell-code}\n# Impute geo coordinates and elevation for those missing\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'] == \"Oberkulm Post\", [\"E_WGS84\",\"N_WGS84\",\"Z_WGS84\"]] = (8.11970, 47.30414, 483)\ndf.loc[df['BEZEICHNUNG_OFFIZIELL'] == \"Borgnone-Cadanza\", [\"E_WGS84\",\"N_WGS84\",\"Z_WGS84\"]] = (8.62254, 46.15853, 713)\n```\n:::\n\n\nNow, we are finally ready to start extracting the network from this data!\n\n### Convert it to a network\n\nAs I mentioned several times, every row corresponds to a stop of a train at a train station. One train ride from some initial station to some end station (called \"Fahrt\" in German) then typically consists of several stops along the way. However, there are some \"Fahrten\" with only one entry. Presumably these are mostly foreign trains that have their end destination at some border station. I decided to remove those entries:\n\n::: {#85604bb1 .cell execution_count=21}\n``` {.python .cell-code}\n# First group by FAHRT_BEZEICHNER and then filter out all groups with only one entry\n# It's mostly trains that stop at a place at the border (I think)\ndf_filtered = df.groupby('FAHRT_BEZEICHNER').filter(lambda g: len(g) > 1)\n\n# How many rows do we loose with that?\nprint(df.shape[0] - df_filtered.shape[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n566\n```\n:::\n:::\n\n\nThis preprocessing step removes 566 rows.\n\nNow we group the rows by `FAHRT_BEZEICHNER` so that each group is one \"Fahrt\". In every group we sort the stops along the way in an ascending order of the departure time.\n\n::: {#15b680f4 .cell execution_count=22}\n``` {.python .cell-code}\n# Function to sort entries within a group in ascending order of ABFAHRTSZEIT\ndef sort_data(group):\n    return group.sort_values('ABFAHRTSZEIT', ascending = True)\n\n# Sort for each group\ndf_sorted = df_filtered.groupby('FAHRT_BEZEICHNER', group_keys=True).apply(sort_data)\n```\n:::\n\n\nLet's have a look at one \"Fahrt\" to get a better idea:\n\n::: {#fd55070b .cell execution_count=23}\n``` {.python .cell-code}\n# Look at one example Fahrt\ndf_sorted.loc[['85:11:1511:003'],['BETREIBER_ABK','LINIEN_TEXT','BEZEICHNUNG_OFFIZIELL','ABFAHRTSZEIT']]\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>BETREIBER_ABK</th>\n      <th>LINIEN_TEXT</th>\n      <th>BEZEICHNUNG_OFFIZIELL</th>\n      <th>ABFAHRTSZEIT</th>\n    </tr>\n    <tr>\n      <th>FAHRT_BEZEICHNER</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"12\" valign=\"top\">85:11:1511:003</th>\n      <th>1758</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Lausanne</td>\n      <td>2023-06-06 06:15:00</td>\n    </tr>\n    <tr>\n      <th>1759</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Yverdon-les-Bains</td>\n      <td>2023-06-06 06:37:00</td>\n    </tr>\n    <tr>\n      <th>1760</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Neuchâtel</td>\n      <td>2023-06-06 06:58:00</td>\n    </tr>\n    <tr>\n      <th>1761</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Biel/Bienne</td>\n      <td>2023-06-06 07:17:00</td>\n    </tr>\n    <tr>\n      <th>1762</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Grenchen Süd</td>\n      <td>2023-06-06 07:26:00</td>\n    </tr>\n    <tr>\n      <th>1763</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Solothurn</td>\n      <td>2023-06-06 07:34:00</td>\n    </tr>\n    <tr>\n      <th>1764</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Oensingen</td>\n      <td>2023-06-06 07:46:00</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Olten</td>\n      <td>2023-06-06 07:59:00</td>\n    </tr>\n    <tr>\n      <th>1766</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Zürich HB</td>\n      <td>2023-06-06 08:33:00</td>\n    </tr>\n    <tr>\n      <th>1767</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Zürich Flughafen</td>\n      <td>2023-06-06 08:43:00</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>Winterthur</td>\n      <td>2023-06-06 08:59:00</td>\n    </tr>\n    <tr>\n      <th>1769</th>\n      <td>SBB</td>\n      <td>IC5</td>\n      <td>St. Gallen</td>\n      <td>NaT</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis is a train that goes from Lausanne to St.Gallen with many stops in-between. In St.Gallen the `ABFAHRTSZEIT` is missing as that \"Fahrt\" ends there (the train will most likely go back in the other direction, but that will be a new \"Fahrt\").\n\nWe now have enough knowledge about the data that we can extract the edges in a for loop. Basically, what we do is to loop over the rows of a given \"Fahrt\", starting with the second row and extracting the edges as\n\n`(previous station, current station, travel time between stations)`.\n\nThe Python code for this looks as follows:\n\n::: {#0b36b4c8 .cell execution_count=24}\n``` {.python .cell-code}\n# Empty list\nedgelist = []\n\n# Variables to store previous row and its index\nprev_row = None\nprev_idx = None\n\n# Loop over rows of dataframe\nfor i, row in df_sorted.iterrows():\n    # Only start with second row\n    # Only if the two rows belong to the same Fahrt\n    if prev_idx is not None and prev_idx == i[0]:\n        # Add edge to edgelist assuming it's a directed edge\n        edgelist.append((prev_row['BEZEICHNUNG_OFFIZIELL'], \n                         row['BEZEICHNUNG_OFFIZIELL'], \n                         (row['ANKUNFTSZEIT'] - prev_row['ABFAHRTSZEIT']).total_seconds() / 60))\n    # Set current row and row index to previous ones\n    prev_idx = i[0]\n    prev_row = row\n```\n:::\n\n\nTo get a better idea, let's have a look at the first list element:\n\n::: {#33fb1aa1 .cell execution_count=25}\n``` {.python .cell-code}\n# First list element\nedgelist[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n('Schaffhausen', 'Basel Bad Bf', 75.0)\n```\n:::\n:::\n\n\nWe are still not quite done yet. The problem is that the `edgelist` contains many duplicated entries as, for example, the stretch Olten - Zürich HB is served by many different trains on a given day.\n\nWhat we want to do is to go through all possible edges and sum up the number of times they occur. In addition, we would like to average the travel time between a given pair of stations over all trips between the two stations. The following code does exactly that and saves the result in the form of a dictionary.\n\n::: {#18b79d62 .cell execution_count=26}\n``` {.python .cell-code}\n# Empty dict\nedges = {}\n\n# Loop over elements in edgelist\nfor i in edgelist:\n    # Create key\n    key = (i[0], i[1])\n    # Get previous entries in dict (if there are any)\n    prev = edges.get(key, (0, 0))\n    # Update values in dict\n    edges[key] = (prev[0] + 1, prev[1] + i[2])\n\n# Divide summed up travel times by number of trips\nedges = {k: (v[0], round(v[1]/v[0], 2)) for k, v in edges.items()}\n```\n:::\n\n\nLet's look at the entry for the stretch between Schaffhausen and Basel Badischer Bahnhof again:\n\n::: {#1e49ba15 .cell execution_count=27}\n``` {.python .cell-code}\n# Look at some element in dict\nedges[('Schaffhausen', 'Basel Bad Bf')]\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n(17, 73.18)\n```\n:::\n:::\n\n\nThere are 17 trips between these two stations (in this direction) and they take 73 minutes on average. (Note that there are quite a few stops between these two stations but they do not appear in the data since they are all located in Germany.)\n\nWe are now ready to create the final node list (and export it). First, we add the two missing stations to the dataframe `ds` (above we only added them to the merged `df`, not `ds`). Then we reduce `ds` to the train stations that appear in the edges (it still contains many bus and tram stops and other things). Finally, we give it nicer column names.\n\n::: {#4233fb5d .cell execution_count=28}\n``` {.python .cell-code}\n# Add two missing places to ds\nds.loc[len(ds)] = [8502183, 'Oberkulm Post', 8.1197, 47.30414, 483.0]\nds.loc[len(ds)] = [8505498, 'Borgnone-Cadanza', 8.62254, 46.15853, 713.0]\n\n# Set of stations that appear in edgelist\nstations_in_edgelist = set(sum(list(edges.keys()), ()))\n\n# Reduces nodes dataframe to only places in edgelist\nnodes = ds[ds['BEZEICHNUNG_OFFIZIELL'].isin(stations_in_edgelist)]\n\n# Better column names\nnodes.columns = ['BPUIC','STATION_NAME','LONGITUDE','LATITUDE','ELEVATION']\n\n# Have a look\nnodes.head()\n\n# Export node list\n# nodes.sort_values(\"BPUIC\").to_csv(\"nodelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BPUIC</th>\n      <th>STATION_NAME</th>\n      <th>LONGITUDE</th>\n      <th>LATITUDE</th>\n      <th>ELEVATION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2375</th>\n      <td>8500329</td>\n      <td>Koblenz</td>\n      <td>8.227050</td>\n      <td>47.600338</td>\n      <td>320.3</td>\n    </tr>\n    <tr>\n      <th>2431</th>\n      <td>8503093</td>\n      <td>Zürich Manegg</td>\n      <td>8.519751</td>\n      <td>47.338009</td>\n      <td>430.0</td>\n    </tr>\n    <tr>\n      <th>3816</th>\n      <td>8508208</td>\n      <td>Trubschachen</td>\n      <td>7.846142</td>\n      <td>46.921700</td>\n      <td>732.0</td>\n    </tr>\n    <tr>\n      <th>4013</th>\n      <td>8503290</td>\n      <td>Biberegg</td>\n      <td>8.669443</td>\n      <td>47.093610</td>\n      <td>933.0</td>\n    </tr>\n    <tr>\n      <th>4102</th>\n      <td>8502271</td>\n      <td>Wohlen Oberdorf</td>\n      <td>8.286887</td>\n      <td>47.346852</td>\n      <td>433.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBefore we export the edges, we change the station names in the edgelist to the BPUIC to make the edges more compact. Then we transform the dictionary into a dataframe which can finally be exported.\n\n::: {#d1d71f47 .cell execution_count=29}\n``` {.python .cell-code}\n# Create a node dict with BPUIC as values\nnode_dict = dict(zip(nodes.STATION_NAME, nodes.BPUIC))\n\n# Transform edge dict to nested list and replace all station names with their BPUIC\nedges = [[node_dict[k[0]], node_dict[k[1]], v[0], v[1]] for k,v in edges.items()]\n\n# Create a dataframe\nedges = pd.DataFrame(edges, columns = ['BPUIC1','BPUIC2','NUM_CONNECTIONS','AVG_DURATION'])\n\n# Have a look\nedges.head()\n\n# Export edge list\n# edges.to_csv(\"edgelist.csv\", sep = ';', encoding = 'utf-8', index = False)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BPUIC1</th>\n      <th>BPUIC2</th>\n      <th>NUM_CONNECTIONS</th>\n      <th>AVG_DURATION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8503424</td>\n      <td>8500090</td>\n      <td>17</td>\n      <td>73.18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8500090</td>\n      <td>8503424</td>\n      <td>18</td>\n      <td>72.39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8503000</td>\n      <td>8500010</td>\n      <td>35</td>\n      <td>54.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8500010</td>\n      <td>8500090</td>\n      <td>68</td>\n      <td>6.06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8500090</td>\n      <td>8500010</td>\n      <td>73</td>\n      <td>6.33</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFeel free to download the final results: <a href=\"nodelist.csv\" download>Nodelist (CSV)</a> and <a href=\"edgelist.csv\" download>Edgelist (CSV)</a>.\n\n*The title image has been created by Wikimedia user JoachimKohler-HB and is licensed under [Creative Commons](https://creativecommons.org/licenses/by-sa/4.0/deed.en).*\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}